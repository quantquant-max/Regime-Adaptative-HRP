{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b57ff856",
   "metadata": {},
   "source": [
    "# CUDA-Accelerated HRP - Marcos Lopez de Prado's Algorithm\n",
    "\n",
    "**Based on**: Original HRP implementation from \"Advances in Financial Machine Learning\"\n",
    "\n",
    "**Key Fixes**:\n",
    "- Uses exact algorithm from the book\n",
    "- Proper cluster variance calculation with reshape\n",
    "- Correct recursive bisection logic\n",
    "- CUDA acceleration for covariance/correlation\n",
    "\n",
    "**Runtime**: ~2-5 minutes for 5 random years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8737047c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports and GPU Setup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import scipy.cluster.hierarchy as sch\n",
    "from sklearn.covariance import LedoitWolf\n",
    "\n",
    "# Try to import GPU libraries\n",
    "GPU_AVAILABLE = False\n",
    "try:\n",
    "    import cupy as cp\n",
    "    import cuml\n",
    "    GPU_AVAILABLE = True\n",
    "    print(\"✓ GPU (CUDA) libraries loaded successfully\")\n",
    "    print(f\"  CuPy version: {cp.__version__}\")\n",
    "except ImportError as e:\n",
    "    print(\"⚠ GPU libraries not available, using CPU fallback\")\n",
    "    print(f\"  Error: {e}\")\n",
    "\n",
    "print(f\"\\nMode: {'GPU (CUDA)' if GPU_AVAILABLE else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d769319e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Load Data and Select Random Years\n",
    "data_path = r'DATA (CRSP)\\PREPROCESSED DATA\\ADA-HRP-Preprocessed-DATA.csv'\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "stocks_df = df[df['PERMNO'].notna()].copy()\n",
    "date_cols = [col for col in stocks_df.columns if col not in ['PERMNO', 'Company_Ticker']]\n",
    "dates = pd.to_datetime(date_cols)\n",
    "date_strs = [d.strftime('%Y-%m-%d') for d in dates]\n",
    "\n",
    "# Select 5 random years\n",
    "random.seed(42)\n",
    "all_years = sorted(set(dates.year))\n",
    "selected_years = sorted(random.sample(all_years, 5))\n",
    "\n",
    "# Get quarterly rebalance dates (last day of Mar, Jun, Sep, Dec) for selected years\n",
    "quarterly_rebalance_dates = []\n",
    "for date in dates:\n",
    "    if date.year in selected_years and date.month in [3, 6, 9, 12]:\n",
    "        if date == dates[dates.to_period('M') == date.to_period('M')].max():\n",
    "            quarterly_rebalance_dates.append(date)\n",
    "\n",
    "quarterly_rebalance_dates = sorted(set(quarterly_rebalance_dates))\n",
    "\n",
    "print(f\"Selected years: {selected_years}\")\n",
    "print(f\"Quarterly rebalance dates: {len(quarterly_rebalance_dates)}\")\n",
    "print(f\"Total stocks in dataset: {len(stocks_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bfd4a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: HRP Functions - Marcos Lopez de Prado's Original Algorithm\n",
    "\n",
    "def getIVP(cov):\n",
    "    \"\"\"Compute the inverse-variance portfolio\"\"\"\n",
    "    ivp = 1. / np.diag(cov)\n",
    "    ivp /= ivp.sum()\n",
    "    return ivp\n",
    "\n",
    "def getClusterVar(cov, cItems):\n",
    "    \"\"\"Compute variance per cluster - EXACT Marcos implementation\"\"\"\n",
    "    cov_ = cov.loc[cItems, cItems]  # matrix slice\n",
    "    w_ = getIVP(cov_).reshape(-1, 1)  # CRITICAL: reshape to column vector\n",
    "    cVar = np.dot(np.dot(w_.T, cov_), w_)[0, 0]  # CRITICAL: [0,0] indexing\n",
    "    return cVar\n",
    "\n",
    "def getQuasiDiag(link):\n",
    "    \"\"\"Sort clustered items by distance - Marcos implementation\"\"\"\n",
    "    link = link.astype(int)\n",
    "    sortIx = pd.Series([link[-1, 0], link[-1, 1]])\n",
    "    numItems = link[-1, 3]  # number of original items\n",
    "    while sortIx.max() >= numItems:\n",
    "        sortIx.index = range(0, sortIx.shape[0] * 2, 2)  # make space\n",
    "        df0 = sortIx[sortIx >= numItems]  # find clusters\n",
    "        i = df0.index\n",
    "        j = df0.values - numItems\n",
    "        sortIx[i] = link[j, 0]  # item 1\n",
    "        df0 = pd.Series(link[j, 1], index=i + 1)\n",
    "        sortIx = pd.concat([sortIx, df0])  # item 2\n",
    "        sortIx = sortIx.sort_index()  # re-sort\n",
    "        sortIx.index = range(sortIx.shape[0])  # re-index\n",
    "    return sortIx.tolist()\n",
    "\n",
    "def getRecBipart(cov, sortIx):\n",
    "    \"\"\"Compute HRP allocation - EXACT Marcos implementation\"\"\"\n",
    "    w = pd.Series(1, index=sortIx)\n",
    "    cItems = [sortIx]  # initialize all items in one cluster\n",
    "    while len(cItems) > 0:\n",
    "        # Bi-section - CRITICAL: use integer division //\n",
    "        cItems = [i[j:k] for i in cItems for j, k in ((0, len(i) // 2), (len(i) // 2, len(i))) if len(i) > 1]\n",
    "        # Parse in pairs\n",
    "        for i in range(0, len(cItems), 2):\n",
    "            cItems0 = cItems[i]  # cluster 1\n",
    "            cItems1 = cItems[i + 1]  # cluster 2\n",
    "            cVar0 = getClusterVar(cov, cItems0)\n",
    "            cVar1 = getClusterVar(cov, cItems1)\n",
    "            alpha = 1 - cVar0 / (cVar0 + cVar1)\n",
    "            w[cItems0] *= alpha  # weight 1\n",
    "            w[cItems1] *= 1 - alpha  # weight 2\n",
    "    return w\n",
    "\n",
    "def correlDist(corr):\n",
    "    \"\"\"A distance matrix based on correlation - Marcos implementation\"\"\"\n",
    "    dist = ((1 - corr) / 2.) ** 0.5  # distance matrix\n",
    "    return dist\n",
    "\n",
    "def compute_covariance_gpu(returns_np, returns_gpu=None):\n",
    "    \"\"\"Compute shrunk covariance using GPU (with Ledoit-Wolf shrinkage)\"\"\"\n",
    "    if GPU_AVAILABLE:\n",
    "        if returns_gpu is None:\n",
    "            returns_gpu = cp.asarray(returns_np)\n",
    "        mean = cp.mean(returns_gpu, axis=0, keepdims=True)\n",
    "        centered = returns_gpu - mean\n",
    "        n_samples = returns_gpu.shape[0]\n",
    "        cov_sample = (centered.T @ centered) / (n_samples - 1)\n",
    "        mu = cp.trace(cov_sample) / cov_sample.shape[0]\n",
    "        delta = cp.sum((cov_sample - mu * cp.eye(cov_sample.shape[0])) ** 2)\n",
    "        X2 = centered ** 2\n",
    "        gamma = cp.sum((X2.T @ X2) / n_samples - cov_sample ** 2)\n",
    "        kappa = gamma / delta if delta > 0 else 1.0\n",
    "        shrinkage = max(0.0, min(1.0, float(cp.asnumpy(kappa))))\n",
    "        target = mu * cp.eye(cov_sample.shape[0])\n",
    "        cov_shrunk_gpu = shrinkage * target + (1 - shrinkage) * cov_sample\n",
    "        return cp.asnumpy(cov_shrunk_gpu), shrinkage\n",
    "    else:\n",
    "        lw = LedoitWolf().fit(returns_np)\n",
    "        return lw.covariance_, lw.shrinkage_\n",
    "\n",
    "print(\"✓ HRP functions defined (Marcos Lopez de Prado's algorithm)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492b8d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Process Quarterly Rebalances\n",
    "\n",
    "weights_list = []\n",
    "timing_stats = {'cov': [], 'corr': [], 'dist': [], 'cluster': [], 'weights': [], 'total': []}\n",
    "skipped_count = 0\n",
    "\n",
    "for rebal_date in tqdm(quarterly_rebalance_dates, desc=\"Processing rebalance dates\"):\n",
    "    t_start = time.time()\n",
    "    rebal_str = rebal_date.strftime('%Y-%m-%d')\n",
    "    \n",
    "    try:\n",
    "        rebal_idx = date_strs.index(rebal_str)\n",
    "    except ValueError:\n",
    "        skipped_count += 1\n",
    "        continue\n",
    "    \n",
    "    if rebal_idx < 11:\n",
    "        skipped_count += 1\n",
    "        continue\n",
    "    \n",
    "    window_indices = list(range(rebal_idx - 11, rebal_idx + 1))\n",
    "    actual_window_cols = [date_cols[i] for i in window_indices]\n",
    "    \n",
    "    if len(actual_window_cols) != 12:\n",
    "        skipped_count += 1\n",
    "        continue\n",
    "    \n",
    "    window_df = stocks_df[['PERMNO', 'Company_Ticker'] + actual_window_cols].copy()\n",
    "    window_df = window_df[window_df['Company_Ticker'].notna()]\n",
    "    \n",
    "    valid_mask = window_df[actual_window_cols].notna().sum(axis=1) == 12\n",
    "    window_df = window_df[valid_mask]\n",
    "    \n",
    "    if len(window_df) < 20:\n",
    "        skipped_count += 1\n",
    "        continue\n",
    "    \n",
    "    # Prepare returns matrix\n",
    "    returns = window_df[actual_window_cols].T  # Time x Assets\n",
    "    returns.columns = window_df['PERMNO'].astype(str)\n",
    "    \n",
    "    # Filter out stocks with zero or near-zero variance\n",
    "    stock_variance = returns.values.var(axis=0, ddof=1)\n",
    "    min_variance = 1e-10\n",
    "    valid_variance_mask = (stock_variance > min_variance) & np.isfinite(stock_variance)\n",
    "\n",
    "    if valid_variance_mask.sum() < 2:\n",
    "        skipped_count += 1\n",
    "        continue\n",
    "\n",
    "    # Filter returns to keep only valid stocks\n",
    "    valid_permnos = returns.columns[valid_variance_mask].tolist()\n",
    "    returns = returns[valid_permnos]\n",
    "    returns_np = returns.values\n",
    "    \n",
    "    # === GPU-ACCELERATED COVARIANCE ===\n",
    "    t0 = time.time()\n",
    "    if GPU_AVAILABLE:\n",
    "        returns_gpu = cp.asarray(returns_np)\n",
    "        cov_array, shrinkage = compute_covariance_gpu(returns_np, returns_gpu)\n",
    "    else:\n",
    "        cov_array, shrinkage = compute_covariance_gpu(returns_np)\n",
    "    timing_stats['cov'].append(time.time() - t0)\n",
    "    \n",
    "    # Create covariance DataFrame with PERMNO labels\n",
    "    cov = pd.DataFrame(cov_array, index=returns.columns, columns=returns.columns)\n",
    "    \n",
    "    # === GPU-ACCELERATED CORRELATION ===\n",
    "    t0 = time.time()\n",
    "    if GPU_AVAILABLE:\n",
    "        cov_gpu = cp.asarray(cov_array)\n",
    "        std_gpu = cp.sqrt(cp.diag(cov_gpu))\n",
    "        std_gpu = cp.where(std_gpu < 1e-10, 1e-10, std_gpu)\n",
    "        corr_gpu = cov_gpu / cp.outer(std_gpu, std_gpu)\n",
    "        corr_gpu = cp.clip(corr_gpu, -1.0, 1.0)\n",
    "        corr_array = cp.asnumpy(corr_gpu)\n",
    "    else:\n",
    "        std = np.sqrt(np.diag(cov_array))\n",
    "        std = np.where(std < 1e-10, 1e-10, std)\n",
    "        corr_array = cov_array / np.outer(std, std)\n",
    "        corr_array = np.clip(corr_array, -1.0, 1.0)\n",
    "    timing_stats['corr'].append(time.time() - t0)\n",
    "    \n",
    "    # Create correlation DataFrame with PERMNO labels\n",
    "    corr = pd.DataFrame(corr_array, index=returns.columns, columns=returns.columns)\n",
    "    \n",
    "    # === CORRELATION DISTANCE ===\n",
    "    t0 = time.time()\n",
    "    dist = correlDist(corr)\n",
    "    timing_stats['dist'].append(time.time() - t0)\n",
    "    \n",
    "    # === CLUSTERING ===\n",
    "    t0 = time.time()\n",
    "    try:\n",
    "        link = sch.linkage(dist, method='single')\n",
    "        sortIx = getQuasiDiag(link)\n",
    "        # CRITICAL: sortIx contains INTEGER indices, convert to PERMNO labels\n",
    "        sortIx = corr.index[sortIx].tolist()\n",
    "    except Exception as e:\n",
    "        print(f\"⚠ Clustering failed for {rebal_str}: {e}, skipping\")\n",
    "        skipped_count += 1\n",
    "        continue\n",
    "    timing_stats['cluster'].append(time.time() - t0)\n",
    "    \n",
    "    # === COMPUTE HRP WEIGHTS ===\n",
    "    t0 = time.time()\n",
    "    try:\n",
    "        hrp_weights = getRecBipart(cov, sortIx)\n",
    "        \n",
    "        weight_sum = hrp_weights.sum()\n",
    "        if abs(weight_sum - 1.0) > 1e-6:\n",
    "            print(f\"⚠ WARNING {rebal_str}: Weights sum to {weight_sum:.10f}, renormalizing...\")\n",
    "            hrp_weights = hrp_weights / weight_sum\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"⚠ Weight computation failed for {rebal_str}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        skipped_count += 1\n",
    "        continue\n",
    "    \n",
    "    timing_stats['weights'].append(time.time() - t0)\n",
    "    timing_stats['total'].append(time.time() - t_start)\n",
    "    \n",
    "    # Store weights\n",
    "    weight_series = pd.Series(0.0, index=stocks_df['PERMNO'].astype(str))\n",
    "    weight_series.update(hrp_weights)\n",
    "    weights_list.append({\n",
    "        'date': rebal_date,\n",
    "        'weights': weight_series\n",
    "    })\n",
    "\n",
    "# Create weights DataFrame\n",
    "if len(weights_list) > 0:\n",
    "    all_weights = stocks_df[['PERMNO', 'Company_Ticker']].copy()\n",
    "    for w_dict in weights_list:\n",
    "        col_name = w_dict['date'].strftime('%Y-%m-%d')\n",
    "        all_weights[col_name] = w_dict['weights'].values\n",
    "    \n",
    "    # Save to CSV\n",
    "    rolling_dir = os.path.join('Rolling Windows Test')\n",
    "    os.makedirs(rolling_dir, exist_ok=True)\n",
    "    output_path = os.path.join(rolling_dir, 'hrp_weights_quicktest_marcos.csv')\n",
    "    all_weights.to_csv(output_path, index=False)\n",
    "else:\n",
    "    print(\"⚠ No weights computed!\")\n",
    "    all_weights = pd.DataFrame()\n",
    "\n",
    "# Print timing statistics\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"QUICK TEST PERFORMANCE SUMMARY (Marcos Algorithm)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Mode: {'GPU (CUDA)' if GPU_AVAILABLE else 'CPU'}\")\n",
    "print(f\"Selected years: {selected_years}\")\n",
    "print(f\"Total quarterly dates processed: {len(weights_list)}\")\n",
    "print(f\"Skipped (insufficient history): {skipped_count}\")\n",
    "if len(timing_stats['total']) > 0:\n",
    "    print(f\"\\nAverage timing per rebalance:\")\n",
    "    print(f\"  Covariance:      {np.mean(timing_stats['cov'])*1000:.2f} ms\")\n",
    "    print(f\"  Correlation:     {np.mean(timing_stats['corr'])*1000:.2f} ms\")\n",
    "    print(f\"  Distances:       {np.mean(timing_stats['dist'])*1000:.2f} ms\")\n",
    "    print(f\"  Clustering:      {np.mean(timing_stats['cluster'])*1000:.2f} ms\")\n",
    "    print(f\"  Weight Calc:     {np.mean(timing_stats['weights'])*1000:.2f} ms\")\n",
    "    print(f\"  Total:           {np.mean(timing_stats['total'])*1000:.2f} ms\")\n",
    "    print(f\"\\nTotal runtime:    {np.sum(timing_stats['total']):.2f} seconds\")\n",
    "print(\"\\n✓ Saved test weights to hrp_weights_quicktest_marcos.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebbc22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Validate the Fix - Check for Equal Weights Bug\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"VALIDATION: CHECKING FOR EQUAL WEIGHTS BUG\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "weights_file = os.path.join('Rolling Windows Test', 'hrp_weights_quicktest_marcos.csv')\n",
    "df_weights = pd.read_csv(weights_file)\n",
    "\n",
    "date_cols_check = [col for col in df_weights.columns if col not in ['PERMNO', 'Company_Ticker']]\n",
    "\n",
    "equal_dates = []\n",
    "proper_dates = []\n",
    "\n",
    "for date_col in date_cols_check:\n",
    "    weights = df_weights[date_col].dropna()\n",
    "    if len(weights) == 0:\n",
    "        continue\n",
    "    \n",
    "    equal_weight = 1.0 / len(weights)\n",
    "    all_equal = np.allclose(weights.values, equal_weight, rtol=1e-10)\n",
    "    \n",
    "    if all_equal:\n",
    "        equal_dates.append(date_col)\n",
    "        print(f\"X {date_col}: ALL EQUAL (N={len(weights)}, w={weights.iloc[0]:.10f})\")\n",
    "    else:\n",
    "        proper_dates.append(date_col)\n",
    "        print(f\"✓ {date_col}: PROPER HRP (N={len(weights)}, min={weights.min():.10f}, max={weights.max():.10f}, ratio={weights.max()/weights.min():.2f}x)\")\n",
    "\n",
    "print(f\"\\n{\"=\"*80}\")\n",
    "print(f\"SUMMARY:\")\n",
    "print(f\"  Equal weight dates: {len(equal_dates)}/{len(date_cols_check)}\")\n",
    "print(f\"  Proper HRP dates:   {len(proper_dates)}/{len(date_cols_check)}\")\n",
    "\n",
    "if len(equal_dates) == 0:\n",
    "    print(f\"\\nSUCCESS! Bug is FIXED! All weights show proper HRP dispersion!\")\n",
    "elif len(equal_dates) == len(date_cols_check):\n",
    "    print(f\"\\nPROBLEM! Bug still present - all dates show equal weights.\")\n",
    "else:\n",
    "    print(f\"\\nPARTIAL: {len(proper_dates)} dates fixed, {len(equal_dates)} still equal.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
