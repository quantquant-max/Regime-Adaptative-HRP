{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492d3188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports and Paths\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.cluster.hierarchy as sch\n",
    "from sklearn.covariance import LedoitWolf\n",
    "import os\n",
    "import random\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "# Define paths\n",
    "data_path = r'C:/Users/lucas/OneDrive/Bureau/HRP/DATA (CRSP)/PREPROCESSED DATA/ADA-HRP-Preprocessed-DATA.csv'\n",
    "rolling_dir = r'C:/Users/lucas/OneDrive/Bureau/HRP/DATA (CRSP)/Rolling Windows'\n",
    "os.makedirs(rolling_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b5bfd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Load Data and Prepare Dates (Corrected Sorting)\n",
    "# Load the data\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Identify date columns\n",
    "date_cols = [col for col in df.columns if col not in ['PERMNO', 'Company_Ticker']]\n",
    "\n",
    "# Parse mangled column names to dates for sorting\n",
    "parsed_strs = [col.replace('_', ':') for col in date_cols]\n",
    "parsed_dates = pd.to_datetime(parsed_strs, errors='coerce')\n",
    "\n",
    "# Sort by parsed dates\n",
    "sort_order = np.argsort(parsed_dates)\n",
    "date_cols = [date_cols[i] for i in sort_order]\n",
    "dates = parsed_dates[sort_order]\n",
    "date_strs = [d.strftime('%Y-%m-%d') for d in dates]\n",
    "\n",
    "# Convert date columns to numeric\n",
    "for col in date_cols:\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "# Filter to stock rows only (exclude benchmarks)\n",
    "stocks_df = df[df['PERMNO'].notna()].copy()\n",
    "\n",
    "# Function to get quarterly end dates\n",
    "def get_quarterly_dates(dates):\n",
    "    quarterly_dates = []\n",
    "    df_dates = pd.DataFrame({'date': dates})\n",
    "    df_dates['year'] = df_dates['date'].dt.year\n",
    "    df_dates['quarter'] = df_dates['date'].dt.quarter\n",
    "    quarterly_ends = df_dates.groupby(['year', 'quarter'])['date'].max()\n",
    "    return quarterly_ends.tolist()\n",
    "\n",
    "quarterly_rebalance_dates = get_quarterly_dates(dates)\n",
    "print(f\"Total quarterly dates available: {len(quarterly_rebalance_dates)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36d15ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: HRP Functions\n",
    "def get_correlation_distance(corr):\n",
    "    return ((1 - corr) / 2) ** 0.5\n",
    "\n",
    "def get_quasi_diag(link):\n",
    "    link = link.astype(int)\n",
    "    sort_ix = pd.Series([link[-1, 0], link[-1, 1]])\n",
    "    num_items = link[-1, 3]\n",
    "    while sort_ix.max() >= num_items:\n",
    "        sort_ix.index = range(0, sort_ix.shape[0] * 2, 2)\n",
    "        df0 = sort_ix[sort_ix >= num_items]\n",
    "        i = df0.index\n",
    "        j = df0.values - num_items\n",
    "        sort_ix[i] = link[j, 0]\n",
    "        df0 = pd.Series(link[j, 1], index=i + 1)\n",
    "        sort_ix = pd.concat([sort_ix, df0])\n",
    "        sort_ix = sort_ix.sort_index()\n",
    "        sort_ix.index = range(sort_ix.shape[0])\n",
    "    return sort_ix.tolist()\n",
    "\n",
    "def get_cluster_var(cov, c_items):\n",
    "    cov_ = cov.loc[c_items, c_items]\n",
    "    ivp = 1 / np.diag(cov_)\n",
    "    ivp /= ivp.sum()\n",
    "    return (ivp @ cov_ @ ivp)\n",
    "\n",
    "def get_recursive_bisection(cov, sort_ix):\n",
    "    w = pd.Series(1.0, index=sort_ix)\n",
    "    c_items = [sort_ix]\n",
    "    while len(c_items) > 0:\n",
    "        c_items = [i[j:k] for i in c_items for j, k in ((0, len(i) // 2), (len(i) // 2, len(i))) if len(i) > 1]\n",
    "        for i in range(0, len(c_items), 2):\n",
    "            c_items0 = c_items[i]\n",
    "            c_items1 = c_items[i + 1]\n",
    "            c_var0 = get_cluster_var(cov, c_items0)\n",
    "            c_var1 = get_cluster_var(cov, c_items1)\n",
    "            alpha = 1 - c_var0 / (c_var0 + c_var1)\n",
    "            w[c_items0] *= alpha\n",
    "            w[c_items1] *= 1 - alpha\n",
    "    # Normalize to ensure weights sum to exactly 1.0 (fix numerical drift)\n",
    "    w = w / w.sum()\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d70ef14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Process ONE RANDOM Quarterly Rebalance (QuickTest)\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "# Optimized Euclidean distance function\n",
    "def get_euclidean_distance(dist):\n",
    "    return pd.DataFrame(euclidean_distances(dist), index=dist.index, columns=dist.columns)\n",
    "\n",
    "# Select ONE random quarterly date\n",
    "random.seed(42)  # For reproducibility, remove this line for truly random selection\n",
    "rebal_date = random.choice(quarterly_rebalance_dates)\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"QUICK TEST - Processing ONE random window\")\n",
    "print(f\"Selected rebalance date: {rebal_date.strftime('%Y-%m-%d')}\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "rebal_str = rebal_date.strftime('%Y-%m-%d')\n",
    "\n",
    "# Find the start of the 12-month lookback\n",
    "start_date = rebal_date - relativedelta(months=11)  # 12 periods including rebal_date\n",
    "window_dates = [d for d in dates if start_date <= d <= rebal_date]\n",
    "\n",
    "if len(window_dates) < 12:\n",
    "    print(f\"ERROR: Insufficient data for 12-month window (only {len(window_dates)} periods)\")\n",
    "else:\n",
    "    print(f\"Window period: {window_dates[0].strftime('%Y-%m-%d')} to {window_dates[-1].strftime('%Y-%m-%d')}\")\n",
    "    print(f\"Number of periods in window: {len(window_dates)}\")\n",
    "    \n",
    "    window_strs = [d.strftime('%Y-%m-%d') for d in window_dates]\n",
    "    \n",
    "    # Map to actual mangled column names\n",
    "    window_indices = [date_strs.index(s) for s in window_strs]\n",
    "    actual_window_cols = [date_cols[i] for i in window_indices]\n",
    "    \n",
    "    # Select window data\n",
    "    window_df = stocks_df[['PERMNO', 'Company_Ticker'] + actual_window_cols].copy()\n",
    "    \n",
    "    # Drop rows with NaN Company_Ticker if any\n",
    "    window_df = window_df[window_df['Company_Ticker'].notna()]\n",
    "    \n",
    "    # Filter stocks with at least 12 valid (non-NaN) values in the window\n",
    "    valid_mask = window_df[actual_window_cols].notna().sum(axis=1) >= 12\n",
    "    window_df = window_df[valid_mask]\n",
    "    \n",
    "    print(f\"Number of stocks with sufficient data: {len(window_df)}\")\n",
    "    \n",
    "    if len(window_df) < 2:\n",
    "        print(f\"ERROR: Fewer than 2 valid stocks\")\n",
    "    else:\n",
    "        # Save the rolling window CSV\n",
    "        window_path = os.path.join(rolling_dir, f'rolling_window_{rebal_str}_quicktest.csv')\n",
    "        window_df.to_csv(window_path, index=False)\n",
    "        print(f\"✓ Saved rolling window to {window_path}\\n\")\n",
    "        \n",
    "        # Compute HRP weights using PERMNO as unique index\n",
    "        returns = window_df[actual_window_cols].T  # Time x Assets\n",
    "        returns.columns = window_df['PERMNO'].astype(str)  # Use PERMNO as unique columns (str)\n",
    "        \n",
    "        print(f\"Computing HRP weights for {len(returns.columns)} stocks...\")\n",
    "        \n",
    "        # Compute shrunk covariance using Ledoit-Wolf\n",
    "        lw = LedoitWolf().fit(returns)\n",
    "        cov_array = lw.covariance_\n",
    "        cov = pd.DataFrame(cov_array, index=returns.columns, columns=returns.columns)\n",
    "        \n",
    "        # Compute correlation from shrunk cov\n",
    "        std = np.sqrt(np.diag(cov_array))\n",
    "        corr_array = cov_array / np.outer(std, std)\n",
    "        corr = pd.DataFrame(corr_array, index=returns.columns, columns=returns.columns)\n",
    "        \n",
    "        # Distances\n",
    "        dist = get_correlation_distance(corr)\n",
    "        eucl_dist = get_euclidean_distance(dist)\n",
    "        \n",
    "        # Clustering\n",
    "        link = sch.linkage(eucl_dist, method='single')\n",
    "        \n",
    "        # Seriation\n",
    "        sort_ix = get_quasi_diag(link)\n",
    "        sort_ix = returns.columns[sort_ix].tolist()  # List of str(PERMNO)\n",
    "        \n",
    "        # Weights\n",
    "        hrp_weights = get_recursive_bisection(cov, sort_ix)\n",
    "        \n",
    "        # Map back to Company_Ticker\n",
    "        permno_to_ticker = dict(zip(window_df['PERMNO'].astype(str), window_df['Company_Ticker']))\n",
    "        \n",
    "        # Store with Company_Ticker\n",
    "        weights_df = pd.DataFrame({'PERMNO': hrp_weights.index, 'Weight': hrp_weights.values})\n",
    "        weights_df['Company_Ticker'] = weights_df['PERMNO'].map(permno_to_ticker)\n",
    "        weights_df = weights_df[['Company_Ticker', 'PERMNO', 'Weight']]\n",
    "        weights_df['Rebalance_Date'] = rebal_str\n",
    "        \n",
    "        # Save weights\n",
    "        weights_path = os.path.join(rolling_dir, f'hrp_weights_quicktest_{rebal_str}.csv')\n",
    "        weights_df.to_csv(weights_path, index=False)\n",
    "        print(f\"✓ Saved HRP weights to {weights_path}\\n\")\n",
    "        \n",
    "        # Display summary statistics\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"WEIGHT STATISTICS\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Total number of stocks: {len(weights_df)}\")\n",
    "        print(f\"Sum of weights: {weights_df['Weight'].sum():.6f}\")\n",
    "        print(f\"\\nWeight distribution:\")\n",
    "        print(weights_df['Weight'].describe())\n",
    "        print(f\"\\nTop 10 holdings:\")\n",
    "        print(weights_df.nlargest(10, 'Weight')[['Company_Ticker', 'Weight']])\n",
    "        print(f\"\\nBottom 10 holdings:\")\n",
    "        print(weights_df.nsmallest(10, 'Weight')[['Company_Ticker', 'Weight']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18e263e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Visualize the Dendrogram (Optional)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if 'link' in locals():\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    sch.dendrogram(link, labels=returns.columns.tolist(), leaf_font_size=8)\n",
    "    plt.title(f'Hierarchical Clustering Dendrogram - {rebal_str}')\n",
    "    plt.xlabel('Stock PERMNO')\n",
    "    plt.ylabel('Distance')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No clustering data available to plot.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e585512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Visualize Weight Distribution\n",
    "if 'weights_df' in locals():\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Histogram\n",
    "    axes[0].hist(weights_df['Weight'], bins=50, edgecolor='black')\n",
    "    axes[0].set_xlabel('Weight')\n",
    "    axes[0].set_ylabel('Frequency')\n",
    "    axes[0].set_title('Distribution of HRP Weights')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Sorted weights plot\n",
    "    sorted_weights = weights_df.sort_values('Weight', ascending=False).reset_index(drop=True)\n",
    "    axes[1].plot(sorted_weights.index, sorted_weights['Weight'], marker='o', markersize=2, linestyle='-')\n",
    "    axes[1].set_xlabel('Stock Rank')\n",
    "    axes[1].set_ylabel('Weight')\n",
    "    axes[1].set_title('Sorted HRP Weights')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No weights data available to plot.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
