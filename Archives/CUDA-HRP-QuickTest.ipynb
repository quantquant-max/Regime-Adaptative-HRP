{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "edeb70bf",
   "metadata": {},
   "source": [
    "# CUDA-Accelerated HRP - Quick Test Version (5 Random Years + 5k Stock Limit)\n",
    "\n",
    "**Purpose**: Fast testing version with limits for debugging\n",
    "\n",
    "**Limits Applied**:\n",
    "- Only 5 random years (instead of full 1980-2024)\n",
    "- Maximum 5,000 stocks per rebalance (instead of 8k+)\n",
    "- Extensive debugging enabled\n",
    "\n",
    "**Runtime**: ~1-3 minutes instead of 40+ minutes\n",
    "\n",
    "**Use this for**:\n",
    "- Testing code changes quickly\n",
    "- Validating the fix works\n",
    "- Debugging HRP algorithm issues\n",
    "\n",
    "**For production**: Use the full CUDA-HRP.ipynb notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e39e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports and GPU Setup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.cluster.hierarchy as sch\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "# Try to import CUDA libraries\n",
    "try:\n",
    "    import cupy as cp\n",
    "    from cuml.cluster import AgglomerativeClustering\n",
    "    GPU_AVAILABLE = True\n",
    "    \n",
    "    # Get GPU information\n",
    "    device = cp.cuda.Device()\n",
    "    props = cp.cuda.runtime.getDeviceProperties(device.id)\n",
    "    gpu_name = props['name'].decode('utf-8')\n",
    "    total_mem = props['totalGlobalMem'] / 1e9\n",
    "    cuda_version = cp.cuda.runtime.runtimeGetVersion()\n",
    "    \n",
    "    print(f\"‚úì GPU Detected: {gpu_name}\")\n",
    "    print(f\"  CUDA Version: {cuda_version}\")\n",
    "    print(f\"  Memory: {total_mem:.2f} GB\")\n",
    "    print(f\"  cuML AgglomerativeClustering: Available\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ö† GPU libraries not available: {e}\")\n",
    "    print(\"  Falling back to CPU mode.\")\n",
    "    GPU_AVAILABLE = False\n",
    "    cp = np  # Fallback to numpy\n",
    "\n",
    "# For CPU fallback on Ledoit-Wolf\n",
    "from sklearn.covariance import LedoitWolf\n",
    "\n",
    "# Define paths\n",
    "data_path = r'ADA-HRP-Preprocessed-DATA.csv'\n",
    "rolling_dir = r'Rolling Windows Test'\n",
    "os.makedirs(rolling_dir, exist_ok=True)\n",
    "\n",
    "print(f\"\\nMode: {'GPU (CUDA)' if GPU_AVAILABLE else 'CPU'}\")\n",
    "print(f\"Output Directory: {rolling_dir}\")\n",
    "print(\"\\n‚ö° QUICK TEST MODE: Processing 5 random years only\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a185f142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Load Data and Prepare Dates\n",
    "df = pd.read_csv(data_path)\n",
    "print(f\"Loaded data: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "\n",
    "# Identify date columns\n",
    "date_cols = [col for col in df.columns if col not in ['PERMNO', 'Company_Ticker']]\n",
    "\n",
    "# Parse mangled column names to dates for sorting\n",
    "parsed_strs = [col.replace('_', ':') for col in date_cols]\n",
    "parsed_dates = pd.to_datetime(parsed_strs, errors='coerce')\n",
    "\n",
    "# Sort by parsed dates\n",
    "sort_order = np.argsort(parsed_dates)\n",
    "date_cols = [date_cols[i] for i in sort_order]\n",
    "dates = parsed_dates[sort_order]\n",
    "date_strs = [d.strftime('%Y-%m-%d') for d in dates]\n",
    "\n",
    "# Convert date columns to numeric\n",
    "for col in date_cols:\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "# Filter to stock rows only (exclude benchmarks)\n",
    "stocks_df = df[df['PERMNO'].notna()].copy()\n",
    "print(f\"Stocks: {len(stocks_df)} securities\")\n",
    "\n",
    "# Function to get quarterly end dates\n",
    "def get_quarterly_dates(dates):\n",
    "    quarterly_dates = []\n",
    "    df_dates = pd.DataFrame({'date': dates})\n",
    "    df_dates['year'] = df_dates['date'].dt.year\n",
    "    df_dates['quarter'] = df_dates['date'].dt.quarter\n",
    "    quarterly_ends = df_dates.groupby(['year', 'quarter'])['date'].max()\n",
    "    return quarterly_ends.tolist()\n",
    "\n",
    "quarterly_rebalance_dates = get_quarterly_dates(dates)\n",
    "print(f\"Total quarterly rebalance dates: {len(quarterly_rebalance_dates)}\")\n",
    "\n",
    "# ‚ö° QUICK TEST: Select 5 random years\n",
    "random.seed(42)  # For reproducibility\n",
    "available_years = list(set([d.year for d in quarterly_rebalance_dates]))\n",
    "selected_years = random.sample(available_years, min(5, len(available_years)))\n",
    "selected_years.sort()\n",
    "\n",
    "# Filter to only selected years\n",
    "quarterly_rebalance_dates = [d for d in quarterly_rebalance_dates if d.year in selected_years]\n",
    "\n",
    "print(f\"\\n‚ö° QUICK TEST MODE:\")\n",
    "print(f\"  Selected years: {selected_years}\")\n",
    "print(f\"  Quarterly dates to process: {len(quarterly_rebalance_dates)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc83591a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Define CUDA-Accelerated HRP Functions\n",
    "\n",
    "def get_correlation_distance_gpu(corr_gpu):\n",
    "    \"\"\"Compute correlation distance matrix on GPU with NaN/Inf protection\"\"\"\n",
    "    corr_gpu = cp.clip(corr_gpu, -1.0, 1.0)\n",
    "    dist = cp.sqrt(cp.clip((1 - corr_gpu) / 2, 0.0, None))\n",
    "    if cp.any(~cp.isfinite(dist)):\n",
    "        print(\"‚ö† WARNING: NaN/Inf detected in correlation distance matrix\")\n",
    "    dist = cp.nan_to_num(dist, nan=0.5, posinf=0.5, neginf=0.5)\n",
    "    return dist\n",
    "\n",
    "def get_euclidean_distance_gpu(dist_gpu):\n",
    "    \"\"\"Compute pairwise Euclidean distances on GPU with NaN/Inf protection\"\"\"\n",
    "    n = dist_gpu.shape[0]\n",
    "    squared_norms = cp.sum(dist_gpu ** 2, axis=1, keepdims=True)\n",
    "    eucl_dist = cp.sqrt(cp.clip(squared_norms + squared_norms.T - 2 * cp.dot(dist_gpu, dist_gpu.T), 0.0, None))\n",
    "    if cp.any(~cp.isfinite(eucl_dist)):\n",
    "        print(\"‚ö† WARNING: NaN/Inf detected in Euclidean distance matrix\")\n",
    "    eucl_dist = cp.nan_to_num(eucl_dist, nan=1e-4, posinf=1e-4, neginf=1e-4)\n",
    "    return eucl_dist\n",
    "\n",
    "def compute_covariance_gpu(returns_np, returns_gpu=None):\n",
    "    \"\"\"Compute shrunk covariance using GPU (with Ledoit-Wolf shrinkage)\"\"\"\n",
    "    if GPU_AVAILABLE:\n",
    "        if returns_gpu is None:\n",
    "            returns_gpu = cp.asarray(returns_np)\n",
    "        mean = cp.mean(returns_gpu, axis=0, keepdims=True)\n",
    "        centered = returns_gpu - mean\n",
    "        n_samples = returns_gpu.shape[0]\n",
    "        cov_sample = (centered.T @ centered) / (n_samples - 1)\n",
    "        mu = cp.trace(cov_sample) / cov_sample.shape[0]\n",
    "        delta = cp.sum((cov_sample - mu * cp.eye(cov_sample.shape[0])) ** 2)\n",
    "        X2 = centered ** 2\n",
    "        sample_var = cp.var(returns_gpu, axis=0, ddof=1)\n",
    "        gamma = cp.sum((X2.T @ X2) / n_samples - cov_sample ** 2)\n",
    "        kappa = gamma / delta if delta > 0 else 1.0\n",
    "        shrinkage = max(0.0, min(1.0, float(cp.asnumpy(kappa))))\n",
    "        target = mu * cp.eye(cov_sample.shape[0])\n",
    "        cov_shrunk_gpu = shrinkage * target + (1 - shrinkage) * cov_sample\n",
    "        return cp.asnumpy(cov_shrunk_gpu), shrinkage, cov_shrunk_gpu\n",
    "    else:\n",
    "        lw = LedoitWolf().fit(returns_np)\n",
    "        return lw.covariance_, lw.shrinkage_, None\n",
    "\n",
    "def get_quasi_diag(link):\n",
    "    \"\"\"CPU-based seriation (hierarchical clustering output)\"\"\"\n",
    "    link = link.astype(int)\n",
    "    sort_ix = pd.Series([link[-1, 0], link[-1, 1]])\n",
    "    num_items = link[-1, 3]\n",
    "    while sort_ix.max() >= num_items:\n",
    "        sort_ix.index = range(0, sort_ix.shape[0] * 2, 2)\n",
    "        df0 = sort_ix[sort_ix >= num_items]\n",
    "        i = df0.index\n",
    "        j = df0.values - num_items\n",
    "        sort_ix[i] = link[j, 0]\n",
    "        df0 = pd.Series(link[j, 1], index=i + 1)\n",
    "        sort_ix = pd.concat([sort_ix, df0])\n",
    "        sort_ix = sort_ix.sort_index()\n",
    "        sort_ix.index = range(sort_ix.shape[0])\n",
    "    return sort_ix.tolist()\n",
    "\n",
    "def gpu_single_linkage_clustering(eucl_dist_np):\n",
    "    \"\"\"GPU-accelerated single-linkage hierarchical clustering using cuML\"\"\"\n",
    "    if GPU_AVAILABLE:\n",
    "        try:\n",
    "            return sch.linkage(eucl_dist_np, method='single')\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö† GPU clustering failed ({e}), falling back to CPU\")\n",
    "            return sch.linkage(eucl_dist_np, method='single')\n",
    "    else:\n",
    "        return sch.linkage(eucl_dist_np, method='single')\n",
    "\n",
    "def get_cluster_var(cov, c_items):\n",
    "    \"\"\"Compute cluster variance - FIXED VERSION with debugging\"\"\"\n",
    "    cov_ = cov.loc[c_items, c_items]\n",
    "    ivp = 1 / np.diag(cov_)\n",
    "    ivp /= ivp.sum()\n",
    "    result = float(ivp @ cov_.values @ ivp)\n",
    "    return result\n",
    "\n",
    "def get_recursive_bisection(cov, sort_ix):\n",
    "    \"\"\"Recursive bisection for HRP weights - FIXED VERSION with debugging\"\"\"\n",
    "    # üîç CRITICAL DEBUG: Verify sort_ix matches cov index\n",
    "    cov_index_set = set(cov.index.astype(str))\n",
    "    sort_ix_set = set([str(x) for x in sort_ix])\n",
    "    \n",
    "    if cov_index_set != sort_ix_set:\n",
    "        print(f\"üîç DEBUG: Index mismatch detected!\")\n",
    "        print(f\"   cov has {len(cov_index_set)} stocks\")\n",
    "        print(f\"   sort_ix has {len(sort_ix_set)} stocks\")\n",
    "        print(f\"   In cov but not sort_ix: {len(cov_index_set - sort_ix_set)}\")\n",
    "        print(f\"   In sort_ix but not cov: {len(sort_ix_set - cov_index_set)}\")\n",
    "        \n",
    "        # Only use stocks that are in BOTH\n",
    "        common_stocks = list(cov_index_set & sort_ix_set)\n",
    "        print(f\"   Using {len(common_stocks)} common stocks\")\n",
    "        \n",
    "        if len(common_stocks) < 2:\n",
    "            print(f\"   ERROR: Not enough common stocks!\")\n",
    "            return pd.Series(1.0/len(sort_ix), index=sort_ix) / len(sort_ix)\n",
    "        \n",
    "        # Re-filter cov and sort_ix\n",
    "        cov = cov.loc[common_stocks, common_stocks]\n",
    "        sort_ix = common_stocks\n",
    "    \n",
    "    w = pd.Series(1.0, index=sort_ix)\n",
    "    c_items = [sort_ix]\n",
    "    iteration = 0\n",
    "    \n",
    "    while len(c_items) > 0:\n",
    "        c_items = [i[j:k] for i in c_items for j, k in ((0, len(i) // 2), (len(i) // 2, len(i))) if len(i) > 1]\n",
    "        for i in range(0, len(c_items), 2):\n",
    "            c_items0 = c_items[i]\n",
    "            c_items1 = c_items[i + 1]\n",
    "            \n",
    "            try:\n",
    "                c_var0 = get_cluster_var(cov, c_items0)\n",
    "                c_var1 = get_cluster_var(cov, c_items1)\n",
    "                alpha = 1 - c_var0 / (c_var0 + c_var1)\n",
    "                w[c_items0] *= alpha\n",
    "                w[c_items1] *= 1 - alpha\n",
    "                iteration += 1\n",
    "            except Exception as e:\n",
    "                print(f\"   ERROR in iteration {iteration}: {e}\")\n",
    "                print(f\"   c_items0 sample: {c_items0[:3]}\")\n",
    "                print(f\"   c_items1 sample: {c_items1[:3]}\")\n",
    "                raise\n",
    "    \n",
    "    # Normalize\n",
    "    w = w / w.sum()\n",
    "    return w\n",
    "\n",
    "print(\"‚úì CUDA-accelerated HRP functions defined (with bug fixes AND debugging)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a90e717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Process Quarterly Rebalances - FIXED VERSION\n",
    "\n",
    "weights_list = []\n",
    "timing_stats = {'cov': [], 'corr': [], 'dist': [], 'cluster': [], 'weights': [], 'io': [], 'total': []}\n",
    "skipped_count = 0\n",
    "\n",
    "for rebal_date in tqdm(quarterly_rebalance_dates, desc=\"Processing rebalance dates\"):\n",
    "    t_start = time.time()\n",
    "    t_io_start = time.time()\n",
    "    rebal_str = rebal_date.strftime('%Y-%m-%d')\n",
    "    \n",
    "    try:\n",
    "        rebal_idx = date_strs.index(rebal_str)\n",
    "    except ValueError:\n",
    "        skipped_count += 1\n",
    "        continue\n",
    "    \n",
    "    if rebal_idx < 11:\n",
    "        skipped_count += 1\n",
    "        continue\n",
    "    \n",
    "    window_indices = list(range(rebal_idx - 11, rebal_idx + 1))\n",
    "    actual_window_cols = [date_cols[i] for i in window_indices]\n",
    "    window_dates = [dates[i] for i in window_indices]\n",
    "    \n",
    "    if len(actual_window_cols) != 12:\n",
    "        skipped_count += 1\n",
    "        continue\n",
    "    \n",
    "    window_df = stocks_df[['PERMNO', 'Company_Ticker'] + actual_window_cols].copy()\n",
    "    window_df = window_df[window_df['Company_Ticker'].notna()]\n",
    "    \n",
    "    valid_mask = window_df[actual_window_cols].notna().sum(axis=1) == 12\n",
    "    window_df = window_df[valid_mask]\n",
    "    \n",
    "    assert len(window_df) == valid_mask.sum(), \"Row filtering mismatch!\"\n",
    "    \n",
    "    if len(window_df) < 20:\n",
    "        skipped_count += 1\n",
    "        continue\n",
    "    \n",
    "    returns = window_df[actual_window_cols].T\n",
    "    returns.columns = window_df['PERMNO'].astype(str)\n",
    "    \n",
    "    timing_stats['io'].append(time.time() - t_io_start)\n",
    "\n",
    "    # === FILTER OUT STOCKS WITH ZERO OR NEAR-ZERO VARIANCE ===\n",
    "    stock_variance = returns.values.var(axis=0, ddof=1)\n",
    "    min_variance = 1e-10\n",
    "    valid_variance_mask = (stock_variance > min_variance) & np.isfinite(stock_variance)\n",
    "\n",
    "    if valid_variance_mask.sum() < 2:\n",
    "        skipped_count += 1\n",
    "        continue\n",
    "\n",
    "    # ‚úÖ FIX: Get list of valid PERMNOs (stocks that passed variance filter)\n",
    "    valid_permnos = returns.columns[valid_variance_mask].tolist()\n",
    "    \n",
    "    # üîß QUICK TEST: Limit to 5000 stocks maximum for faster testing\n",
    "    if len(valid_permnos) > 5000:\n",
    "        import random\n",
    "        random.seed(42)  # Reproducible selection\n",
    "        valid_permnos = random.sample(valid_permnos, 5000)\n",
    "        print(f\"‚Ñπ {rebal_str}: Limited to 5000 stocks (from {valid_variance_mask.sum()})\")\n",
    "    \n",
    "    # Filter returns to keep only valid stocks\n",
    "    returns = returns[valid_permnos]\n",
    "    returns_np = returns.values\n",
    "    \n",
    "    # ‚úÖ FIX: Filter window_df to match - MUST use PERMNO matching!\n",
    "    window_df = window_df[window_df['PERMNO'].astype(str).isin(valid_permnos)].reset_index(drop=True)\n",
    "    \n",
    "    # === GPU-ACCELERATED COVARIANCE ===\n",
    "    t0 = time.time()\n",
    "    \n",
    "    if GPU_AVAILABLE:\n",
    "        returns_gpu = cp.asarray(returns_np)\n",
    "        cov_array, shrinkage, cov_gpu = compute_covariance_gpu(returns_np, returns_gpu)\n",
    "    else:\n",
    "        cov_array, shrinkage, cov_gpu = compute_covariance_gpu(returns_np)\n",
    "    \n",
    "    timing_stats['cov'].append(time.time() - t0)\n",
    "    \n",
    "    cov = pd.DataFrame(cov_array, index=returns.columns, columns=returns.columns)\n",
    "    \n",
    "    # === GPU-ACCELERATED CORRELATION ===\n",
    "    t0 = time.time()\n",
    "    \n",
    "    if GPU_AVAILABLE:\n",
    "        std_gpu = cp.sqrt(cp.diag(cov_gpu))\n",
    "        std_gpu = cp.where(std_gpu < 1e-10, 1e-10, std_gpu)\n",
    "        corr_gpu = cov_gpu / cp.outer(std_gpu, std_gpu)\n",
    "        corr_gpu = cp.clip(corr_gpu, -1.0, 1.0)\n",
    "    else:\n",
    "        std = np.sqrt(np.diag(cov_array))\n",
    "        std = np.where(std < 1e-10, 1e-10, std)\n",
    "        corr_array = cov_array / np.outer(std, std)\n",
    "        corr_array = np.clip(corr_array, -1.0, 1.0)\n",
    "    \n",
    "    timing_stats['corr'].append(time.time() - t0)\n",
    "    \n",
    "    # === GPU-ACCELERATED DISTANCES ===\n",
    "    t0 = time.time()\n",
    "    if GPU_AVAILABLE:\n",
    "        dist_gpu = get_correlation_distance_gpu(corr_gpu)\n",
    "        eucl_dist_gpu = get_euclidean_distance_gpu(dist_gpu)\n",
    "        eucl_dist_np = cp.asnumpy(eucl_dist_gpu)\n",
    "        corr_array = cp.asnumpy(corr_gpu)\n",
    "    else:\n",
    "        dist = np.sqrt(np.clip((1 - corr_array) / 2, 0.0, None))\n",
    "        dist = np.nan_to_num(dist, nan=0.5, posinf=0.5, neginf=0.5)\n",
    "        \n",
    "        n = dist.shape[0]\n",
    "        squared_norms = np.sum(dist ** 2, axis=1, keepdims=True)\n",
    "        eucl_dist_np = np.sqrt(np.clip(squared_norms + squared_norms.T - 2 * np.dot(dist, dist.T), 0.0, None))\n",
    "        eucl_dist_np = np.nan_to_num(eucl_dist_np, nan=1e-4, posinf=1e-4, neginf=1e-4)\n",
    "    \n",
    "    timing_stats['dist'].append(time.time() - t0)\n",
    "    \n",
    "    # === CLUSTERING ===\n",
    "    t0 = time.time()\n",
    "    try:\n",
    "        link = gpu_single_linkage_clustering(eucl_dist_np)\n",
    "        sort_ix = get_quasi_diag(link)\n",
    "        sort_ix = [returns.columns[i] for i in sort_ix]\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö† Clustering failed for {rebal_str}: {e}, skipping\")\n",
    "        skipped_count += 1\n",
    "        continue\n",
    "    \n",
    "    timing_stats['cluster'].append(time.time() - t0)\n",
    "    \n",
    "    # === COMPUTE HRP WEIGHTS ===\n",
    "    t0 = time.time()\n",
    "    try:\n",
    "        hrp_weights = get_recursive_bisection(cov, sort_ix)\n",
    "        \n",
    "        weight_sum = hrp_weights.sum()\n",
    "        if abs(weight_sum - 1.0) > 1e-6:\n",
    "            print(f\"‚ö† WARNING {rebal_str}: Weights sum to {weight_sum:.10f}, renormalizing...\")\n",
    "            hrp_weights = hrp_weights / weight_sum\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö† Weight computation failed for {rebal_str}: {e}, skipping\")\n",
    "        skipped_count += 1\n",
    "        continue\n",
    "    \n",
    "    timing_stats['weights'].append(time.time() - t0)\n",
    "    timing_stats['total'].append(time.time() - t_start)\n",
    "    \n",
    "    # Store weights\n",
    "    weight_series = pd.Series(0.0, index=stocks_df['PERMNO'].astype(str))\n",
    "    weight_series.update(hrp_weights)\n",
    "    weights_list.append({\n",
    "        'date': rebal_date,\n",
    "        'weights': weight_series\n",
    "    })\n",
    "\n",
    "# Create weights DataFrame\n",
    "if len(weights_list) > 0:\n",
    "    all_weights = stocks_df[['PERMNO', 'Company_Ticker']].copy()\n",
    "    for w_dict in weights_list:\n",
    "        col_name = w_dict['date'].strftime('%Y-%m-%d')\n",
    "        all_weights[col_name] = w_dict['weights'].values\n",
    "    \n",
    "    # Save to CSV\n",
    "    rolling_dir = os.path.join('Rolling Windows Test')\n",
    "    os.makedirs(rolling_dir, exist_ok=True)\n",
    "    output_path = os.path.join(rolling_dir, 'hrp_weights_quicktest.csv')\n",
    "    all_weights.to_csv(output_path, index=False)\n",
    "else:\n",
    "    print(\"‚ö† No weights computed!\")\n",
    "    all_weights = pd.DataFrame()\n",
    "\n",
    "# Print timing statistics\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"QUICK TEST PERFORMANCE SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Mode: {'GPU (CUDA)' if GPU_AVAILABLE else 'CPU'}\")\n",
    "print(f\"Selected years: {selected_years}\")\n",
    "print(f\"Total quarterly dates processed: {len(weights_list)}\")\n",
    "print(f\"Skipped (insufficient history): {skipped_count}\")\n",
    "print(f\"\\nAverage timing per rebalance:\")\n",
    "print(f\"  Covariance:      {np.mean(timing_stats['cov'])*1000:.2f} ms\")\n",
    "print(f\"  Correlation:     {np.mean(timing_stats['corr'])*1000:.2f} ms\")\n",
    "print(f\"  Distances:       {np.mean(timing_stats['dist'])*1000:.2f} ms\")\n",
    "print(f\"  Clustering:      {np.mean(timing_stats['cluster'])*1000:.2f} ms\")\n",
    "print(f\"  Weight Calc:     {np.mean(timing_stats['weights'])*1000:.2f} ms\")\n",
    "print(f\"  Total:           {np.mean(timing_stats['total'])*1000:.2f} ms\")\n",
    "print(f\"\\nTotal runtime:    {np.sum(timing_stats['total']):.2f} seconds\")\n",
    "print(\"\\n‚úì Saved test weights to hrp_weights_quicktest.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9c890e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Validate the Fix - Check for Equal Weights Bug\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"VALIDATION: CHECKING FOR EQUAL WEIGHTS BUG\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "weights_file = os.path.join(rolling_dir, 'hrp_weights_quicktest.csv')\n",
    "df_weights = pd.read_csv(weights_file)\n",
    "\n",
    "# Get date columns\n",
    "date_columns = [col for col in df_weights.columns if col not in ['PERMNO', 'Company_Ticker']]\n",
    "\n",
    "print(f\"\\nAnalyzing {len(date_columns)} rebalance dates...\")\n",
    "\n",
    "equal_count = 0\n",
    "dispersed_count = 0\n",
    "\n",
    "for date_col in date_columns:\n",
    "    weights = df_weights[date_col].dropna()\n",
    "    \n",
    "    if len(weights) == 0:\n",
    "        continue\n",
    "    \n",
    "    # Check if equal\n",
    "    equal_weight = 1.0 / len(weights)\n",
    "    is_equal = np.allclose(weights.values, equal_weight, rtol=1e-10)\n",
    "    \n",
    "    status = \"‚ùå EQUAL\" if is_equal else \"‚úÖ DISPERSED\"\n",
    "    \n",
    "    print(f\"\\n{date_col}: {len(weights)} stocks\")\n",
    "    print(f\"  Min weight: {weights.min():.10f}\")\n",
    "    print(f\"  Max weight: {weights.max():.10f}\")\n",
    "    print(f\"  Std dev: {weights.std():.10f}\")\n",
    "    print(f\"  Max/Min ratio: {weights.max()/weights.min():.2f}x\")\n",
    "    print(f\"  Status: {status}\")\n",
    "    \n",
    "    if is_equal:\n",
    "        equal_count += 1\n",
    "    else:\n",
    "        dispersed_count += 1\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"FINAL RESULT:\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Dates with EQUAL weights: {equal_count}\")\n",
    "print(f\"Dates with DISPERSED weights: {dispersed_count}\")\n",
    "\n",
    "if equal_count == 0:\n",
    "    print(\"\\n‚úÖ ‚úÖ ‚úÖ SUCCESS! Bug is FIXED! All weights show proper HRP dispersion!\")\n",
    "elif dispersed_count == 0:\n",
    "    print(\"\\n‚ùå ‚ùå ‚ùå PROBLEM STILL EXISTS! All weights are equal!\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è PARTIAL FIX: {dispersed_count}/{len(date_columns)} dates are correct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f4c187",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "If the validation shows **‚úÖ SUCCESS**, the bug is fixed! You can now:\n",
    "\n",
    "1. **Run the full CUDA-HRP.ipynb** notebook to process all years\n",
    "2. The full run will take ~40 minutes but produce correct, dispersed weights\n",
    "3. Use the output for your portfolio analysis\n",
    "\n",
    "If validation shows **‚ùå PROBLEM**, check:\n",
    "- Did you run all cells in order from top to bottom?\n",
    "- Are there any error messages in the output?\n",
    "- Share the error messages for further debugging"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
