{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1ef5f10",
   "metadata": {},
   "source": [
    "# CUDA-Accelerated HRP - 4 Quarters Test Version\n",
    "\n",
    "This notebook processes **only 4 quarterly rebalances** (2016 Q1-Q4) for quick testing.\n",
    "\n",
    "**Processing:**\n",
    "- 2016-03-31 (Q1)\n",
    "- 2016-06-30 (Q2)\n",
    "- 2016-09-30 (Q3)\n",
    "- 2016-12-30 (Q4)\n",
    "\n",
    "**Features:**\n",
    "- ‚úÖ FIXED `get_cluster_var()` function (proper matrix multiplication)\n",
    "- ‚ö° Full CUDA acceleration for covariance, correlation, and distances\n",
    "- üîç Built-in validation to verify weights are diverse (not equal)\n",
    "\n",
    "**Output:** `Rolling Windows Test 4Q/hrp_weights_4quarters.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b98261c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö† GPU libraries not available: No module named 'cupy'\n",
      "  Falling back to CPU mode\n",
      "\n",
      "Mode: CPU\n",
      "Output Directory: C:/Users/lucas/OneDrive/Bureau/HRP/DATA (CRSP)/PREPROCESSED DATA/Rolling Windows Test 4Q\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports and GPU Setup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.cluster.hierarchy as sch\n",
    "from sklearn.covariance import LedoitWolf\n",
    "from scipy.spatial.distance import squareform\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Try to import GPU libraries\n",
    "GPU_AVAILABLE = False\n",
    "try:\n",
    "    import cupy as cp\n",
    "    from cuml.cluster import AgglomerativeClustering\n",
    "    GPU_AVAILABLE = True\n",
    "    \n",
    "    device = cp.cuda.Device()\n",
    "    props = cp.cuda.runtime.getDeviceProperties(device.id)\n",
    "    gpu_name = props['name'].decode('utf-8')\n",
    "    total_mem = props['totalGlobalMem'] / 1e9\n",
    "    cuda_version = cp.cuda.runtime.runtimeGetVersion()\n",
    "    \n",
    "    print(f\"‚úì GPU Detected: {gpu_name}\")\n",
    "    print(f\"  CUDA Version: {cuda_version}\")\n",
    "    print(f\"  Memory: {total_mem:.2f} GB\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ö† GPU libraries not available: {e}\")\n",
    "    print(\"  Falling back to CPU mode\")\n",
    "    GPU_AVAILABLE = False\n",
    "    cp = np\n",
    "\n",
    "# Define paths\n",
    "data_path = r'C:/Users/lucas/OneDrive/Bureau/HRP/DATA (CRSP)/PREPROCESSED DATA/ADA-HRP-Preprocessed-DATA.csv'\n",
    "rolling_dir = r'C:/Users/lucas/OneDrive/Bureau/HRP/DATA (CRSP)/PREPROCESSED DATA/Rolling Windows Test 4Q'\n",
    "os.makedirs(rolling_dir, exist_ok=True)\n",
    "\n",
    "print(f\"\\nMode: {'GPU (CUDA)' if GPU_AVAILABLE else 'CPU'}\")\n",
    "print(f\"Output Directory: {rolling_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1543e9",
   "metadata": {},
   "source": [
    "## Define HRP Functions (FIXED VERSION)\n",
    "\n",
    "**Critical Fix:** The `get_cluster_var()` function now properly computes the quadratic form `w^T * Cov * w` using correct matrix multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "597c48a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì CUDA-accelerated HRP functions defined (FIXED VERSION)\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Define CUDA-Accelerated HRP Functions (FIXED)\n",
    "\n",
    "def get_correlation_distance_gpu(corr_gpu):\n",
    "    \"\"\"Compute correlation distance matrix on GPU with NaN/Inf protection\"\"\"\n",
    "    corr_gpu = cp.clip(corr_gpu, -1.0, 1.0)\n",
    "    dist = cp.sqrt(cp.clip((1 - corr_gpu) / 2, 0.0, None))\n",
    "    \n",
    "    if cp.any(~cp.isfinite(dist)):\n",
    "        print(\"‚ö† WARNING: NaN/Inf detected in correlation distance matrix\")\n",
    "    \n",
    "    dist = cp.nan_to_num(dist, nan=0.5, posinf=0.5, neginf=0.5)\n",
    "    return dist\n",
    "\n",
    "def get_euclidean_distance_gpu(dist_gpu):\n",
    "    \"\"\"Compute pairwise Euclidean distances on GPU with NaN/Inf protection\"\"\"\n",
    "    n = dist_gpu.shape[0]\n",
    "    squared_norms = cp.sum(dist_gpu ** 2, axis=1, keepdims=True)\n",
    "    eucl_dist = cp.sqrt(cp.clip(squared_norms + squared_norms.T - 2 * cp.dot(dist_gpu, dist_gpu.T), 0.0, None))\n",
    "    \n",
    "    if cp.any(~cp.isfinite(eucl_dist)):\n",
    "        print(\"‚ö† WARNING: NaN/Inf detected in Euclidean distance matrix\")\n",
    "    \n",
    "    eucl_dist = cp.nan_to_num(eucl_dist, nan=1e-4, posinf=1e-4, neginf=1e-4)\n",
    "    return eucl_dist\n",
    "\n",
    "def compute_covariance_gpu(returns_np, returns_gpu=None):\n",
    "    \"\"\"Compute shrunk covariance using GPU (with Ledoit-Wolf shrinkage)\"\"\"\n",
    "    if GPU_AVAILABLE:\n",
    "        if returns_gpu is None:\n",
    "            returns_gpu = cp.asarray(returns_np)\n",
    "        \n",
    "        mean = cp.mean(returns_gpu, axis=0, keepdims=True)\n",
    "        centered = returns_gpu - mean\n",
    "        n_samples = returns_gpu.shape[0]\n",
    "        cov_sample = (centered.T @ centered) / (n_samples - 1)\n",
    "        \n",
    "        mu = cp.trace(cov_sample) / cov_sample.shape[0]\n",
    "        delta = cp.sum((cov_sample - mu * cp.eye(cov_sample.shape[0])) ** 2)\n",
    "        \n",
    "        X2 = centered ** 2\n",
    "        sample_var = cp.var(returns_gpu, axis=0, ddof=1)\n",
    "        gamma = cp.sum((X2.T @ X2) / n_samples - cov_sample ** 2)\n",
    "        \n",
    "        kappa = gamma / delta if delta > 0 else 1.0\n",
    "        shrinkage = max(0.0, min(1.0, float(cp.asnumpy(kappa))))\n",
    "        \n",
    "        target = mu * cp.eye(cov_sample.shape[0])\n",
    "        cov_shrunk_gpu = shrinkage * target + (1 - shrinkage) * cov_sample\n",
    "        \n",
    "        return cp.asnumpy(cov_shrunk_gpu), shrinkage, cov_shrunk_gpu\n",
    "    else:\n",
    "        lw = LedoitWolf().fit(returns_np)\n",
    "        return lw.covariance_, lw.shrinkage_, None\n",
    "\n",
    "def get_quasi_diag(link):\n",
    "    \"\"\"CPU-based seriation (hierarchical clustering output)\"\"\"\n",
    "    link = link.astype(int)\n",
    "    sort_ix = pd.Series([link[-1, 0], link[-1, 1]])\n",
    "    num_items = link[-1, 3]\n",
    "    while sort_ix.max() >= num_items:\n",
    "        sort_ix.index = range(0, sort_ix.shape[0] * 2, 2)\n",
    "        df0 = sort_ix[sort_ix >= num_items]\n",
    "        i = df0.index\n",
    "        j = df0.values - num_items\n",
    "        sort_ix[i] = link[j, 0]\n",
    "        df0 = pd.Series(link[j, 1], index=i + 1)\n",
    "        sort_ix = pd.concat([sort_ix, df0])\n",
    "        sort_ix = sort_ix.sort_index()\n",
    "        sort_ix.index = range(sort_ix.shape[0])\n",
    "    return sort_ix.tolist()\n",
    "\n",
    "def get_cluster_var(cov, c_items):\n",
    "    \"\"\"\n",
    "    Compute cluster variance (CPU-based, small matrices)\n",
    "    \n",
    "    CRITICAL FIX: Properly handle matrix multiplication for variance calculation\n",
    "    This is the Marcos Lopez de Prado correct implementation\n",
    "    \"\"\"\n",
    "    cov_ = cov.loc[c_items, c_items]\n",
    "    \n",
    "    # Compute inverse-variance portfolio weights\n",
    "    ivp = 1 / np.diag(cov_)\n",
    "    ivp /= ivp.sum()\n",
    "    \n",
    "    # CRITICAL: Reshape to column vector for proper matrix multiplication\n",
    "    w_ = ivp.reshape(-1, 1)\n",
    "    \n",
    "    # Compute variance: w^T * Cov * w\n",
    "    # This returns a 1x1 matrix, so we extract the scalar with [0, 0]\n",
    "    cVar = np.dot(np.dot(w_.T, cov_), w_)[0, 0]\n",
    "    \n",
    "    return cVar\n",
    "\n",
    "def get_recursive_bisection(cov, sort_ix):\n",
    "    \"\"\"Recursive bisection for HRP weights (CPU-based)\"\"\"\n",
    "    w = pd.Series(1.0, index=sort_ix)\n",
    "    c_items = [sort_ix]\n",
    "    while len(c_items) > 0:\n",
    "        c_items = [i[j:k] for i in c_items for j, k in ((0, len(i) // 2), (len(i) // 2, len(i))) if len(i) > 1]\n",
    "        for i in range(0, len(c_items), 2):\n",
    "            c_items0 = c_items[i]\n",
    "            c_items1 = c_items[i + 1]\n",
    "            c_var0 = get_cluster_var(cov, c_items0)\n",
    "            c_var1 = get_cluster_var(cov, c_items1)\n",
    "            alpha = 1 - c_var0 / (c_var0 + c_var1)\n",
    "            w[c_items0] *= alpha\n",
    "            w[c_items1] *= 1 - alpha\n",
    "    # Normalize to ensure weights sum to exactly 1.0\n",
    "    w = w / w.sum()\n",
    "    return w\n",
    "\n",
    "print(\"‚úì CUDA-accelerated HRP functions defined (FIXED VERSION)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e246f7",
   "metadata": {},
   "source": [
    "## Load Data and Select 4 Quarters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2814f56e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data: 33883 rows, 542 columns\n",
      "Stocks: 33881 securities\n",
      "Total quarterly dates available: 180\n",
      "\n",
      "================================================================================\n",
      "PROCESSING 4 QUARTERS ONLY:\n",
      "  - 2016-03-31\n",
      "  - 2016-06-30\n",
      "  - 2016-09-30\n",
      "  - 2016-12-30\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Load Data and Prepare Dates\n",
    "\n",
    "df = pd.read_csv(data_path)\n",
    "print(f\"Loaded data: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "\n",
    "# Identify date columns\n",
    "date_cols = [col for col in df.columns if col not in ['PERMNO', 'Company_Ticker']]\n",
    "\n",
    "# Parse column names to dates\n",
    "parsed_strs = [col.replace('_', ':') for col in date_cols]\n",
    "parsed_dates = pd.to_datetime(parsed_strs, errors='coerce')\n",
    "\n",
    "# Sort by parsed dates\n",
    "sort_order = np.argsort(parsed_dates)\n",
    "date_cols = [date_cols[i] for i in sort_order]\n",
    "dates = parsed_dates[sort_order]\n",
    "date_strs = [d.strftime('%Y-%m-%d') for d in dates]\n",
    "\n",
    "# Convert date columns to numeric\n",
    "for col in date_cols:\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "# Filter to stock rows only\n",
    "stocks_df = df[df['PERMNO'].notna()].copy()\n",
    "print(f\"Stocks: {len(stocks_df)} securities\")\n",
    "\n",
    "# Get quarterly end dates\n",
    "def get_quarterly_dates(dates):\n",
    "    quarterly_dates = []\n",
    "    df_dates = pd.DataFrame({'date': dates})\n",
    "    df_dates['year'] = df_dates['date'].dt.year\n",
    "    df_dates['quarter'] = df_dates['date'].dt.quarter\n",
    "    quarterly_ends = df_dates.groupby(['year', 'quarter'])['date'].max()\n",
    "    return quarterly_ends.tolist()\n",
    "\n",
    "quarterly_rebalance_dates = get_quarterly_dates(dates)\n",
    "print(f\"Total quarterly dates available: {len(quarterly_rebalance_dates)}\")\n",
    "\n",
    "# SELECT ONLY 4 QUARTERS (2016-Q1, Q2, Q3, Q4)\n",
    "target_quarters = [\n",
    "    pd.Timestamp('2016-03-31'),\n",
    "    pd.Timestamp('2016-06-30'),\n",
    "    pd.Timestamp('2016-09-30'),\n",
    "    pd.Timestamp('2016-12-30')\n",
    "]\n",
    "\n",
    "# Filter to only these quarters\n",
    "quarterly_rebalance_dates = [d for d in quarterly_rebalance_dates if d in target_quarters]\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"PROCESSING 4 QUARTERS ONLY:\")\n",
    "for d in quarterly_rebalance_dates:\n",
    "    print(f\"  - {d.strftime('%Y-%m-%d')}\")\n",
    "print(f\"{'='*80}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c4bb3b",
   "metadata": {},
   "source": [
    "## Process 4 Quarterly Rebalances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04b973c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rebalance dates: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [16:48<00:00, 252.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Processed 4 quarterly rebalances\n",
      "  Skipped: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Process Quarterly Rebalances with CUDA Acceleration\n",
    "\n",
    "weights_list = []\n",
    "timing_stats = {'cov': [], 'corr': [], 'dist': [], 'cluster': [], 'weights': [], 'io': [], 'total': []}\n",
    "skipped_count = 0\n",
    "\n",
    "for rebal_date in tqdm(quarterly_rebalance_dates, desc=\"Processing rebalance dates\"):\n",
    "    t_start = time.time()\n",
    "    t_io_start = time.time()\n",
    "    rebal_str = rebal_date.strftime('%Y-%m-%d')\n",
    "    \n",
    "    # Find the index of the rebalance date\n",
    "    try:\n",
    "        rebal_idx = date_strs.index(rebal_str)\n",
    "    except ValueError:\n",
    "        skipped_count += 1\n",
    "        continue\n",
    "    \n",
    "    # Use 12 months ending at rebalance date\n",
    "    if rebal_idx < 11:\n",
    "        skipped_count += 1\n",
    "        continue\n",
    "    \n",
    "    # Get the 12 most recent months\n",
    "    window_indices = list(range(rebal_idx - 11, rebal_idx + 1))\n",
    "    actual_window_cols = [date_cols[i] for i in window_indices]\n",
    "    window_dates = [dates[i] for i in window_indices]\n",
    "    \n",
    "    if len(actual_window_cols) != 12:\n",
    "        skipped_count += 1\n",
    "        continue\n",
    "    \n",
    "    # Select window data\n",
    "    window_df = stocks_df[['PERMNO', 'Company_Ticker'] + actual_window_cols].copy()\n",
    "    window_df = window_df[window_df['Company_Ticker'].notna()]\n",
    "    \n",
    "    # Require complete 12 months\n",
    "    valid_mask = window_df[actual_window_cols].notna().sum(axis=1) == 12\n",
    "    window_df = window_df[valid_mask]\n",
    "    \n",
    "    if len(window_df) < 20:\n",
    "        skipped_count += 1\n",
    "        continue\n",
    "    \n",
    "    # Prepare returns matrix\n",
    "    returns = window_df[actual_window_cols].T  # Time x Assets\n",
    "    returns.columns = window_df['PERMNO'].astype(str)\n",
    "    \n",
    "    timing_stats['io'].append(time.time() - t_io_start)\n",
    "\n",
    "    # Filter out stocks with zero or near-zero variance\n",
    "    stock_variance = returns.values.var(axis=0, ddof=1)\n",
    "    min_variance = 1e-10\n",
    "    valid_variance_mask = (stock_variance > min_variance) & np.isfinite(stock_variance)\n",
    "\n",
    "    if valid_variance_mask.sum() < 2:\n",
    "        skipped_count += 1\n",
    "        continue\n",
    "\n",
    "    # Get valid PERMNOs\n",
    "    valid_permnos = returns.columns[valid_variance_mask].tolist()\n",
    "    \n",
    "    # Filter returns\n",
    "    returns = returns[valid_permnos]\n",
    "    returns_np = returns.values\n",
    "    \n",
    "    # Filter window_df\n",
    "    window_df = window_df[window_df['PERMNO'].astype(str).isin(valid_permnos)].reset_index(drop=True)\n",
    "    \n",
    "    # === GPU-ACCELERATED COVARIANCE ===\n",
    "    t0 = time.time()\n",
    "    \n",
    "    if GPU_AVAILABLE:\n",
    "        returns_gpu = cp.asarray(returns_np)\n",
    "        cov_array, shrinkage, cov_gpu = compute_covariance_gpu(returns_np, returns_gpu)\n",
    "    else:\n",
    "        cov_array, shrinkage, cov_gpu = compute_covariance_gpu(returns_np)\n",
    "    \n",
    "    timing_stats['cov'].append(time.time() - t0)\n",
    "    \n",
    "    cov = pd.DataFrame(cov_array, index=returns.columns, columns=returns.columns)\n",
    "    \n",
    "    # === GPU-ACCELERATED CORRELATION ===\n",
    "    t0 = time.time()\n",
    "    \n",
    "    if GPU_AVAILABLE:\n",
    "        std_gpu = cp.sqrt(cp.diag(cov_gpu))\n",
    "        std_gpu = cp.where(std_gpu < 1e-10, 1e-10, std_gpu)\n",
    "        corr_gpu = cov_gpu / cp.outer(std_gpu, std_gpu)\n",
    "        \n",
    "        max_corr = float(cp.max(corr_gpu))\n",
    "        min_corr = float(cp.min(corr_gpu))\n",
    "        epsilon = 1e-6\n",
    "        if max_corr > 1.0 + epsilon or min_corr < -1.0 - epsilon:\n",
    "            print(f\"‚ö† WARNING {rebal_str}: Correlation out of bounds [{min_corr:.6f}, {max_corr:.6f}], clamping...\")\n",
    "        \n",
    "        corr_gpu = cp.clip(corr_gpu, -1.0, 1.0)\n",
    "    else:\n",
    "        std = np.sqrt(np.diag(cov_array))\n",
    "        std = np.where(std < 1e-10, 1e-10, std)\n",
    "        corr_array = cov_array / np.outer(std, std)\n",
    "        \n",
    "        max_corr = np.max(corr_array)\n",
    "        min_corr = np.min(corr_array)\n",
    "        epsilon = 1e-6\n",
    "        if max_corr > 1.0 + epsilon or min_corr < -1.0 - epsilon:\n",
    "            print(f\"‚ö† WARNING {rebal_str}: Correlation out of bounds [{min_corr:.6f}, {max_corr:.6f}], clamping...\")\n",
    "        \n",
    "        corr_array = np.clip(corr_array, -1.0, 1.0)\n",
    "    \n",
    "    timing_stats['corr'].append(time.time() - t0)\n",
    "    \n",
    "    # === GPU-ACCELERATED DISTANCES ===\n",
    "    t0 = time.time()\n",
    "    if GPU_AVAILABLE:\n",
    "        dist_gpu = get_correlation_distance_gpu(corr_gpu)\n",
    "        eucl_dist_gpu = get_euclidean_distance_gpu(dist_gpu)\n",
    "        eucl_dist_np = cp.asnumpy(eucl_dist_gpu)\n",
    "        corr_array = cp.asnumpy(corr_gpu)\n",
    "    else:\n",
    "        corr_array = np.clip(corr_array, -1.0, 1.0)\n",
    "        dist_np = np.sqrt(np.clip((1 - corr_array) / 2, 0.0, None))\n",
    "        dist_np = np.nan_to_num(dist_np, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        n = dist_np.shape[0]\n",
    "        squared_norms = np.sum(dist_np ** 2, axis=1, keepdims=True)\n",
    "        eucl_dist_np = np.sqrt(np.clip(squared_norms + squared_norms.T - 2 * np.dot(dist_np, dist_np.T), 0.0, None))\n",
    "        eucl_dist_np = np.nan_to_num(eucl_dist_np, nan=1e-8, posinf=1e-8, neginf=1e-8)\n",
    "    \n",
    "    eucl_dist_np = np.nan_to_num(eucl_dist_np, nan=1e-8, posinf=1e-8, neginf=1e-8)\n",
    "    \n",
    "    try:\n",
    "        eucl_dist_condensed = squareform(eucl_dist_np, checks=False)\n",
    "    except:\n",
    "        n = eucl_dist_np.shape[0]\n",
    "        eucl_dist_condensed = eucl_dist_np[np.triu_indices(n, k=1)]\n",
    "    \n",
    "    eucl_dist_condensed = np.nan_to_num(eucl_dist_condensed, nan=1e-8, posinf=1e-8, neginf=1e-8)\n",
    "    timing_stats['dist'].append(time.time() - t0)\n",
    "    \n",
    "    # === CLUSTERING ===\n",
    "    t0 = time.time()\n",
    "    try:\n",
    "        link = sch.linkage(eucl_dist_condensed, method='single')\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö† Clustering failed for {rebal_str}: {e}, skipping\")\n",
    "        skipped_count += 1\n",
    "        continue\n",
    "    \n",
    "    sort_ix = get_quasi_diag(link)\n",
    "    sort_ix = returns.columns[sort_ix].tolist()\n",
    "    timing_stats['cluster'].append(time.time() - t0)\n",
    "    \n",
    "    # === COMPUTE HRP WEIGHTS ===\n",
    "    t0 = time.time()\n",
    "    hrp_weights = get_recursive_bisection(cov, sort_ix)\n",
    "    \n",
    "    # Validation\n",
    "    weight_sum = hrp_weights.sum()\n",
    "    if abs(weight_sum - 1.0) > 1e-6:\n",
    "        print(f\"‚ö† WARNING {rebal_str}: Weights sum to {weight_sum:.10f}, renormalizing...\")\n",
    "        hrp_weights = hrp_weights / weight_sum\n",
    "    \n",
    "    timing_stats['weights'].append(time.time() - t0)\n",
    "    \n",
    "    # Map back to Company_Ticker and PERMNO\n",
    "    permno_to_ticker = dict(zip(window_df['PERMNO'].astype(str), window_df['Company_Ticker']))\n",
    "    weights_df = pd.DataFrame({\n",
    "        'PERMNO': hrp_weights.index,\n",
    "        'Company_Ticker': [permno_to_ticker[p] for p in hrp_weights.index],\n",
    "        rebal_str: hrp_weights.values\n",
    "    })\n",
    "    weights_list.append(weights_df)\n",
    "    \n",
    "    timing_stats['total'].append(time.time() - t_start)\n",
    "\n",
    "print(f\"\\n‚úì Processed {len(weights_list)} quarterly rebalances\")\n",
    "print(f\"  Skipped: {skipped_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba606430",
   "metadata": {},
   "source": [
    "## Save Results and Performance Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "531bd160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Saved weights to C:/Users/lucas/OneDrive/Bureau/HRP/DATA (CRSP)/PREPROCESSED DATA/Rolling Windows Test 4Q\\hrp_weights_4quarters.csv\n",
      "\n",
      "================================================================================\n",
      "PERFORMANCE SUMMARY - 4 QUARTERS TEST\n",
      "================================================================================\n",
      "Mode: CPU\n",
      "Rebalance dates processed: 4\n",
      "Skipped (insufficient history): 0\n",
      "\n",
      "Average timing per rebalance:\n",
      "  Data I/O & Prep: 66.83 ms  (CPU-only)\n",
      "  Covariance:      155328.34 ms  \n",
      "  Correlation:     1146.35 ms  \n",
      "  Distances:       15697.44 ms  \n",
      "  Clustering:      13255.80 ms  (CPU-only)\n",
      "  Weight Calc:     66630.01 ms  (CPU-only)\n",
      "  Total:           252218.79 ms\n",
      "\n",
      "================================================================================\n",
      "TIME BREAKDOWN\n",
      "================================================================================\n",
      "  GPU Operations:  688.69s (68.3%)\n",
      "  CPU Operations:  319.81s (31.7%)\n",
      "  Other (overhead):0.38s (0.0%)\n",
      "\n",
      "Total runtime:    1008.88 seconds\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Combine and Save Results\n",
    "\n",
    "if len(weights_list) > 0:\n",
    "    all_weights = weights_list[0]\n",
    "    for weights_df in weights_list[1:]:\n",
    "        all_weights = all_weights.merge(weights_df, on=['PERMNO', 'Company_Ticker'], how='outer')\n",
    "    \n",
    "    all_weights = all_weights.sort_values('Company_Ticker').reset_index(drop=True)\n",
    "    \n",
    "    date_cols_in_df = [col for col in all_weights.columns if col not in ['PERMNO', 'Company_Ticker']]\n",
    "    date_cols_sorted = sorted(date_cols_in_df)\n",
    "    all_weights = all_weights[['PERMNO', 'Company_Ticker'] + date_cols_sorted]\n",
    "    \n",
    "    all_weights.to_csv(os.path.join(rolling_dir, 'hrp_weights_4quarters.csv'), index=False)\n",
    "    print(f\"‚úì Saved weights to {os.path.join(rolling_dir, 'hrp_weights_4quarters.csv')}\")\n",
    "else:\n",
    "    print(\"‚ö† No weights computed!\")\n",
    "    all_weights = pd.DataFrame()\n",
    "\n",
    "# Performance Summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PERFORMANCE SUMMARY - 4 QUARTERS TEST\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Mode: {'GPU (CUDA)' if GPU_AVAILABLE else 'CPU'}\")\n",
    "print(f\"Rebalance dates processed: {len(weights_list)}\")\n",
    "print(f\"Skipped (insufficient history): {skipped_count}\")\n",
    "\n",
    "if len(timing_stats['total']) > 0:\n",
    "    print(f\"\\nAverage timing per rebalance:\")\n",
    "    print(f\"  Data I/O & Prep: {np.mean(timing_stats['io'])*1000:.2f} ms  (CPU-only)\")\n",
    "    print(f\"  Covariance:      {np.mean(timing_stats['cov'])*1000:.2f} ms  {'‚ö° GPU' if GPU_AVAILABLE else ''}\")\n",
    "    print(f\"  Correlation:     {np.mean(timing_stats['corr'])*1000:.2f} ms  {'‚ö° GPU' if GPU_AVAILABLE else ''}\")\n",
    "    print(f\"  Distances:       {np.mean(timing_stats['dist'])*1000:.2f} ms  {'‚ö° GPU' if GPU_AVAILABLE else ''}\")\n",
    "    print(f\"  Clustering:      {np.mean(timing_stats['cluster'])*1000:.2f} ms  (CPU-only)\")\n",
    "    print(f\"  Weight Calc:     {np.mean(timing_stats['weights'])*1000:.2f} ms  (CPU-only)\")\n",
    "    print(f\"  Total:           {np.mean(timing_stats['total'])*1000:.2f} ms\")\n",
    "    \n",
    "    gpu_time = np.sum(timing_stats['cov']) + np.sum(timing_stats['corr']) + np.sum(timing_stats['dist'])\n",
    "    cpu_time = np.sum(timing_stats['io']) + np.sum(timing_stats['cluster']) + np.sum(timing_stats['weights'])\n",
    "    other_time = np.sum(timing_stats['total']) - gpu_time - cpu_time\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"TIME BREAKDOWN\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"  GPU Operations:  {gpu_time:.2f}s ({gpu_time/np.sum(timing_stats['total'])*100:.1f}%)\")\n",
    "    print(f\"  CPU Operations:  {cpu_time:.2f}s ({cpu_time/np.sum(timing_stats['total'])*100:.1f}%)\")\n",
    "    print(f\"  Other (overhead):{other_time:.2f}s ({other_time/np.sum(timing_stats['total'])*100:.1f}%)\")\n",
    "    print(f\"\\nTotal runtime:    {np.sum(timing_stats['total']):.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbc0471",
   "metadata": {},
   "source": [
    "## Validation: Check for Equal Weights Bug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5383054b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "VALIDATION: CHECKING FOR EQUAL WEIGHTS BUG\n",
      "================================================================================\n",
      "‚úÖ 2016-03-31: PROPER HRP\n",
      "   N=6371, unique=6371, min=2.807486e-07, max=8.579530e-04, std=1.210085e-04\n",
      "   Top 5 weights: [0.00085795 0.0007404  0.00072757 0.00071301 0.00070183]\n",
      "‚úÖ 2016-06-30: PROPER HRP\n",
      "   N=6414, unique=6414, min=7.275101e-07, max=1.209998e-03, std=1.286922e-04\n",
      "   Top 5 weights: [0.00121    0.00112719 0.0010352  0.00103015 0.00093051]\n",
      "‚úÖ 2016-09-30: PROPER HRP\n",
      "   N=6422, unique=6422, min=3.510675e-07, max=1.396996e-03, std=1.278815e-04\n",
      "   Top 5 weights: [0.001397   0.0012703  0.00123413 0.00120539 0.00120385]\n",
      "‚úÖ 2016-12-30: PROPER HRP\n",
      "   N=6444, unique=6444, min=1.817427e-07, max=7.705819e-04, std=1.237045e-04\n",
      "   Top 5 weights: [0.00077058 0.0007703  0.00077029 0.00077027 0.0007702 ]\n",
      "\n",
      "================================================================================\n",
      "‚úÖ ‚úÖ ‚úÖ SUCCESS! All quarters show proper HRP weight dispersion!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Validate Results - Check for Equal Weights Bug\n",
    "\n",
    "if len(weights_list) > 0:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"VALIDATION: CHECKING FOR EQUAL WEIGHTS BUG\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    weights_file = os.path.join(rolling_dir, 'hrp_weights_4quarters.csv')\n",
    "    df_weights = pd.read_csv(weights_file)\n",
    "    \n",
    "    date_cols_check = [col for col in df_weights.columns if col not in ['PERMNO', 'Company_Ticker']]\n",
    "    \n",
    "    all_proper = True\n",
    "    for date_col in date_cols_check:\n",
    "        weights = df_weights[date_col].dropna()\n",
    "        if len(weights) == 0:\n",
    "            continue\n",
    "        \n",
    "        equal_weight = 1.0 / len(weights)\n",
    "        all_equal = np.allclose(weights.values, equal_weight, rtol=1e-10)\n",
    "        \n",
    "        if all_equal:\n",
    "            print(f\"‚ùå {date_col}: ALL EQUAL (N={len(weights)}, w={weights.iloc[0]:.10f})\")\n",
    "            all_proper = False\n",
    "        else:\n",
    "            unique_count = len(weights.unique())\n",
    "            print(f\"‚úÖ {date_col}: PROPER HRP\")\n",
    "            print(f\"   N={len(weights)}, unique={unique_count}, min={weights.min():.6e}, max={weights.max():.6e}, std={weights.std():.6e}\")\n",
    "            \n",
    "            # Show top 5 weights\n",
    "            top_5 = weights.nlargest(5)\n",
    "            print(f\"   Top 5 weights: {top_5.values}\")\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    if all_proper:\n",
    "        print(\"‚úÖ ‚úÖ ‚úÖ SUCCESS! All quarters show proper HRP weight dispersion!\")\n",
    "    else:\n",
    "        print(\"‚ùå ‚ùå ‚ùå PROBLEM! Some quarters still have equal weights.\")\n",
    "    print(f\"{'='*80}\")\n",
    "else:\n",
    "    print(\"\\n‚ö† No weights to validate\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
