{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78c23981",
   "metadata": {},
   "source": [
    "# CUDA-Accelerated HRP Portfolio Optimization (24-Month Lookback)\n",
    "\n",
    "This notebook implements GPU-accelerated Hierarchical Risk Parity (HRP) portfolio optimization with **SciPy single-linkage clustering** for all quarterly rebalances from 1980 to present using a **24-month lookback window**.\n",
    "\n",
    "**GPU Acceleration:**\n",
    "- **Covariance matrix computation** (Ledoit-Wolf shrinkage estimation)\n",
    "- **Correlation matrix calculation** (vectorized operations)\n",
    "- **Distance matrix computations** (correlation distance + Euclidean distance)\n",
    "- **Cumulative return calculations**\n",
    "\n",
    "**CPU Operations:**\n",
    "- **Hierarchical clustering** via `scipy.cluster.hierarchy.linkage(method='single')` with precomputed distance matrix\n",
    "- **Recursive bisection** for weight allocation\n",
    "- **Data I/O operations**\n",
    "\n",
    "**Processing Scope:**\n",
    "- All quarterly rebalance dates in the dataset (typically 150+ windows)\n",
    "- **24-month rolling window** for each rebalance (lookback period)\n",
    "- Minimum 20 stocks per portfolio after filtering\n",
    "\n",
    "**Requirements:**\n",
    "- NVIDIA GPU with CUDA support (compute capability 6.0+)\n",
    "- `cupy-cuda11x` or `cupy-cuda12x` for GPU operations\n",
    "- `scikit-learn` for Ledoit-Wolf covariance\n",
    "- `scipy` for hierarchical clustering\n",
    "- `pandas`, `numpy`, `tqdm`, `matplotlib`\n",
    "\n",
    "**Output:**\n",
    "- CSV file with HRP weights for all quarterly rebalances: `hrp_weights_24m_lookback.csv`\n",
    "- Performance comparison plots vs CRSP value-weighted and equal-weighted benchmarks\n",
    "- Detailed timing statistics for GPU vs CPU operations\n",
    "\n",
    "**Note:** Automatically falls back to CPU-only mode if GPU unavailable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7355f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Version Info:\n",
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2022 NVIDIA Corporation\n",
      "Built on Wed_Sep_21_10:33:58_PDT_2022\n",
      "Cuda compilation tools, release 11.8, V11.8.89\n",
      "Build cuda_11.8.r11.8/compiler.31833905_0\n",
      "\n",
      "\n",
      "✓ Detected CUDA 11.x\n",
      "Install: !pip install cupy-cuda11x scikit-learn scipy tqdm pandas numpy matplotlib\n"
     ]
    }
   ],
   "source": [
    "# Cell 0: Install Dependencies (Optional - run if packages not installed)\n",
    "\n",
    "# For CUDA 11.x\n",
    "# !pip install cupy-cuda11x scikit-learn scipy tqdm pandas numpy matplotlib\n",
    "\n",
    "# For CUDA 12.x\n",
    "# !pip install cupy-cuda12x scikit-learn scipy tqdm pandas numpy matplotlib\n",
    "\n",
    "import subprocess\n",
    "try:\n",
    "    cuda_version = subprocess.check_output(['nvcc', '--version']).decode('utf-8')\n",
    "    print(\"CUDA Version Info:\")\n",
    "    print(cuda_version)\n",
    "    \n",
    "    if 'release 11' in cuda_version:\n",
    "        print(\"\\n✓ Detected CUDA 11.x\")\n",
    "        print(\"Install: !pip install cupy-cuda11x scikit-learn scipy tqdm pandas numpy matplotlib\")\n",
    "    elif 'release 12' in cuda_version:\n",
    "        print(\"\\n✓ Detected CUDA 12.x\")\n",
    "        print(\"Install: !pip install cupy-cuda12x scikit-learn scipy tqdm pandas numpy matplotlib\")\n",
    "    else:\n",
    "        print(\"\\n⚠ Unknown CUDA version\")\n",
    "except:\n",
    "    print(\"Cannot detect CUDA version via nvcc. Checking with nvidia-smi...\")\n",
    "    try:\n",
    "        nvidia_smi = subprocess.check_output(['nvidia-smi']).decode('utf-8')\n",
    "        print(nvidia_smi)\n",
    "        print(\"\\nCheck the 'CUDA Version' in the output above.\")\n",
    "    except:\n",
    "        print(\"⚠ Could not detect GPU. Will run in CPU-only mode.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28bb289a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ GPU Detected: NVIDIA L40\n",
      "  CUDA Version: 11080\n",
      "  Memory: 47.58 GB\n",
      "\n",
      "Mode: GPU (CUDA)\n",
      "Output Directory: Rolling Windows\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports and GPU Setup\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.cluster.hierarchy as sch\n",
    "from sklearn.covariance import LedoitWolf\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "try:\n",
    "    import cupy as cp\n",
    "    GPU_AVAILABLE = True\n",
    "    \n",
    "    device = cp.cuda.Device()\n",
    "    props = cp.cuda.runtime.getDeviceProperties(device.id)\n",
    "    gpu_name = props['name'].decode('utf-8')\n",
    "    total_mem = props['totalGlobalMem'] / 1e9\n",
    "    cuda_version = cp.cuda.runtime.runtimeGetVersion()\n",
    "    \n",
    "    print(f\"✓ GPU Detected: {gpu_name}\")\n",
    "    print(f\"  CUDA Version: {cuda_version}\")\n",
    "    print(f\"  Memory: {total_mem:.2f} GB\")\n",
    "except ImportError as e:\n",
    "    print(f\"⚠ GPU libraries not available: {e}\")\n",
    "    print(\"  Running in CPU-only mode\")\n",
    "    GPU_AVAILABLE = False\n",
    "    cp = np\n",
    "\n",
    "data_path = r'ADA-HRP-Preprocessed-DATA.csv'\n",
    "rolling_dir = r'Rolling Windows'\n",
    "os.makedirs(rolling_dir, exist_ok=True)\n",
    "\n",
    "print(f\"\\nMode: {'GPU (CUDA)' if GPU_AVAILABLE else 'CPU'}\")\n",
    "print(f\"Output Directory: {rolling_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cfe3454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data: 33883 rows, 542 columns\n",
      "Stocks: 33881 securities\n",
      "Quarterly rebalance dates: 180\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Load Data and Prepare Quarterly Rebalance Dates\n",
    "\n",
    "df = pd.read_csv(data_path)\n",
    "print(f\"Loaded data: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "\n",
    "date_cols = [col for col in df.columns if col not in ['PERMNO', 'Company_Ticker']]\n",
    "\n",
    "parsed_strs = [col.replace('_', ':') for col in date_cols]\n",
    "parsed_dates = pd.to_datetime(parsed_strs, errors='coerce')\n",
    "\n",
    "sort_order = np.argsort(parsed_dates)\n",
    "date_cols = [date_cols[i] for i in sort_order]\n",
    "dates = parsed_dates[sort_order]\n",
    "date_strs = [d.strftime('%Y-%m-%d') for d in dates]\n",
    "\n",
    "for col in date_cols:\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "stocks_df = df[df['PERMNO'].notna()].copy()\n",
    "print(f\"Stocks: {len(stocks_df)} securities\")\n",
    "\n",
    "def get_quarterly_dates(dates):\n",
    "    quarterly_dates = []\n",
    "    df_dates = pd.DataFrame({'date': dates})\n",
    "    df_dates['year'] = df_dates['date'].dt.year\n",
    "    df_dates['quarter'] = df_dates['date'].dt.quarter\n",
    "    quarterly_ends = df_dates.groupby(['year', 'quarter'])['date'].max()\n",
    "    return quarterly_ends.tolist()\n",
    "\n",
    "quarterly_rebalance_dates = get_quarterly_dates(dates)\n",
    "print(f\"Quarterly rebalance dates: {len(quarterly_rebalance_dates)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ccca688e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ HRP functions defined\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Define HRP Functions\n",
    "\n",
    "def get_correlation_distance_gpu(corr_gpu):\n",
    "    \"\"\"Compute correlation distance matrix on GPU\"\"\"\n",
    "    corr_gpu = cp.clip(corr_gpu, -1.0, 1.0)\n",
    "    dist = cp.sqrt(cp.clip((1 - corr_gpu) / 2, 0.0, None))\n",
    "    \n",
    "    if cp.any(~cp.isfinite(dist)):\n",
    "        print(\"⚠ WARNING: NaN/Inf detected in correlation distance matrix\")\n",
    "    \n",
    "    dist = cp.nan_to_num(dist, nan=0.5, posinf=0.5, neginf=0.5)\n",
    "    return dist\n",
    "\n",
    "def get_euclidean_distance_gpu(dist_gpu):\n",
    "    \"\"\"Compute pairwise Euclidean distances on GPU\"\"\"\n",
    "    n = dist_gpu.shape[0]\n",
    "    squared_norms = cp.sum(dist_gpu ** 2, axis=1, keepdims=True)\n",
    "    eucl_dist = cp.sqrt(cp.clip(squared_norms + squared_norms.T - 2 * cp.dot(dist_gpu, dist_gpu.T), 0.0, None))\n",
    "    \n",
    "    if cp.any(~cp.isfinite(eucl_dist)):\n",
    "        print(\"⚠ WARNING: NaN/Inf detected in Euclidean distance matrix\")\n",
    "    \n",
    "    eucl_dist = cp.nan_to_num(eucl_dist, nan=1e-4, posinf=1e-4, neginf=1e-4)\n",
    "    return eucl_dist\n",
    "\n",
    "def compute_covariance_gpu(returns_np, returns_gpu=None):\n",
    "    \"\"\"Compute shrunk covariance using Ledoit-Wolf\"\"\"\n",
    "    lw = LedoitWolf()\n",
    "    lw.fit(returns_np)\n",
    "    cov_array = lw.covariance_\n",
    "    shrinkage = lw.shrinkage_\n",
    "    \n",
    "    if GPU_AVAILABLE:\n",
    "        cov_gpu = cp.asarray(cov_array)\n",
    "        return cov_array, shrinkage, cov_gpu\n",
    "    else:\n",
    "        return cov_array, shrinkage, None\n",
    "\n",
    "def get_quasi_diag(link):\n",
    "    \"\"\"Seriation from hierarchical clustering linkage\"\"\"\n",
    "    link = link.astype(int)\n",
    "    sort_ix = pd.Series([link[-1, 0], link[-1, 1]])\n",
    "    num_items = link[-1, 3]\n",
    "    while sort_ix.max() >= num_items:\n",
    "        sort_ix.index = range(0, sort_ix.shape[0] * 2, 2)\n",
    "        df0 = sort_ix[sort_ix >= num_items]\n",
    "        i = df0.index\n",
    "        j = df0.values - num_items\n",
    "        sort_ix[i] = link[j, 0]\n",
    "        df0 = pd.Series(link[j, 1], index=i + 1)\n",
    "        sort_ix = pd.concat([sort_ix, df0])\n",
    "        sort_ix = sort_ix.sort_index()\n",
    "        sort_ix.index = range(sort_ix.shape[0])\n",
    "    return sort_ix.tolist()\n",
    "\n",
    "def perform_single_linkage_clustering(eucl_dist_condensed):\n",
    "    \"\"\"Single-linkage hierarchical clustering using SciPy\"\"\"\n",
    "    return sch.linkage(eucl_dist_condensed, method='single')\n",
    "\n",
    "def get_cluster_var(cov, c_items):\n",
    "    \"\"\"Compute cluster variance using inverse-variance portfolio\"\"\"\n",
    "    cov_ = cov.loc[c_items, c_items].values\n",
    "    \n",
    "    if len(c_items) == 1:\n",
    "        return cov_[0, 0]\n",
    "    \n",
    "    variances = np.diag(cov_)\n",
    "    variances = np.maximum(variances, 1e-10)\n",
    "    \n",
    "    ivp = 1.0 / variances\n",
    "    ivp /= ivp.sum()\n",
    "    \n",
    "    w_ = ivp.reshape(-1, 1)\n",
    "    cVar = (w_.T @ cov_ @ w_)[0, 0]\n",
    "    \n",
    "    return cVar\n",
    "\n",
    "def get_recursive_bisection(cov, sort_ix, debug=False):\n",
    "    \"\"\"Recursive bisection for HRP weight allocation\"\"\"\n",
    "    w = pd.Series(1.0, index=sort_ix)\n",
    "    c_items = [sort_ix]\n",
    "    \n",
    "    iteration = 0\n",
    "    while len(c_items) > 0:\n",
    "        c_items = [i[j:k] for i in c_items \n",
    "                   for j, k in ((0, len(i) // 2), (len(i) // 2, len(i))) \n",
    "                   if len(i) > 1]\n",
    "        \n",
    "        if len(c_items) == 0:\n",
    "            break\n",
    "            \n",
    "        for i in range(0, len(c_items), 2):\n",
    "            if i + 1 >= len(c_items):\n",
    "                break\n",
    "                \n",
    "            c_items0 = c_items[i]\n",
    "            c_items1 = c_items[i + 1]\n",
    "            \n",
    "            c_var0 = get_cluster_var(cov, c_items0)\n",
    "            c_var1 = get_cluster_var(cov, c_items1)\n",
    "            \n",
    "            alpha = 1 - c_var0 / (c_var0 + c_var1)\n",
    "            \n",
    "            if debug and iteration < 5:\n",
    "                print(f\"  Iteration {iteration}, pair {i//2}: var0={c_var0:.6f}, var1={c_var1:.6f}, alpha={alpha:.6f}\")\n",
    "            \n",
    "            w[c_items0] *= alpha\n",
    "            w[c_items1] *= 1 - alpha\n",
    "            \n",
    "        iteration += 1\n",
    "    \n",
    "    w = w / w.sum()\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"  Total iterations: {iteration}\")\n",
    "        print(f\"  Final weight sum: {w.sum():.10f}\")\n",
    "        print(f\"  Weight range: [{w.min():.6f}, {w.max():.6f}]\")\n",
    "    \n",
    "    return w\n",
    "\n",
    "print(\"✓ HRP functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc435af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing all 180 quarterly rebalance dates\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing quarterly rebalances: 100%|██████████| 180/180 [3:08:19<00:00, 62.77s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PERFORMANCE SUMMARY\n",
      "============================================================\n",
      "Mode: GPU (CUDA)\n",
      "Total quarterly dates available: 180\n",
      "Rebalance dates processed: 177\n",
      "Skipped (insufficient history): 3\n",
      "\n",
      "Average timing per rebalance:\n",
      "  Data I/O & Prep: 11.84 ms  (CPU-only)\n",
      "  Covariance:      43605.62 ms  ⚡ GPU\n",
      "  Correlation:     3.06 ms  ⚡ GPU\n",
      "  Distances:       1757.44 ms  ⚡ GPU\n",
      "  Clustering:      2486.02 ms  (SciPy CPU)\n",
      "  Weight Calc:     15950.01 ms  (CPU-only)\n",
      "  Total:           63834.34 ms\n",
      "\n",
      "============================================================\n",
      "TIME BREAKDOWN\n",
      "============================================================\n",
      "  GPU Operations:  8029.80s (71.1%)\n",
      "  CPU Operations:  3265.27s (28.9%)\n",
      "  Other (overhead):3.60s (0.0%)\n",
      "\n",
      "Total runtime:    11298.68 seconds\n",
      "Speedup estimate: 1.00x\n",
      "\n",
      "✓ Saved all HRP weights to hrp_weights_all_scipy.csv\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Process All Quarterly Rebalances (24-Month Lookback)\n",
    "\n",
    "lookback_months = 24\n",
    "weights_filename = 'hrp_weights_24m_lookback.csv'\n",
    "weights_output_path = os.path.join(rolling_dir, weights_filename)\n",
    "target_rebalance_dates = quarterly_rebalance_dates\n",
    "print(f\"Processing all {len(target_rebalance_dates)} quarterly rebalance dates (24-month lookback)\")\n",
    "\n",
    "weights_list = []\n",
    "timing_stats = {'cov': [], 'corr': [], 'dist': [], 'cluster': [], 'weights': [], 'io': [], 'total': []}\n",
    "skipped_count = 0\n",
    "\n",
    "for rebal_date in tqdm(target_rebalance_dates, desc=\"Processing quarterly rebalances\"):\n",
    "    t_start = time.time()\n",
    "    t_io_start = time.time()\n",
    "    rebal_str = rebal_date.strftime('%Y-%m-%d')\n",
    "    \n",
    "    try:\n",
    "        rebal_idx = date_strs.index(rebal_str)\n",
    "    except ValueError:\n",
    "        skipped_count += 1\n",
    "        continue\n",
    "    \n",
    "    if rebal_idx < (lookback_months - 1):\n",
    "        skipped_count += 1\n",
    "        continue\n",
    "    \n",
    "    window_indices = list(range(rebal_idx - (lookback_months - 1), rebal_idx + 1))\n",
    "    actual_window_cols = [date_cols[i] for i in window_indices]\n",
    "    window_dates = [dates[i] for i in window_indices]\n",
    "    \n",
    "    if len(actual_window_cols) != lookback_months:\n",
    "        skipped_count += 1\n",
    "        continue\n",
    "    \n",
    "    window_df = stocks_df[['PERMNO', 'Company_Ticker'] + actual_window_cols].copy()\n",
    "    window_df = window_df[window_df['Company_Ticker'].notna()]\n",
    "    \n",
    "    valid_mask = window_df[actual_window_cols].notna().sum(axis=1) == lookback_months\n",
    "    window_df = window_df[valid_mask]\n",
    "    \n",
    "    if len(window_df) < 20:\n",
    "        skipped_count += 1\n",
    "        continue\n",
    "    \n",
    "    returns = window_df[actual_window_cols].T\n",
    "    returns.columns = window_df['PERMNO'].astype(str)\n",
    "    \n",
    "    timing_stats['io'].append(time.time() - t_io_start)\n",
    "\n",
    "    stock_variance = returns.values.var(axis=0, ddof=1)\n",
    "    min_variance = 1e-10\n",
    "    valid_variance_mask = (stock_variance > min_variance) & np.isfinite(stock_variance)\n",
    "\n",
    "    if valid_variance_mask.sum() < 2:\n",
    "        skipped_count += 1\n",
    "        continue\n",
    "\n",
    "    valid_permnos = returns.columns[valid_variance_mask].tolist()\n",
    "    returns = returns[valid_permnos]\n",
    "    returns_np = returns.values\n",
    "    window_df = window_df[window_df['PERMNO'].astype(str).isin(valid_permnos)].reset_index(drop=True)\n",
    "    \n",
    "    t0 = time.time()\n",
    "    if GPU_AVAILABLE:\n",
    "        returns_gpu = cp.asarray(returns_np)\n",
    "        cov_array, shrinkage, cov_gpu = compute_covariance_gpu(returns_np, returns_gpu)\n",
    "    else:\n",
    "        cov_array, shrinkage, cov_gpu = compute_covariance_gpu(returns_np)\n",
    "    timing_stats['cov'].append(time.time() - t0)\n",
    "    \n",
    "    cov = pd.DataFrame(cov_array, index=returns.columns, columns=returns.columns)\n",
    "    \n",
    "    t0 = time.time()\n",
    "    if GPU_AVAILABLE:\n",
    "        std_gpu = cp.sqrt(cp.diag(cov_gpu))\n",
    "        std_gpu = cp.where(std_gpu < 1e-10, 1e-10, std_gpu)\n",
    "        corr_gpu = cov_gpu / cp.outer(std_gpu, std_gpu)\n",
    "        corr_gpu = cp.clip(corr_gpu, -1.0, 1.0)\n",
    "    else:\n",
    "        std = np.sqrt(np.diag(cov_array))\n",
    "        std = np.where(std < 1e-10, 1e-10, std)\n",
    "        corr_array = cov_array / np.outer(std, std)\n",
    "        corr_array = np.clip(corr_array, -1.0, 1.0)\n",
    "    timing_stats['corr'].append(time.time() - t0)\n",
    "    \n",
    "    t0 = time.time()\n",
    "    if GPU_AVAILABLE:\n",
    "        dist_gpu = get_correlation_distance_gpu(corr_gpu)\n",
    "        eucl_dist_gpu = get_euclidean_distance_gpu(dist_gpu)\n",
    "        eucl_dist_np = cp.asnumpy(eucl_dist_gpu)\n",
    "        corr_array = cp.asnumpy(corr_gpu)\n",
    "    else:\n",
    "        corr_array = np.clip(corr_array, -1.0, 1.0)\n",
    "        dist_np = np.sqrt(np.clip((1 - corr_array) / 2, 0.0, None))\n",
    "        dist_np = np.nan_to_num(dist_np, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        n = dist_np.shape[0]\n",
    "        squared_norms = np.sum(dist_np ** 2, axis=1, keepdims=True)\n",
    "        eucl_dist_np = np.sqrt(np.clip(squared_norms + squared_norms.T - 2 * np.dot(dist_np, dist_np.T), 0.0, None))\n",
    "        eucl_dist_np = np.nan_to_num(eucl_dist_np, nan=1e-8, posinf=1e-8, neginf=1e-8)\n",
    "    \n",
    "    eucl_dist_np = np.nan_to_num(eucl_dist_np, nan=1e-8, posinf=1e-8, neginf=1e-8)\n",
    "    from scipy.spatial.distance import squareform\n",
    "    eucl_dist_condensed = squareform(eucl_dist_np, checks=False)\n",
    "    eucl_dist_condensed = np.nan_to_num(eucl_dist_condensed, nan=1e-8, posinf=1e-8, neginf=1e-8)\n",
    "    timing_stats['dist'].append(time.time() - t0)\n",
    "    \n",
    "    t0 = time.time()\n",
    "    try:\n",
    "        link = perform_single_linkage_clustering(eucl_dist_condensed)\n",
    "    except Exception as e:\n",
    "        print(f\"⚠ Clustering failed for {rebal_str}: {e}, skipping\")\n",
    "        skipped_count += 1\n",
    "        continue\n",
    "    \n",
    "    sort_ix = get_quasi_diag(link)\n",
    "    sort_ix = returns.columns[sort_ix].tolist()\n",
    "    timing_stats['cluster'].append(time.time() - t0)\n",
    "    \n",
    "    t0 = time.time()\n",
    "    is_first_rebal = (rebal_date == target_rebalance_dates[0])\n",
    "    hrp_weights = get_recursive_bisection(cov, sort_ix, debug=is_first_rebal)\n",
    "    \n",
    "    weight_sum = hrp_weights.sum()\n",
    "    if abs(weight_sum - 1.0) > 1e-6:\n",
    "        print(f\"⚠ WARNING {rebal_str}: Weights sum to {weight_sum:.10f}, renormalizing...\")\n",
    "        hrp_weights = hrp_weights / weight_sum\n",
    "    timing_stats['weights'].append(time.time() - t0)\n",
    "    \n",
    "    permno_to_ticker = dict(zip(window_df['PERMNO'].astype(str), window_df['Company_Ticker']))\n",
    "    weights_df = pd.DataFrame({\n",
    "        'PERMNO': hrp_weights.index,\n",
    "        'Company_Ticker': [permno_to_ticker[p] for p in hrp_weights.index],\n",
    "        rebal_str: hrp_weights.values\n",
    "    })\n",
    "    weights_list.append(weights_df)\n",
    "    \n",
    "    timing_stats['total'].append(time.time() - t_start)\n",
    "\n",
    "if len(weights_list) > 0:\n",
    "    all_weights = weights_list[0]\n",
    "    for weights_df in weights_list[1:]:\n",
    "        all_weights = all_weights.merge(weights_df, on=['PERMNO', 'Company_Ticker'], how='outer')\n",
    "    all_weights = all_weights.sort_values('Company_Ticker').reset_index(drop=True)\n",
    "    date_cols_in_df = [col for col in all_weights.columns if col not in ['PERMNO', 'Company_Ticker']]\n",
    "    date_cols_sorted = sorted(date_cols_in_df)\n",
    "    all_weights = all_weights[['PERMNO', 'Company_Ticker'] + date_cols_sorted]\n",
    "    all_weights.to_csv(weights_output_path, index=False)\n",
    "else:\n",
    "    print(\"⚠ No weights computed!\")\n",
    "    all_weights = pd.DataFrame()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PERFORMANCE SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Mode: {'GPU (CUDA)' if GPU_AVAILABLE else 'CPU'}\")\n",
    "print(f\"Total quarterly dates available: {len(quarterly_rebalance_dates)}\")\n",
    "print(f\"Rebalance dates processed: {len(weights_list)}\")\n",
    "print(f\"Skipped (insufficient history): {skipped_count}\")\n",
    "print(f\"\\nAverage timing per rebalance:\")\n",
    "print(f\"  Data I/O & Prep: {np.mean(timing_stats['io'])*1000:.2f} ms  (CPU-only)\")\n",
    "print(f\"  Covariance:      {np.mean(timing_stats['cov'])*1000:.2f} ms  ⚡ GPU\")\n",
    "print(f\"  Correlation:     {np.mean(timing_stats['corr'])*1000:.2f} ms  ⚡ GPU\")\n",
    "print(f\"  Distances:       {np.mean(timing_stats['dist'])*1000:.2f} ms  ⚡ GPU\")\n",
    "print(f\"  Clustering:      {np.mean(timing_stats['cluster'])*1000:.2f} ms  (SciPy CPU)\")\n",
    "print(f\"  Weight Calc:     {np.mean(timing_stats['weights'])*1000:.2f} ms  (CPU-only)\")\n",
    "print(f\"  Total:           {np.mean(timing_stats['total'])*1000:.2f} ms\")\n",
    "\n",
    "gpu_time = np.sum(timing_stats['cov']) + np.sum(timing_stats['corr']) + np.sum(timing_stats['dist'])\n",
    "cpu_time = np.sum(timing_stats['io']) + np.sum(timing_stats['cluster']) + np.sum(timing_stats['weights'])\n",
    "other_time = np.sum(timing_stats['total']) - gpu_time - cpu_time\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"TIME BREAKDOWN\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"  GPU Operations:  {gpu_time:.2f}s ({gpu_time/np.sum(timing_stats['total'])*100:.1f}%)\")\n",
    "print(f\"  CPU Operations:  {cpu_time:.2f}s ({cpu_time/np.sum(timing_stats['total'])*100:.1f}%)\")\n",
    "print(f\"  Other (overhead):{other_time:.2f}s ({other_time/np.sum(timing_stats['total'])*100:.1f}%)\")\n",
    "print(f\"\\nTotal runtime:    {np.sum(timing_stats['total']):.2f} seconds\")\n",
    "print(f\"Speedup estimate: {np.sum(timing_stats['total']) / max(np.sum(timing_stats['total']), 1):.2f}x\")\n",
    "print(f\"\\n✓ Saved all HRP weights to {weights_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19065213",
   "metadata": {},
   "source": [
    "## Performance Architecture\n",
    "\n",
    "**GPU-Accelerated Operations:**\n",
    "- Covariance matrix computation (Ledoit-Wolf shrinkage)\n",
    "- Correlation matrix calculation\n",
    "- Distance matrix computations (correlation distance + Euclidean)\n",
    "- Cumulative return calculations\n",
    "\n",
    "**CPU Operations:**\n",
    "- Hierarchical clustering (SciPy single-linkage with precomputed distances)\n",
    "- Recursive bisection weight allocation\n",
    "- Data I/O operations (loading, filtering, saving)\n",
    "\n",
    "**Processing:** All quarterly rebalance dates from 1980 to present with 12-month rolling windows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19318f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded weights for 31491 unique companies\n",
      "Rebalance dates: 177\n",
      "Total weight entries (long format): 1105298\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Load HRP Weights and Prepare Benchmark Data\n",
    "\n",
    "if 'weights_output_path' not in globals():\n",
    "    raise RuntimeError(\"weights_output_path is not defined. Run the previous cell first.\")\n",
    "    \n",
    "all_weights = pd.read_csv(weights_output_path)\n",
    "\n",
    "rebal_dates_str = [col for col in all_weights.columns if col not in ['PERMNO', 'Company_Ticker']]\n",
    "rebal_dates = pd.to_datetime(rebal_dates_str)\n",
    "\n",
    "all_weights_long = all_weights.melt(\n",
    "    id_vars=['PERMNO', 'Company_Ticker'],\n",
    "    var_name='Rebalance_Date',\n",
    "    value_name='Weight'\n",
    ")\n",
    "all_weights_long = all_weights_long.dropna(subset=['Weight'])\n",
    "\n",
    "all_dates = dates\n",
    "\n",
    "bench_df = df[df['PERMNO'].isna()]\n",
    "vwretd_row = bench_df[bench_df['Company_Ticker'] == 'Value Weighted Benchmark']\n",
    "ewretd_row = bench_df[bench_df['Company_Ticker'] == 'Equally Weighted Benchmark']\n",
    "\n",
    "vwretd = pd.Series(vwretd_row.iloc[0][date_cols].values, index=dates, name='vwretd')\n",
    "ewretd = pd.Series(ewretd_row.iloc[0][date_cols].values, index=dates, name='ewretd')\n",
    "\n",
    "vwretd = pd.to_numeric(vwretd, errors='coerce')\n",
    "ewretd = pd.to_numeric(ewretd, errors='coerce')\n",
    "\n",
    "vwretd = vwretd.sort_index()\n",
    "ewretd = ewretd.sort_index()\n",
    "\n",
    "print(f\"Loaded weights for {len(all_weights)} unique companies\")\n",
    "print(f\"Rebalance dates: {len(rebal_dates)}\")\n",
    "print(f\"Total weight entries (long format): {len(all_weights_long)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52e2e89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing portfolio returns:  45%|████▌     | 243/540 [00:22<00:28, 10.41it/s]"
     ]
    }
   ],
   "source": [
    "# Cell 6: Compute HRP Portfolio Monthly Returns\n",
    "\n",
    "port_returns = pd.Series(index=dates, dtype=float)\n",
    "port_returns.name = 'HRP_CUDA_V2'\n",
    "\n",
    "stocks_indexed = stocks_df.set_index('Company_Ticker')\n",
    "\n",
    "for t_idx, t in enumerate(tqdm(dates, desc=\"Computing portfolio returns\")):\n",
    "    prev_rebal = max([rd for rd in rebal_dates if rd <= t], default=None)\n",
    "    if prev_rebal is None:\n",
    "        continue\n",
    "    \n",
    "    weights_df = all_weights_long[all_weights_long['Rebalance_Date'] == prev_rebal.strftime('%Y-%m-%d')]\n",
    "    weights = weights_df.set_index('Company_Ticker')['Weight']\n",
    "    \n",
    "    col_name = date_cols[t_idx]\n",
    "    \n",
    "    available_tickers = weights.index.intersection(stocks_indexed.index)\n",
    "    if len(available_tickers) == 0:\n",
    "        port_returns[t] = np.nan\n",
    "        continue\n",
    "    \n",
    "    ret_at_t = stocks_indexed.loc[available_tickers, col_name]\n",
    "    weights_aligned = weights.loc[available_tickers]\n",
    "    \n",
    "    valid = ret_at_t.notna()\n",
    "    if valid.sum() == 0:\n",
    "        port_returns[t] = np.nan\n",
    "        continue\n",
    "    \n",
    "    ret_valid = ret_at_t.loc[valid[valid].index]\n",
    "    w_valid = weights_aligned.loc[valid[valid].index]\n",
    "    w_valid /= w_valid.sum()\n",
    "    \n",
    "    port_returns[t] = (w_valid * ret_valid).sum()\n",
    "\n",
    "port_returns = port_returns.dropna()\n",
    "print(f\"\\n✓ Computed {len(port_returns)} monthly portfolio returns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197cd205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Compute Cumulative Returns\n",
    "\n",
    "start_date = port_returns.index.min()\n",
    "vwretd = vwretd[vwretd.index >= start_date]\n",
    "ewretd = ewretd[ewretd.index >= start_date]\n",
    "port_returns = port_returns[port_returns.index >= start_date]\n",
    "\n",
    "vwretd = vwretd.dropna()\n",
    "ewretd = ewretd.dropna()\n",
    "\n",
    "common_index = port_returns.index.intersection(vwretd.index).intersection(ewretd.index)\n",
    "port_returns = port_returns.loc[common_index]\n",
    "vwretd = vwretd.loc[common_index]\n",
    "ewretd = ewretd.loc[common_index]\n",
    "\n",
    "if GPU_AVAILABLE:\n",
    "    hrp_gpu = cp.asarray((1 + port_returns.values).astype(np.float64))\n",
    "    vw_gpu = cp.asarray((1 + vwretd.values).astype(np.float64))\n",
    "    ew_gpu = cp.asarray((1 + ewretd.values).astype(np.float64))\n",
    "    \n",
    "    cum_hrp = pd.Series(cp.asnumpy(cp.cumprod(hrp_gpu)), index=port_returns.index)\n",
    "    cum_vw = pd.Series(cp.asnumpy(cp.cumprod(vw_gpu)), index=vwretd.index)\n",
    "    cum_ew = pd.Series(cp.asnumpy(cp.cumprod(ew_gpu)), index=ewretd.index)\n",
    "else:\n",
    "    cum_hrp = (1 + port_returns).cumprod()\n",
    "    cum_vw = (1 + vwretd).cumprod()\n",
    "    cum_ew = (1 + ewretd).cumprod()\n",
    "\n",
    "print(\"✓ Cumulative returns computed\")\n",
    "print(f\"  HRP Final Value: {cum_hrp.iloc[-1]:.4f}\")\n",
    "print(f\"  VW Final Value:  {cum_vw.iloc[-1]:.4f}\")\n",
    "print(f\"  EW Final Value:  {cum_ew.iloc[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813f76dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Plot Performance Comparison\n",
    "\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(cum_hrp.index, cum_hrp, label='HRP Portfolio (CUDA + SciPy)', linewidth=2.5, color='#0173B2')\n",
    "plt.plot(cum_vw.index, cum_vw, label='Value Weighted Benchmark', linewidth=1.5, color='#DE8F05')\n",
    "plt.plot(cum_ew.index, cum_ew, label='Equally Weighted Benchmark', linewidth=1.5, color='#029E73')\n",
    "plt.xlabel('Date', fontsize=12)\n",
    "plt.ylabel('Cumulative Return', fontsize=12)\n",
    "plt.title('Performance Comparison: CUDA-HRP (SciPy Single-Linkage) vs Benchmarks', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PERFORMANCE SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"HRP Total Return:        {(cum_hrp.iloc[-1] - 1) * 100:.2f}%\")\n",
    "print(f\"VW Benchmark Return:     {(cum_vw.iloc[-1] - 1) * 100:.2f}%\")\n",
    "print(f\"EW Benchmark Return:     {(cum_ew.iloc[-1] - 1) * 100:.2f}%\")\n",
    "print(f\"\\nHRP Annualized Return:   {(cum_hrp.iloc[-1] ** (12 / len(cum_hrp)) - 1) * 100:.2f}%\")\n",
    "print(f\"HRP Volatility:          {port_returns.std() * np.sqrt(12) * 100:.2f}%\")\n",
    "print(f\"HRP Sharpe Ratio:        {(port_returns.mean() * 12) / (port_returns.std() * np.sqrt(12)):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db966fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Visualize Weight Evolution (Top 20 Stocks)\n",
    "\n",
    "weights_aggregated = all_weights_long.groupby(['Rebalance_Date', 'Company_Ticker'], as_index=False)['Weight'].sum()\n",
    "\n",
    "weights_pivot = weights_aggregated.pivot(index='Rebalance_Date', columns='Company_Ticker', values='Weight')\n",
    "weights_pivot.index = pd.to_datetime(weights_pivot.index)\n",
    "weights_pivot = weights_pivot.sort_index()\n",
    "\n",
    "ticker_avg_weights = weights_pivot.mean().sort_values(ascending=False)\n",
    "top_20_tickers = ticker_avg_weights.head(20).index.tolist()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 9))\n",
    "\n",
    "import matplotlib.cm as cm\n",
    "colors = cm.tab20(np.linspace(0, 1, 20))\n",
    "\n",
    "for i, ticker in enumerate(top_20_tickers):\n",
    "    if ticker in weights_pivot.columns:\n",
    "        ax.plot(weights_pivot.index, weights_pivot[ticker],\n",
    "               alpha=0.8, linewidth=1.8, color=colors[i], label=ticker, marker='o', markersize=2)\n",
    "\n",
    "ax.set_xlabel('Rebalance Date', fontsize=12)\n",
    "ax.set_ylabel('Portfolio Weight', fontsize=12)\n",
    "ax.set_title('Top 20 Stocks by Average Weight - Evolution Over Time', fontsize=14, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_ylim(bottom=0)\n",
    "\n",
    "overall_avg = ticker_avg_weights.head(20).mean()\n",
    "ax.axhline(y=overall_avg, color='#CC0000', linestyle='--', linewidth=2.5,\n",
    "           label=f'Top 20 Avg: {overall_avg:.4f}', alpha=0.9, zorder=100)\n",
    "\n",
    "ax.legend(loc='upper left', fontsize=7, ncol=3, framealpha=0.95)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TOP 20 STOCKS - WEIGHT EVOLUTION STATISTICS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Rebalance periods:       {len(weights_pivot)}\")\n",
    "print(f\"Overall avg weight:      {overall_avg:.4f}\")\n",
    "print(f\"\\nTop 20 Stocks by Average Weight:\")\n",
    "print(f\"{'Rank':<6}{'Ticker':<55}{'Avg Weight':<12}{'Max Weight':<12}{'Std Dev':<12}\")\n",
    "print(\"=\"*95)\n",
    "for i, ticker in enumerate(top_20_tickers, 1):\n",
    "    avg = weights_pivot[ticker].mean()\n",
    "    max_w = weights_pivot[ticker].max()\n",
    "    std = weights_pivot[ticker].std()\n",
    "    print(f\"{i:<6}{ticker:<55}{avg:<12.6f}{max_w:<12.6f}{std:<12.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ee242f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Diagnostic Analysis - Weight Distribution & Data Quality\n",
    "\n",
    "weights_csv_path = weights_output_path\n",
    "df_weights = pd.read_csv(weights_csv_path)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"WEIGHTS DATA QUALITY DIAGNOSTIC\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n1. DATA STRUCTURE:\")\n",
    "print(f\"   Total companies: {len(df_weights)}\")\n",
    "print(f\"   Total columns: {len(df_weights.columns)}\")\n",
    "print(f\"   Rebalance dates: {len(df_weights.columns) - 2}\")\n",
    "\n",
    "date_columns = [col for col in df_weights.columns if col not in ['PERMNO', 'Company_Ticker']]\n",
    "print(f\"   First rebalance: {date_columns[0] if date_columns else 'N/A'}\")\n",
    "print(f\"   Last rebalance: {date_columns[-1] if date_columns else 'N/A'}\")\n",
    "\n",
    "print(f\"\\n2. DATA COMPLETENESS:\")\n",
    "total_cells = len(df_weights) * len(date_columns)\n",
    "non_null_cells = df_weights[date_columns].notna().sum().sum() if date_columns else 0\n",
    "sparsity = (total_cells - non_null_cells) / total_cells * 100 if total_cells else 0.0\n",
    "print(f\"   Total weight cells: {total_cells:,}\")\n",
    "print(f\"   Non-null weights: {non_null_cells:,}\")\n",
    "print(f\"   Sparsity (NaN %): {sparsity:.2f}%\")\n",
    "\n",
    "print(f\"\\n3. WEIGHT STATISTICS PER REBALANCE:\")\n",
    "if date_columns:\n",
    "    weight_sums = df_weights[date_columns].sum(axis=0)\n",
    "    portfolio_sizes = df_weights[date_columns].notna().sum(axis=0)\n",
    "    print(f\"   Min portfolio size: {portfolio_sizes.min()} stocks\")\n",
    "    print(f\"   Max portfolio size: {portfolio_sizes.max()} stocks\")\n",
    "    print(f\"   Avg portfolio size: {portfolio_sizes.mean():.1f} stocks\")\n",
    "    print(f\"   Min weight sum: {weight_sums.min():.6f}\")\n",
    "    print(f\"   Max weight sum: {weight_sums.max():.6f}\")\n",
    "    print(f\"   Avg weight sum: {weight_sums.mean():.6f}\")\n",
    "else:\n",
    "    print(\"   No rebalance dates found.\")\n",
    "\n",
    "print(f\"\\n4. WEIGHT DISTRIBUTION ANALYSIS:\")\n",
    "all_weights_flat = df_weights[date_columns].values.flatten() if date_columns else np.array([])\n",
    "all_weights_flat = all_weights_flat[~np.isnan(all_weights_flat)]\n",
    "if all_weights_flat.size > 0:\n",
    "    print(f\"   Min weight: {all_weights_flat.min():.8f}\")\n",
    "    print(f\"   Max weight: {all_weights_flat.max():.8f}\")\n",
    "    print(f\"   Mean weight: {all_weights_flat.mean():.8f}\")\n",
    "    print(f\"   Median weight: {np.median(all_weights_flat):.8f}\")\n",
    "    print(f\"   Std dev: {all_weights_flat.std():.8f}\")\n",
    "    extreme_high = (all_weights_flat > 0.01).sum()\n",
    "    extreme_low = (all_weights_flat < 1e-6).sum()\n",
    "    print(f\"   Weights > 1%: {extreme_high} ({extreme_high/len(all_weights_flat)*100:.2f}%)\")\n",
    "    print(f\"   Weights < 0.0001%: {extreme_low} ({extreme_low/len(all_weights_flat)*100:.2f}%)\")\n",
    "else:\n",
    "    print(\"   No weight data available.\")\n",
    "\n",
    "print(f\"\\n5. REBALANCE DATES WITH ISSUES:\")\n",
    "problematic_dates = []\n",
    "for date_col in date_columns:\n",
    "    weight_sum = df_weights[date_col].sum()\n",
    "    portfolio_size = df_weights[date_col].notna().sum()\n",
    "    max_weight = df_weights[date_col].max()\n",
    "    if abs(weight_sum - 1.0) > 0.01 or (portfolio_size > 0 and max_weight > 0.02) or portfolio_size < 20:\n",
    "        problematic_dates.append({\n",
    "            'Date': date_col,\n",
    "            'Weight_Sum': weight_sum,\n",
    "            'Portfolio_Size': portfolio_size,\n",
    "            'Max_Weight': max_weight,\n",
    "            'Max_Weight_Ticker': df_weights.loc[df_weights[date_col].idxmax(), 'Company_Ticker'] if portfolio_size > 0 else 'N/A'\n",
    "        })\n",
    "if problematic_dates:\n",
    "    print(f\"   Found {len(problematic_dates)} potentially problematic dates:\")\n",
    "    for issue in problematic_dates[:10]:\n",
    "        print(f\"   - {issue['Date']}: Sum={issue['Weight_Sum']:.4f}, Size={issue['Portfolio_Size']}, MaxWeight={issue['Max_Weight']:.4f} ({issue['Max_Weight_Ticker']})\")\n",
    "    if len(problematic_dates) > 10:\n",
    "        print(f\"   ... and {len(problematic_dates) - 10} more\")\n",
    "else:\n",
    "    print(\"   ✓ No major issues detected!\")\n",
    "\n",
    "print(f\"\\n6. TOP 20 STOCKS DETAILED ANALYSIS:\")\n",
    "if date_columns:\n",
    "    avg_weights = df_weights[date_columns].mean(axis=1)\n",
    "    df_weights['Avg_Weight'] = avg_weights\n",
    "    top_20_analysis = df_weights.nlargest(20, 'Avg_Weight')[['PERMNO', 'Company_Ticker', 'Avg_Weight']].copy()\n",
    "    for idx, row in top_20_analysis.iterrows():\n",
    "        ticker = row['Company_Ticker']\n",
    "        weights_series = df_weights.loc[idx, date_columns]\n",
    "        non_zero_periods = weights_series.notna().sum()\n",
    "        first_appear = weights_series.first_valid_index()\n",
    "        last_appear = weights_series.last_valid_index()\n",
    "        max_weight = weights_series.max()\n",
    "        print(f\"   {ticker[:50]:<50} | Avg: {row['Avg_Weight']:.6f} | Periods: {non_zero_periods}/{len(date_columns)} | Max: {max_weight:.6f} | {first_appear} to {last_appear}\")\n",
    "else:\n",
    "    print(\"   No rebalance data to analyze.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1024ee05",
   "metadata": {},
   "source": [
    "## Understanding HRP Weight Behavior\n",
    "\n",
    "**Normal HRP Characteristics:**\n",
    "- **Sparse allocations**: Stocks rotate in/out based on data availability and filters\n",
    "- **Correlation-driven concentration**: Market regimes affect concentration levels\n",
    "- **Quarterly rebalancing**: Produces discrete jumps in weight allocation\n",
    "- **Diversification**: Typically hundreds of positions with small individual weights\n",
    "\n",
    "**Data Quality Indicators:**\n",
    "- Weight sums should equal 1.0 (±0.01 tolerance)\n",
    "- Portfolio sizes vary with data availability (typically 1000-6000 stocks)\n",
    "- Individual weights usually < 1% in large universes\n",
    "- Sparsity patterns reflect rolling window data requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7adc23bf",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook implements GPU-accelerated Hierarchical Risk Parity (HRP) with SciPy single-linkage clustering.\n",
    "\n",
    "**Key Features:**\n",
    "1. **GPU Acceleration**: Covariance, correlation, and distance computations run on CUDA\n",
    "2. **Exact Clustering**: SciPy's `linkage(method='single')` with precomputed distance matrix\n",
    "3. **Full Dataset**: Processes all quarterly rebalance dates (1980-present, 150+ windows)\n",
    "4. **Robust Implementation**: Ledoit-Wolf covariance shrinkage with proper numpy array conversions\n",
    "\n",
    "**Output Files:**\n",
    "- `hrp_weights_all_scipy.csv`: HRP weights for all quarters\n",
    "- Performance comparison plots vs CRSP benchmarks\n",
    "- Detailed timing statistics for optimization analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
