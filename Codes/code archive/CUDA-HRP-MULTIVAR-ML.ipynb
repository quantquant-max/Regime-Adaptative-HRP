{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d3e64de",
   "metadata": {},
   "source": [
    "# CUDA HRP Multi-Variant ML Super Agent\n",
    "\n",
    "## Key Issues to Address:\n",
    "1. **Label Leakage**: Features and labels must be properly time-aligned to avoid look-ahead bias\n",
    "2. **Sharpe Calculation**: Need consistent return frequency for annualization\n",
    "3. **Label Encoding**: Ensure y_train contains class indices (0-4), not window values\n",
    "4. **Missing Data**: Handle NaN values in economic features\n",
    "\n",
    "## Recommended Approach:\n",
    "- Shift features back by one period (use t-1 features to predict t performance)\n",
    "- Standardize feature scaling\n",
    "- Add proper handling for missing economic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f50858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mode: CPU\n",
      "Output Directory: Super_Agent_Output\n"
     ]
    }
   ],
   "source": [
    "# Cell 0: Install Required Packages (Run if not already installed)\n",
    "\n",
    "# Note: ta (not ta-lib) is easier to install on Windows\n",
    "!pip install cupy-cuda12x scikit-learn scipy tqdm pandas numpy matplotlib xgboost ta joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11651d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mode: CPU\n",
      "Output Directory: C:/Users/lucas/OneDrive/Bureau/HRP/Super_Agent_Output\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports and Setup\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost as xgb\n",
    "from sklearn.decomposition import PCA\n",
    "import ta  # For technical indicators like MACD, RSI\n",
    "import random\n",
    "import joblib\n",
    "import sys\n",
    "\n",
    "# Add the path to the utils directory\n",
    "sys.path.append(r'C:/Users/lucas/OneDrive/Bureau/HRP/utils')\n",
    "\n",
    "# Import HRP functions from external script\n",
    "from hrp_functions import compute_hrp_weights\n",
    "\n",
    "# GPU Check (from hrp_functions, but redefine if needed)\n",
    "try:\n",
    "    import cupy as cp\n",
    "    GPU_AVAILABLE = True\n",
    "except ImportError:\n",
    "    GPU_AVAILABLE = False\n",
    "\n",
    "data_path = r'C:/Users/lucas/OneDrive/Bureau/HRP/DATA (CRSP)/PREPROCESSED DATA/ADA-HRP-Preprocessed-DATA.csv'\n",
    "super_agent_dir = r'C:/Users/lucas/OneDrive/Bureau/HRP/Super_Agent_Output'\n",
    "prep_path = r'C:/Users/lucas/OneDrive/Bureau/HRP/DATA (CRSP)/PREPROCESSED DATA'\n",
    "os.makedirs(super_agent_dir, exist_ok=True)\n",
    "\n",
    "windows = [1, 3, 6, 12, 24, 36]  # Months - ADDED 3 MONTH VARIANT\n",
    "rebalance_freq = '3M'\n",
    "min_stocks = 20\n",
    "market_index = '84398.0'  # PERMNO for SPY as market proxy\n",
    "\n",
    "# Removed USEPUINDXD as it's not in preprocessed data\n",
    "series_list = ['CPIAUCSL', 'GDPC1', 'M2SL', 'PPIACO', 'T10Y2Y', 'UNRATE', 'VIXCLS']\n",
    "\n",
    "print(f\"Mode: {'GPU (CUDA)' if GPU_AVAILABLE else 'CPU'}\")\n",
    "print(f\"Output Directory: {super_agent_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f835dc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data: 7670 rows, 1190 columns\n",
      "Stocks: 7668 securities\n",
      "Quarterly rebalance dates: 396\n",
      "Valid rebal dates for all windows: 384\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Load Data and Prepare Dates\n",
    "\n",
    "df = pd.read_csv(data_path)\n",
    "print(f\"Loaded data: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "\n",
    "date_cols = [col for col in df.columns if col not in ['PERMNO', 'Company_Ticker']]\n",
    "parsed_dates = pd.to_datetime([col.replace('_', ':') for col in date_cols], errors='coerce')\n",
    "sort_order = np.argsort(parsed_dates)\n",
    "date_cols = [date_cols[i] for i in sort_order]\n",
    "dates = parsed_dates[sort_order]\n",
    "\n",
    "for col in date_cols:\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "stocks_df = df[df['PERMNO'].notna()].copy()\n",
    "print(f\"Stocks: {len(stocks_df)} securities\")\n",
    "\n",
    "def get_quarterly_dates(dates):\n",
    "    df_dates = pd.DataFrame({'date': dates})\n",
    "    df_dates['year'] = df_dates['date'].dt.year\n",
    "    df_dates['quarter'] = df_dates['date'].dt.quarter\n",
    "    quarterly_ends = df_dates.groupby(['year', 'quarter'])['date'].max()\n",
    "    return quarterly_ends.tolist()\n",
    "\n",
    "quarterly_rebalance_dates = get_quarterly_dates(dates)\n",
    "print(f\"Quarterly rebalance dates: {len(quarterly_rebalance_dates)}\")\n",
    "\n",
    "# Global returns matrix (rows: dates, columns: PERMNO)\n",
    "returns_all = stocks_df[date_cols].T\n",
    "returns_all.columns = stocks_df['PERMNO'].astype(str)\n",
    "returns_all.index = dates\n",
    "\n",
    "# SAFETY CHECK: Verify market index exists\n",
    "if market_index not in returns_all.columns:\n",
    "    print(f\"WARNING: Market index {market_index} not found!\")\n",
    "    print(f\"Available PERMNOs: {sorted(returns_all.columns)[:10]}...\")\n",
    "    raise ValueError(f\"Market index PERMNO {market_index} not in data. Please verify.\")\n",
    "else:\n",
    "    print(f\"✓ Market index {market_index} found in data\")\n",
    "\n",
    "# Filter valid rebal dates with enough history\n",
    "max_win = max(windows)\n",
    "valid_rebal_dates = [d for d in quarterly_rebalance_dates if (d - dates.min()).days / 30 >= max_win]\n",
    "print(f\"Valid rebal dates for all windows: {len(valid_rebal_dates)}\")\n",
    "\n",
    "# Load economic data\n",
    "econ_dict = {}\n",
    "for series in series_list:\n",
    "    file = os.path.join(prep_path, f'{series}_quarterly.csv')\n",
    "    if os.path.exists(file):\n",
    "        df_econ = pd.read_csv(file, index_col='DATE', parse_dates=True)\n",
    "        econ_dict[series] = df_econ[series]\n",
    "        print(f\"✓ Loaded {series}: {len(df_econ)} observations\")\n",
    "    else:\n",
    "        print(f\"✗ Missing: {series}\")\n",
    "        \n",
    "print(f\"\\nLoaded {len(econ_dict)}/{len(series_list)} economic series\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026dcaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Define Feature Extraction Function\n",
    "\n",
    "def extract_features(returns_all, rebal_date, econ_dict=None, market_col=market_index, roll_days_vol=20, roll_days_corr=63):\n",
    "    market_returns = returns_all[market_col].loc[:rebal_date].dropna()\n",
    "    if len(market_returns) < roll_days_vol:\n",
    "        return pd.Series()  # Empty if insufficient data\n",
    "    \n",
    "    # Volatility\n",
    "    vol = market_returns.tail(roll_days_vol).std()\n",
    "    \n",
    "    # MACD\n",
    "    macd = ta.trend.MACD(market_returns)\n",
    "    macd_val = macd.macd().iloc[-1]\n",
    "    \n",
    "    # Mean correlation (on all assets, last 3 months)\n",
    "    recent_returns = returns_all.loc[:rebal_date].tail(roll_days_corr).dropna(axis=1, how='all')\n",
    "    if recent_returns.shape[1] > 1:\n",
    "        corr_matrix = recent_returns.corr()\n",
    "        corr_mean = corr_matrix.mean().mean()\n",
    "    else:\n",
    "        corr_mean = np.nan\n",
    "    \n",
    "    # RSI\n",
    "    rsi = ta.momentum.RSIIndicator(market_returns).rsi().iloc[-1]\n",
    "    \n",
    "    # Max Drawdown (last quarter)\n",
    "    cum_ret = (1 + market_returns.tail(63)).cumprod()\n",
    "    dd = (cum_ret.cummax() - cum_ret).max() / cum_ret.cummax().iloc[-1]\n",
    "    \n",
    "    # Lag return (last quarter)\n",
    "    lag_ret = market_returns.tail(63).sum()\n",
    "    \n",
    "    row_features = pd.Series({\n",
    "        'vol': vol,\n",
    "        'macd': macd_val,\n",
    "        'corr_mean': corr_mean,\n",
    "        'rsi': rsi,\n",
    "        'drawdown': dd,\n",
    "        'lag_return': lag_ret\n",
    "    })\n",
    "    \n",
    "    # Add economic features with forward-fill for missing values\n",
    "    if econ_dict:\n",
    "        for name, series in econ_dict.items():\n",
    "            prev_data = series[series.index <= rebal_date]\n",
    "            if not prev_data.empty:\n",
    "                row_features[name] = prev_data.iloc[-1]\n",
    "            else:\n",
    "                # Use first available value if no data before rebal_date\n",
    "                if not series.empty:\n",
    "                    row_features[name] = series.iloc[0]\n",
    "                else:\n",
    "                    row_features[name] = 0.0  # Default to 0 instead of NaN\n",
    "    \n",
    "    return row_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6d25a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rebalances:  68%|██████▊   | 262/384 [00:07<00:03, 34.44it/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     18\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m     weights = \u001b[43mcompute_hrp_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwindow_returns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     23\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mHRP failed for win=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwin\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, date=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrebal_date\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Users/lucas/OneDrive/Bureau/HRP/utils\\hrp_functions.py:155\u001b[39m, in \u001b[36mcompute_hrp_weights\u001b[39m\u001b[34m(returns_df, debug)\u001b[39m\n\u001b[32m    153\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    154\u001b[39m     returns_gpu = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m155\u001b[39m cov_array, shrinkage, cov_gpu = \u001b[43mcompute_covariance_gpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreturns_np\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturns_gpu\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    156\u001b[39m cov = pd.DataFrame(cov_array, index=valid_assets, columns=valid_assets)\n\u001b[32m    158\u001b[39m \u001b[38;5;66;03m# Compute correlation\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Users/lucas/OneDrive/Bureau/HRP/utils\\hrp_functions.py:42\u001b[39m, in \u001b[36mcompute_covariance_gpu\u001b[39m\u001b[34m(returns_np, returns_gpu)\u001b[39m\n\u001b[32m     40\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Compute shrunk covariance using Ledoit-Wolf\"\"\"\u001b[39;00m\n\u001b[32m     41\u001b[39m lw = LedoitWolf()\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m \u001b[43mlw\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreturns_np\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m cov_array = lw.covariance_\n\u001b[32m     44\u001b[39m shrinkage = lw.shrinkage_\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lucas\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lucas\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\covariance\\_shrunk_covariance.py:610\u001b[39m, in \u001b[36mLedoitWolf.fit\u001b[39m\u001b[34m(self, X, y)\u001b[39m\n\u001b[32m    606\u001b[39m covariance, shrinkage = _ledoit_wolf(\n\u001b[32m    607\u001b[39m     X - \u001b[38;5;28mself\u001b[39m.location_, assume_centered=\u001b[38;5;28;01mTrue\u001b[39;00m, block_size=\u001b[38;5;28mself\u001b[39m.block_size\n\u001b[32m    608\u001b[39m )\n\u001b[32m    609\u001b[39m \u001b[38;5;28mself\u001b[39m.shrinkage_ = shrinkage\n\u001b[32m--> \u001b[39m\u001b[32m610\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_set_covariance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcovariance\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    612\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lucas\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\covariance\\_empirical_covariance.py:215\u001b[39m, in \u001b[36mEmpiricalCovariance._set_covariance\u001b[39m\u001b[34m(self, covariance)\u001b[39m\n\u001b[32m    213\u001b[39m \u001b[38;5;66;03m# set precision\u001b[39;00m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.store_precision:\n\u001b[32m--> \u001b[39m\u001b[32m215\u001b[39m     \u001b[38;5;28mself\u001b[39m.precision_ = \u001b[43mlinalg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpinvh\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcovariance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_finite\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    217\u001b[39m     \u001b[38;5;28mself\u001b[39m.precision_ = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lucas\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\scipy\\linalg\\_basic.py:1700\u001b[39m, in \u001b[36mpinvh\u001b[39m\u001b[34m(a, atol, rtol, lower, return_rank, check_finite)\u001b[39m\n\u001b[32m   1698\u001b[39m a = _asarray_validated(a, check_finite=check_finite)\n\u001b[32m   1699\u001b[39m s, u = _decomp.eigh(a, lower=lower, check_finite=\u001b[38;5;28;01mFalse\u001b[39;00m, driver=\u001b[33m'\u001b[39m\u001b[33mev\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1700\u001b[39m t = \u001b[43mu\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchar\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1701\u001b[39m maxS = np.max(np.abs(s), initial=\u001b[32m0.\u001b[39m)\n\u001b[32m   1703\u001b[39m atol = \u001b[32m0.\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m atol \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m atol\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Cell 4: Precompute Variant Performances and Features (FIXED: No label leakage)\n",
    "\n",
    "sharpes = pd.DataFrame(index=valid_rebal_dates, columns=windows)\n",
    "past_sharpes = pd.DataFrame(index=valid_rebal_dates, columns=windows)\n",
    "features_df = pd.DataFrame(index=valid_rebal_dates)\n",
    "\n",
    "prev_rebal = None\n",
    "prev_weights = {}\n",
    "\n",
    "# Checkpoint: Check if we can resume from saved state\n",
    "checkpoint_file = os.path.join(super_agent_dir, 'cell4_checkpoint.pkl')\n",
    "if os.path.exists(checkpoint_file):\n",
    "    print(f\"Found checkpoint file. Loading previous progress...\")\n",
    "    checkpoint = joblib.load(checkpoint_file)\n",
    "    sharpes = checkpoint['sharpes']\n",
    "    past_sharpes = checkpoint['past_sharpes']\n",
    "    features_df = checkpoint['features_df']\n",
    "    prev_weights = checkpoint['prev_weights']\n",
    "    prev_rebal = checkpoint['prev_rebal']\n",
    "    start_idx = checkpoint['last_completed_idx'] + 1\n",
    "    print(f\"Resuming from rebalance {start_idx}/{len(valid_rebal_dates)}\")\n",
    "else:\n",
    "    start_idx = 0\n",
    "    print(\"Starting from scratch...\")\n",
    "\n",
    "for idx, rebal_date in enumerate(tqdm(valid_rebal_dates[start_idx:], desc=\"Processing rebalances\", initial=start_idx, total=len(valid_rebal_dates))):\n",
    "    current_weights = {}\n",
    "    \n",
    "    # Compute lagged Sharpes if previous rebalance exists\n",
    "    if prev_rebal is not None:\n",
    "        past_start = prev_rebal + pd.DateOffset(days=1)\n",
    "        past_end = rebal_date\n",
    "        past_returns_df = returns_all.loc[past_start:past_end]\n",
    "        for win in windows:\n",
    "            if win in prev_weights:\n",
    "                weights_prev = prev_weights[win]\n",
    "                port_past = past_returns_df.reindex(columns=weights_prev.index, fill_value=0).dot(weights_prev)\n",
    "                if len(port_past) > 1 and port_past.std() > 0:\n",
    "                    # Annualized Sharpe (assuming daily returns)\n",
    "                    sharpe_past = (port_past.mean() / port_past.std()) * np.sqrt(252)\n",
    "                    past_sharpes.loc[rebal_date, win] = sharpe_past\n",
    "                else:\n",
    "                    past_sharpes.loc[rebal_date, win] = np.nan\n",
    "    \n",
    "    # Extract features including economic data\n",
    "    row_features = extract_features(returns_all, rebal_date, econ_dict=econ_dict)\n",
    "    \n",
    "    # Add lagged Sharpes as features\n",
    "    lagged_sharpes = past_sharpes.loc[rebal_date].rename(lambda w: f'lagged_sharpe_{w}')\n",
    "    row_features = pd.concat([row_features, lagged_sharpes])\n",
    "    \n",
    "    features_df.loc[rebal_date] = row_features\n",
    "    \n",
    "    # Compute forward Sharpes for labels\n",
    "    for win in windows:\n",
    "        start = rebal_date - pd.DateOffset(months=win)\n",
    "        window_returns = returns_all.loc[start:rebal_date].dropna(axis=1, how='all')\n",
    "        if window_returns.shape[1] < min_stocks:\n",
    "            continue\n",
    "        \n",
    "        weights = compute_hrp_weights(window_returns)\n",
    "        current_weights[win] = weights\n",
    "        \n",
    "        next_start = rebal_date + pd.DateOffset(days=1)\n",
    "        next_end = rebal_date + pd.DateOffset(months=3)\n",
    "        next_returns_df = returns_all.loc[next_start:next_end].reindex(columns=weights.index, fill_value=0)\n",
    "        port_returns = next_returns_df.dot(weights)\n",
    "        \n",
    "        if len(port_returns) > 1 and port_returns.std() > 0:\n",
    "            sharpe = (port_returns.mean() / port_returns.std()) * np.sqrt(252)\n",
    "        else:\n",
    "            sharpe = np.nan\n",
    "        \n",
    "        sharpes.loc[rebal_date, win] = sharpe\n",
    "    \n",
    "    prev_weights = current_weights\n",
    "    prev_rebal = rebal_date\n",
    "    \n",
    "    # Save checkpoint every 10 iterations\n",
    "    if (start_idx + idx) % 10 == 0:\n",
    "        checkpoint = {\n",
    "            'sharpes': sharpes,\n",
    "            'past_sharpes': past_sharpes,\n",
    "            'features_df': features_df,\n",
    "            'prev_weights': prev_weights,\n",
    "            'prev_rebal': prev_rebal,\n",
    "            'last_completed_idx': start_idx + idx\n",
    "        }\n",
    "        joblib.dump(checkpoint, checkpoint_file)\n",
    "\n",
    "# Labels: best window per date (these are the FORWARD Sharpes)\n",
    "labels_raw = sharpes.idxmax(axis=1)\n",
    "\n",
    "# FIX LABEL LEAKAGE: Shift labels forward by 1 period\n",
    "# Features at time t should predict the best window for period t+1\n",
    "labels = labels_raw.shift(-1).dropna()\n",
    "\n",
    "# Also shift sharpes to align\n",
    "sharpes_aligned = sharpes.shift(-1).dropna()\n",
    "\n",
    "print(f\"Total samples after alignment: {len(labels)}\")\n",
    "print(f\"Label distribution:\\n{labels.value_counts().sort_index()}\")\n",
    "\n",
    "# SAVE FINAL RESULTS\n",
    "print(\"\\nSaving Cell 4 outputs...\")\n",
    "sharpes.to_csv(os.path.join(super_agent_dir, 'forward_sharpes_by_window.csv'))\n",
    "past_sharpes.to_csv(os.path.join(super_agent_dir, 'lagged_sharpes_by_window.csv'))\n",
    "features_df.to_csv(os.path.join(super_agent_dir, 'features_all_rebalances.csv'))\n",
    "labels.to_csv(os.path.join(super_agent_dir, 'labels_best_window.csv'))\n",
    "joblib.dump(prev_weights, os.path.join(super_agent_dir, 'final_weights_dict.pkl'))\n",
    "\n",
    "# Clean up checkpoint\n",
    "if os.path.exists(checkpoint_file):\n",
    "    os.remove(checkpoint_file)\n",
    "    \n",
    "print(\"✓ All Cell 4 outputs saved successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100cb2a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labeled 0 dates\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Train Classifier (FIXED: No look-ahead bias in scaling)\n",
    "\n",
    "# Combine features and labels (now properly time-aligned)\n",
    "data = pd.concat([features_df, labels.rename('label')], axis=1).dropna()\n",
    "\n",
    "# FIX LABEL ENCODING: Convert window values to class indices\n",
    "window_to_class = {win: idx for idx, win in enumerate(windows)}\n",
    "class_to_window = {idx: win for idx, win in enumerate(windows)}\n",
    "data['label'] = data['label'].map(window_to_class)\n",
    "\n",
    "print(f\"Label encoding: {window_to_class}\")\n",
    "print(f\"Class distribution:\\n{data['label'].value_counts().sort_index()}\")\n",
    "\n",
    "# Train/test split\n",
    "train_end_idx = int(len(data) * 0.8)\n",
    "train_end = data.index[train_end_idx]\n",
    "train = data.loc[:train_end]\n",
    "test = data.loc[train_end:]\n",
    "\n",
    "X_train = train.drop('label', axis=1)\n",
    "y_train = train['label']\n",
    "X_test = test.drop('label', axis=1)\n",
    "y_test = test['label']\n",
    "\n",
    "# FIX LOOK-AHEAD BIAS: Use expanding window for scaling\n",
    "# Fit scaler only on initial training data, then update during backtest\n",
    "initial_scaler = StandardScaler()\n",
    "X_train_scaled = pd.DataFrame(\n",
    "    initial_scaler.fit_transform(X_train),\n",
    "    index=X_train.index,\n",
    "    columns=X_train.columns\n",
    ")\n",
    "\n",
    "# For test set, we'll scale dynamically during backtest (see Cell 6)\n",
    "# Here we just create a placeholder using the initial scaler for grid search\n",
    "X_test_scaled = pd.DataFrame(\n",
    "    initial_scaler.transform(X_test),\n",
    "    index=X_test.index,\n",
    "    columns=X_test.columns\n",
    ")\n",
    "\n",
    "# Save initial scaler\n",
    "joblib.dump(initial_scaler, os.path.join(super_agent_dir, 'initial_scaler.pkl'))\n",
    "\n",
    "# Train model on training data\n",
    "model = xgb.XGBClassifier(\n",
    "    n_estimators=200,\n",
    "    tree_method='gpu_hist' if GPU_AVAILABLE else 'hist',\n",
    "    random_state=42\n",
    ")\n",
    "params = {'max_depth': [3, 5, 7], 'learning_rate': [0.01, 0.1, 0.3]}\n",
    "cv = TimeSeriesSplit(n_splits=5)\n",
    "grid = GridSearchCV(model, params, cv=cv, scoring='accuracy')\n",
    "grid.fit(X_train_scaled, y_train)\n",
    "best_model = grid.best_estimator_\n",
    "\n",
    "# Save model and mapping\n",
    "joblib.dump(best_model, os.path.join(super_agent_dir, 'super_agent_model.pkl'))\n",
    "joblib.dump(class_to_window, os.path.join(super_agent_dir, 'class_to_window.pkl'))\n",
    "\n",
    "print(f\"Best params: {grid.best_params_}\")\n",
    "print(f\"Train score: {best_model.score(X_train_scaled, y_train):.2f}\")\n",
    "print(f\"Test score (with initial scaler): {best_model.score(X_test_scaled, y_test):.2f}\")\n",
    "print(\"\\nNote: Test score here uses initial scaler. Backtest will use expanding window scaling.\")\n",
    "\n",
    "# SAVE TRAINING DATA AND METADATA\n",
    "print(\"\\nSaving Cell 5 outputs...\")\n",
    "X_train.to_csv(os.path.join(super_agent_dir, 'X_train.csv'))\n",
    "y_train.to_csv(os.path.join(super_agent_dir, 'y_train.csv'))\n",
    "X_test.to_csv(os.path.join(super_agent_dir, 'X_test.csv'))\n",
    "y_test.to_csv(os.path.join(super_agent_dir, 'y_test.csv'))\n",
    "\n",
    "# Save grid search results\n",
    "cv_results_df = pd.DataFrame(grid.cv_results_)\n",
    "cv_results_df.to_csv(os.path.join(super_agent_dir, 'grid_search_cv_results.csv'), index=False)\n",
    "\n",
    "# Save metadata\n",
    "metadata = {\n",
    "    'windows': windows,\n",
    "    'window_to_class': window_to_class,\n",
    "    'class_to_window': class_to_window,\n",
    "    'best_params': grid.best_params_,\n",
    "    'train_end_date': train_end,\n",
    "    'test_start_date': test.index[0],\n",
    "    'n_train_samples': len(train),\n",
    "    'n_test_samples': len(test),\n",
    "    'n_features': X_train.shape[1],\n",
    "    'feature_names': list(X_train.columns)\n",
    "}\n",
    "joblib.dump(metadata, os.path.join(super_agent_dir, 'training_metadata.pkl'))\n",
    "\n",
    "print(\"✓ All Cell 5 outputs saved successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ca0b7b",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m data = pd.concat([features_df, labels.rename(\u001b[33m'\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m'\u001b[39m)], axis=\u001b[32m1\u001b[39m).dropna()\n\u001b[32m      4\u001b[39m train_end_idx = \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(data) * \u001b[32m0.8\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m train_end = \u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_end_idx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m      6\u001b[39m train = data.loc[:train_end]\n\u001b[32m      7\u001b[39m test = data.loc[train_end:]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lucas\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:5389\u001b[39m, in \u001b[36mIndex.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   5386\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(key) \u001b[38;5;129;01mor\u001b[39;00m is_float(key):\n\u001b[32m   5387\u001b[39m     \u001b[38;5;66;03m# GH#44051 exclude bool, which would return a 2d ndarray\u001b[39;00m\n\u001b[32m   5388\u001b[39m     key = com.cast_scalar_indexer(key)\n\u001b[32m-> \u001b[39m\u001b[32m5389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgetitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5391\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mslice\u001b[39m):\n\u001b[32m   5392\u001b[39m     \u001b[38;5;66;03m# This case is separated from the conditional above to avoid\u001b[39;00m\n\u001b[32m   5393\u001b[39m     \u001b[38;5;66;03m# pessimization com.is_bool_indexer and ndim checks.\u001b[39;00m\n\u001b[32m   5394\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_slice(key)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lucas\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\arrays\\datetimelike.py:381\u001b[39m, in \u001b[36mDatetimeLikeArrayMixin.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m    374\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    375\u001b[39m \u001b[33;03mThis getitem defers to the underlying array, which by-definition can\u001b[39;00m\n\u001b[32m    376\u001b[39m \u001b[33;03monly handle list-likes, slices, and integer scalars\u001b[39;00m\n\u001b[32m    377\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    378\u001b[39m \u001b[38;5;66;03m# Use cast as we know we will get back a DatetimeLikeArray or DTScalar,\u001b[39;00m\n\u001b[32m    379\u001b[39m \u001b[38;5;66;03m# but skip evaluating the Union at runtime for performance\u001b[39;00m\n\u001b[32m    380\u001b[39m \u001b[38;5;66;03m# (see https://github.com/pandas-dev/pandas/pull/44624)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m381\u001b[39m result = cast(\u001b[33m\"\u001b[39m\u001b[33mUnion[Self, DTScalarOrNaT]\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    382\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m lib.is_scalar(result):\n\u001b[32m    383\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lucas\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\arrays\\_mixins.py:284\u001b[39m, in \u001b[36mNDArrayBackedExtensionArray.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m    278\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitem__\u001b[39m(\n\u001b[32m    279\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    280\u001b[39m     key: PositionalIndexer2D,\n\u001b[32m    281\u001b[39m ) -> Self | Any:\n\u001b[32m    282\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m lib.is_integer(key):\n\u001b[32m    283\u001b[39m         \u001b[38;5;66;03m# fast-path\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m284\u001b[39m         result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_ndarray\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    285\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ndim == \u001b[32m1\u001b[39m:\n\u001b[32m    286\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._box_func(result)\n",
      "\u001b[31mIndexError\u001b[39m: index 0 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "# Cell 6: Backtest Super Agent (FIXED: True expanding window - strictly past data only)\n",
    "\n",
    "super_returns = []\n",
    "baseline_returns = []\n",
    "super_dates = []\n",
    "baseline_dates = []\n",
    "predicted_windows_log = []  # Track which window was selected at each date\n",
    "\n",
    "win_fixed = 12\n",
    "\n",
    "# Combine train and test features for expanding window scaling\n",
    "all_features = pd.concat([X_train, X_test])\n",
    "\n",
    "print(\"Starting backtest with expanding window scaling (strictly past data)...\")\n",
    "for idx, t in enumerate(tqdm(test.index, desc=\"Backtesting\")):\n",
    "    # CRITICAL FIX: Only use data BEFORE current time t (not including t)\n",
    "    # This ensures zero look-ahead bias\n",
    "    historical_data = all_features[all_features.index < t]\n",
    "    \n",
    "    # If we don't have enough historical data, skip\n",
    "    if len(historical_data) < 10:\n",
    "        continue\n",
    "    \n",
    "    # Fit scaler on historical data only (strictly past)\n",
    "    scaler_t = StandardScaler()\n",
    "    scaler_t.fit(historical_data)\n",
    "    \n",
    "    # Scale only the current observation\n",
    "    features_t_scaled = scaler_t.transform(X_test.loc[[t]])\n",
    "    \n",
    "    # Make prediction with scaled features\n",
    "    pred_class = best_model.predict(features_t_scaled)[0]\n",
    "    win_pred = class_to_window[pred_class]\n",
    "    \n",
    "    # Log the prediction\n",
    "    predicted_windows_log.append({'date': t, 'predicted_window': win_pred})\n",
    "    \n",
    "    # Super agent\n",
    "    start = t - pd.DateOffset(months=win_pred)\n",
    "    window_returns = returns_all.loc[start:t].dropna(axis=1, how='all')\n",
    "    if window_returns.shape[1] >= min_stocks:\n",
    "        weights = compute_hrp_weights(window_returns)\n",
    "        next_start = t + pd.DateOffset(days=1)\n",
    "        next_end = t + pd.DateOffset(months=3)\n",
    "        next_returns_df = returns_all.loc[next_start:next_end].reindex(columns=weights.index, fill_value=0)\n",
    "        \n",
    "        # Calculate daily portfolio returns\n",
    "        daily_port_returns = next_returns_df.dot(weights)\n",
    "        \n",
    "        # Store mean daily return for this period\n",
    "        for date, ret in daily_port_returns.items():\n",
    "            super_returns.append(ret)\n",
    "            super_dates.append(date)\n",
    "    \n",
    "    # Baseline (fixed 12m)\n",
    "    start_fixed = t - pd.DateOffset(months=win_fixed)\n",
    "    window_fixed = returns_all.loc[start_fixed:t].dropna(axis=1, how='all')\n",
    "    if window_fixed.shape[1] >= min_stocks:\n",
    "        weights_fixed = compute_hrp_weights(window_fixed)\n",
    "        next_start = t + pd.DateOffset(days=1)\n",
    "        next_end = t + pd.DateOffset(months=3)\n",
    "        next_returns_fixed = returns_all.loc[next_start:next_end].reindex(columns=weights_fixed.index, fill_value=0)\n",
    "        \n",
    "        # Calculate daily portfolio returns\n",
    "        daily_port_returns_fixed = next_returns_fixed.dot(weights_fixed)\n",
    "        \n",
    "        for date, ret in daily_port_returns_fixed.items():\n",
    "            baseline_returns.append(ret)\n",
    "            baseline_dates.append(date)\n",
    "\n",
    "# Convert to Series with proper dates\n",
    "super_returns = pd.Series(super_returns, index=super_dates).sort_index()\n",
    "baseline_returns = pd.Series(baseline_returns, index=baseline_dates).sort_index()\n",
    "\n",
    "# Remove duplicates (if any overlap in prediction periods)\n",
    "super_returns = super_returns[~super_returns.index.duplicated(keep='first')]\n",
    "baseline_returns = baseline_returns[~baseline_returns.index.duplicated(keep='first')]\n",
    "\n",
    "# Cumulative returns\n",
    "cum_super = (1 + super_returns).cumprod()\n",
    "cum_baseline = (1 + baseline_returns).cumprod()\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(cum_super, label='Super Agent (No Look-Ahead)', linewidth=2)\n",
    "plt.plot(cum_baseline, label='Baseline HRP 12m', linewidth=2)\n",
    "plt.title('Backtest Equity Curves (Out-of-Sample, Zero Look-Ahead Bias)', fontsize=14)\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Cumulative Return')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(super_agent_dir, 'backtest_equity_curves.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Compute annualized Sharpe ratio from daily returns\n",
    "def compute_sharpe(returns):\n",
    "    \"\"\"Compute annualized Sharpe ratio from daily returns\"\"\"\n",
    "    if returns.std() == 0 or len(returns) == 0:\n",
    "        return np.nan\n",
    "    return (returns.mean() / returns.std()) * np.sqrt(252)  # 252 trading days\n",
    "\n",
    "sharpe_super = compute_sharpe(super_returns)\n",
    "sharpe_baseline = compute_sharpe(baseline_returns)\n",
    "\n",
    "# Additional metrics\n",
    "total_return_super = cum_super.iloc[-1] - 1\n",
    "total_return_baseline = cum_baseline.iloc[-1] - 1\n",
    "max_dd_super = (cum_super.cummax() - cum_super).max() / cum_super.cummax().max()\n",
    "max_dd_baseline = (cum_baseline.cummax() - cum_baseline).max() / cum_baseline.cummax().max()\n",
    "\n",
    "# Compute additional statistics\n",
    "def compute_metrics(returns, name):\n",
    "    \"\"\"Compute comprehensive performance metrics\"\"\"\n",
    "    cum = (1 + returns).cumprod()\n",
    "    metrics = {\n",
    "        'Strategy': name,\n",
    "        'Total Return': cum.iloc[-1] - 1,\n",
    "        'Annualized Return': (cum.iloc[-1] ** (252 / len(returns))) - 1,\n",
    "        'Sharpe Ratio': (returns.mean() / returns.std()) * np.sqrt(252) if returns.std() > 0 else 0,\n",
    "        'Volatility (Ann.)': returns.std() * np.sqrt(252),\n",
    "        'Max Drawdown': (cum.cummax() - cum).max() / cum.cummax().max(),\n",
    "        'Calmar Ratio': ((cum.iloc[-1] ** (252 / len(returns))) - 1) / ((cum.cummax() - cum).max() / cum.cummax().max()) if (cum.cummax() - cum).max() > 0 else 0,\n",
    "        'Win Rate': (returns > 0).sum() / len(returns),\n",
    "        'Avg Win': returns[returns > 0].mean() if (returns > 0).any() else 0,\n",
    "        'Avg Loss': returns[returns < 0].mean() if (returns < 0).any() else 0,\n",
    "        'Best Day': returns.max(),\n",
    "        'Worst Day': returns.min(),\n",
    "        'Total Days': len(returns)\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "metrics_super = compute_metrics(super_returns, 'Super Agent')\n",
    "metrics_baseline = compute_metrics(baseline_returns, 'Baseline HRP 12m')\n",
    "\n",
    "metrics_df = pd.DataFrame([metrics_super, metrics_baseline]).set_index('Strategy')\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BACKTEST RESULTS (Out-of-Sample, Zero Look-Ahead Bias)\")\n",
    "print(\"=\"*70)\n",
    "print(metrics_df.to_string())\n",
    "print(\"=\"*70)\n",
    "\n",
    "# SAVE BACKTEST RESULTS\n",
    "print(\"\\nSaving Cell 6 outputs...\")\n",
    "super_returns.to_csv(os.path.join(super_agent_dir, 'super_agent_returns.csv'))\n",
    "baseline_returns.to_csv(os.path.join(super_agent_dir, 'baseline_returns.csv'))\n",
    "cum_super.to_csv(os.path.join(super_agent_dir, 'super_agent_cumulative.csv'))\n",
    "cum_baseline.to_csv(os.path.join(super_agent_dir, 'baseline_cumulative.csv'))\n",
    "metrics_df.to_csv(os.path.join(super_agent_dir, 'performance_metrics.csv'))\n",
    "\n",
    "# Save predicted windows log\n",
    "predicted_windows_df = pd.DataFrame(predicted_windows_log)\n",
    "predicted_windows_df.to_csv(os.path.join(super_agent_dir, 'predicted_windows_log.csv'), index=False)\n",
    "\n",
    "print(\"✓ All Cell 6 outputs saved successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9a8e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Validation and Iteration\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# For validation, we need to recompute predictions with expanding window scaling\n",
    "print(\"Computing predictions with expanding window scaling (strictly past data)...\")\n",
    "all_features = pd.concat([X_train, X_test])\n",
    "preds = []\n",
    "\n",
    "for t in tqdm(test.index, desc=\"Scaling and predicting\"):\n",
    "    # CRITICAL FIX: Only use data BEFORE time t (not including t)\n",
    "    historical_data = all_features[all_features.index < t]\n",
    "    \n",
    "    if len(historical_data) < 10:\n",
    "        # If not enough history, use a simple prediction (e.g., mode)\n",
    "        preds.append(y_train.mode()[0] if len(y_train.mode()) > 0 else 0)\n",
    "        continue\n",
    "    \n",
    "    scaler_t = StandardScaler()\n",
    "    scaler_t.fit(historical_data)\n",
    "    features_t_scaled = scaler_t.transform(X_test.loc[[t]])\n",
    "    pred = best_model.predict(features_t_scaled)[0]\n",
    "    preds.append(pred)\n",
    "\n",
    "preds = np.array(preds)\n",
    "accuracy = accuracy_score(y_test, preds)\n",
    "print(f\"\\nOut-of-sample accuracy (zero look-ahead bias): {accuracy:.2%}\")\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, preds)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(\"Rows: Actual | Columns: Predicted\")\n",
    "print(f\"Windows: {windows}\")\n",
    "print(cm)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "import seaborn as sns\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=[f'{w}m' for w in windows],\n",
    "            yticklabels=[f'{w}m' for w in windows])\n",
    "plt.title('Confusion Matrix - Window Selection')\n",
    "plt.ylabel('Actual Best Window')\n",
    "plt.xlabel('Predicted Window')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(super_agent_dir, 'confusion_matrix.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Detailed classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "target_names = [f\"{w}m\" for w in windows]\n",
    "class_report = classification_report(y_test, preds, target_names=target_names, zero_division=0, output_dict=True)\n",
    "print(classification_report(y_test, preds, target_names=target_names, zero_division=0))\n",
    "\n",
    "# Feature importance (for XGBoost)\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    importances = pd.Series(best_model.feature_importances_, index=X_train.columns)\n",
    "    top_features = importances.sort_values(ascending=False).head(15)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    top_features.plot(kind='barh')\n",
    "    plt.title('Top 15 Feature Importances')\n",
    "    plt.xlabel('Importance')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(super_agent_dir, 'feature_importances.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nTop 10 Most Important Features:\")\n",
    "    print(top_features.head(10))\n",
    "    \n",
    "    # Save all feature importances\n",
    "    importances.sort_values(ascending=False).to_csv(os.path.join(super_agent_dir, 'all_feature_importances.csv'))\n",
    "\n",
    "# Window selection distribution\n",
    "pred_windows = pd.Series([class_to_window[p] for p in preds])\n",
    "actual_windows = pd.Series([class_to_window[a] for a in y_test])\n",
    "\n",
    "print(f\"\\nPredicted Window Distribution:\")\n",
    "print(pred_windows.value_counts().sort_index())\n",
    "print(f\"\\nActual Best Window Distribution:\")\n",
    "print(actual_windows.value_counts().sort_index())\n",
    "\n",
    "# Plot window distributions\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "actual_windows.value_counts().sort_index().plot(kind='bar', ax=axes[0], color='steelblue')\n",
    "axes[0].set_title('Actual Best Windows (Test Set)')\n",
    "axes[0].set_xlabel('Window (months)')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "pred_windows.value_counts().sort_index().plot(kind='bar', ax=axes[1], color='coral')\n",
    "axes[1].set_title('Predicted Windows (Test Set)')\n",
    "axes[1].set_xlabel('Window (months)')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(super_agent_dir, 'window_distributions.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# SAVE VALIDATION RESULTS\n",
    "print(\"\\nSaving Cell 7 outputs...\")\n",
    "pd.DataFrame({'actual': y_test, 'predicted': preds}, index=y_test.index).to_csv(\n",
    "    os.path.join(super_agent_dir, 'predictions_vs_actual.csv'))\n",
    "\n",
    "pd.DataFrame(cm, \n",
    "             index=[f'{w}m_actual' for w in windows],\n",
    "             columns=[f'{w}m_pred' for w in windows]).to_csv(\n",
    "    os.path.join(super_agent_dir, 'confusion_matrix.csv'))\n",
    "\n",
    "pd.DataFrame(class_report).T.to_csv(os.path.join(super_agent_dir, 'classification_report.csv'))\n",
    "\n",
    "print(\"✓ All Cell 7 outputs saved successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fabf92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Generate Final Summary Report\n",
    "\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"GENERATING FINAL SUMMARY REPORT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create comprehensive summary\n",
    "summary_report = {\n",
    "    'execution_info': {\n",
    "        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'gpu_used': GPU_AVAILABLE,\n",
    "        'total_rebalance_dates': len(valid_rebal_dates),\n",
    "        'train_test_split': '80/20'\n",
    "    },\n",
    "    'model_config': {\n",
    "        'windows': windows,\n",
    "        'model_type': 'XGBoost Classifier',\n",
    "        'best_hyperparameters': metadata['best_params'],\n",
    "        'n_features': metadata['n_features'],\n",
    "        'feature_names': metadata['feature_names']\n",
    "    },\n",
    "    'training_performance': {\n",
    "        'train_accuracy': float(best_model.score(X_train_scaled, y_train)),\n",
    "        'n_train_samples': metadata['n_train_samples'],\n",
    "        'train_period': f\"{metadata['train_end_date'].strftime('%Y-%m-%d')}\"\n",
    "    },\n",
    "    'test_performance': {\n",
    "        'test_accuracy': float(accuracy),\n",
    "        'n_test_samples': metadata['n_test_samples'],\n",
    "        'test_period': f\"{metadata['test_start_date'].strftime('%Y-%m-%d')} onwards\"\n",
    "    },\n",
    "    'backtest_results': {\n",
    "        'super_agent': {\n",
    "            'sharpe_ratio': float(metrics_super['Sharpe Ratio']),\n",
    "            'total_return': float(metrics_super['Total Return']),\n",
    "            'annualized_return': float(metrics_super['Annualized Return']),\n",
    "            'max_drawdown': float(metrics_super['Max Drawdown']),\n",
    "            'volatility': float(metrics_super['Volatility (Ann.)']),\n",
    "            'calmar_ratio': float(metrics_super['Calmar Ratio']),\n",
    "            'win_rate': float(metrics_super['Win Rate'])\n",
    "        },\n",
    "        'baseline_12m': {\n",
    "            'sharpe_ratio': float(metrics_baseline['Sharpe Ratio']),\n",
    "            'total_return': float(metrics_baseline['Total Return']),\n",
    "            'annualized_return': float(metrics_baseline['Annualized Return']),\n",
    "            'max_drawdown': float(metrics_baseline['Max Drawdown']),\n",
    "            'volatility': float(metrics_baseline['Volatility (Ann.)']),\n",
    "            'calmar_ratio': float(metrics_baseline['Calmar Ratio']),\n",
    "            'win_rate': float(metrics_baseline['Win Rate'])\n",
    "        }\n",
    "    },\n",
    "    'improvement_vs_baseline': {\n",
    "        'sharpe_delta': float(metrics_super['Sharpe Ratio'] - metrics_baseline['Sharpe Ratio']),\n",
    "        'return_delta': float(metrics_super['Total Return'] - metrics_baseline['Total Return']),\n",
    "        'sharpe_improvement_pct': float((metrics_super['Sharpe Ratio'] / metrics_baseline['Sharpe Ratio'] - 1) * 100) if metrics_baseline['Sharpe Ratio'] != 0 else None\n",
    "    },\n",
    "    'saved_files': [\n",
    "        'super_agent_model.pkl',\n",
    "        'class_to_window.pkl',\n",
    "        'initial_scaler.pkl',\n",
    "        'training_metadata.pkl',\n",
    "        'forward_sharpes_by_window.csv',\n",
    "        'features_all_rebalances.csv',\n",
    "        'X_train.csv', 'X_test.csv',\n",
    "        'y_train.csv', 'y_test.csv',\n",
    "        'super_agent_returns.csv',\n",
    "        'baseline_returns.csv',\n",
    "        'performance_metrics.csv',\n",
    "        'predicted_windows_log.csv',\n",
    "        'feature_importances.csv',\n",
    "        'classification_report.csv',\n",
    "        'confusion_matrix.csv',\n",
    "        'backtest_equity_curves.png',\n",
    "        'feature_importances.png',\n",
    "        'confusion_matrix.png',\n",
    "        'window_distributions.png'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Save as JSON\n",
    "with open(os.path.join(super_agent_dir, 'SUMMARY_REPORT.json'), 'w') as f:\n",
    "    json.dump(summary_report, f, indent=2)\n",
    "\n",
    "# Save as readable text\n",
    "with open(os.path.join(super_agent_dir, 'SUMMARY_REPORT.txt'), 'w') as f:\n",
    "    f.write(\"=\"*70 + \"\\n\")\n",
    "    f.write(\"SUPER AGENT ML - FINAL SUMMARY REPORT\\n\")\n",
    "    f.write(\"=\"*70 + \"\\n\\n\")\n",
    "    \n",
    "    f.write(f\"Generated: {summary_report['execution_info']['timestamp']}\\n\")\n",
    "    f.write(f\"GPU Acceleration: {'Yes' if GPU_AVAILABLE else 'No'}\\n\\n\")\n",
    "    \n",
    "    f.write(\"-\"*70 + \"\\n\")\n",
    "    f.write(\"MODEL CONFIGURATION\\n\")\n",
    "    f.write(\"-\"*70 + \"\\n\")\n",
    "    f.write(f\"Windows: {windows}\\n\")\n",
    "    f.write(f\"Model: XGBoost Classifier\\n\")\n",
    "    f.write(f\"Best Params: {metadata['best_params']}\\n\")\n",
    "    f.write(f\"Features: {metadata['n_features']}\\n\\n\")\n",
    "    \n",
    "    f.write(\"-\"*70 + \"\\n\")\n",
    "    f.write(\"BACKTEST PERFORMANCE (Out-of-Sample)\\n\")\n",
    "    f.write(\"-\"*70 + \"\\n\")\n",
    "    f.write(f\"Super Agent:\\n\")\n",
    "    f.write(f\"  Sharpe Ratio:       {metrics_super['Sharpe Ratio']:.3f}\\n\")\n",
    "    f.write(f\"  Total Return:       {metrics_super['Total Return']:.2%}\\n\")\n",
    "    f.write(f\"  Annualized Return:  {metrics_super['Annualized Return']:.2%}\\n\")\n",
    "    f.write(f\"  Max Drawdown:       {metrics_super['Max Drawdown']:.2%}\\n\")\n",
    "    f.write(f\"  Volatility (Ann.):  {metrics_super['Volatility (Ann.)']:.2%}\\n\")\n",
    "    f.write(f\"  Calmar Ratio:       {metrics_super['Calmar Ratio']:.3f}\\n\\n\")\n",
    "    \n",
    "    f.write(f\"Baseline HRP 12m:\\n\")\n",
    "    f.write(f\"  Sharpe Ratio:       {metrics_baseline['Sharpe Ratio']:.3f}\\n\")\n",
    "    f.write(f\"  Total Return:       {metrics_baseline['Total Return']:.2%}\\n\")\n",
    "    f.write(f\"  Annualized Return:  {metrics_baseline['Annualized Return']:.2%}\\n\")\n",
    "    f.write(f\"  Max Drawdown:       {metrics_baseline['Max Drawdown']:.2%}\\n\")\n",
    "    f.write(f\"  Volatility (Ann.):  {metrics_baseline['Volatility (Ann.)']:.2%}\\n\")\n",
    "    f.write(f\"  Calmar Ratio:       {metrics_baseline['Calmar Ratio']:.3f}\\n\\n\")\n",
    "    \n",
    "    sharpe_imp = summary_report['improvement_vs_baseline']['sharpe_improvement_pct']\n",
    "    f.write(f\"Improvement:\\n\")\n",
    "    f.write(f\"  Sharpe Δ:           {summary_report['improvement_vs_baseline']['sharpe_delta']:+.3f}\\n\")\n",
    "    f.write(f\"  Return Δ:           {summary_report['improvement_vs_baseline']['return_delta']:+.2%}\\n\")\n",
    "    if sharpe_imp is not None:\n",
    "        f.write(f\"  Sharpe Improvement: {sharpe_imp:+.1f}%\\n\")\n",
    "    \n",
    "    f.write(\"\\n\" + \"-\"*70 + \"\\n\")\n",
    "    f.write(\"SAVED FILES\\n\")\n",
    "    f.write(\"-\"*70 + \"\\n\")\n",
    "    for file in summary_report['saved_files']:\n",
    "        f.write(f\"  ✓ {file}\\n\")\n",
    "    \n",
    "    f.write(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SUMMARY REPORT\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\n📊 BACKTEST RESULTS:\")\n",
    "print(f\"   Super Agent Sharpe: {metrics_super['Sharpe Ratio']:.3f}\")\n",
    "print(f\"   Baseline Sharpe:    {metrics_baseline['Sharpe Ratio']:.3f}\")\n",
    "print(f\"   Improvement:        {summary_report['improvement_vs_baseline']['sharpe_delta']:+.3f}\")\n",
    "\n",
    "if sharpe_imp is not None and sharpe_imp > 0:\n",
    "    print(f\"   🎯 Super Agent outperforms by {sharpe_imp:.1f}%!\")\n",
    "elif sharpe_imp is not None:\n",
    "    print(f\"   ⚠️  Super Agent underperforms by {abs(sharpe_imp):.1f}%\")\n",
    "\n",
    "print(f\"\\n💾 OUTPUT DIRECTORY: {super_agent_dir}\")\n",
    "print(f\"📁 Total files saved: {len(summary_report['saved_files'])}\")\n",
    "print(f\"\\n✅ All results saved successfully!\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
