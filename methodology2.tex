\documentclass[11pt,a4paper]{article}

% ============================================================================
% PACKAGES
% ============================================================================
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{mathtools}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{natbib}
\usepackage{geometry}
\usepackage{enumitem}
\usepackage{xcolor}
\usepackage{float}
\usepackage{multirow}
\usepackage{longtable}

\geometry{margin=1in}

% ============================================================================
% CUSTOM COMMANDS
% ============================================================================
\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\text{Var}}
\newcommand{\Cov}{\text{Cov}}
\newcommand{\Corr}{\text{Corr}}
\newcommand{\tr}{\text{tr}}
\newcommand{\diag}{\text{diag}}
\newcommand{\argmin}{\mathop{\text{argmin}}}
\newcommand{\argmax}{\mathop{\text{argmax}}}
\newcommand{\Prob}{\mathbb{P}}

\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem{remark}{Remark}
\newtheorem{proposition}{Proposition}

% ============================================================================
% TITLE
% ============================================================================
\title{\textbf{Regime-Based HRP Strategy with Machine Learning Prediction}\\[0.5em]
\large Part II: Hidden Markov Models, Feature Engineering, and XGBoost Walk-Forward Prediction}

\author{HRP Research Pipeline Documentation}
\date{January 2025}

\begin{document}

\maketitle

\begin{abstract}
This document provides a comprehensive technical description of the regime-based enhancement to the Hierarchical Risk Parity (HRP) portfolio optimization methodology. We detail a six-phase implementation: (1) ground truth construction using HRP strategy returns, (2) regime identification via Gaussian Hidden Markov Models with expanding-window training and filtered probabilities to eliminate look-ahead bias, (3) feature engineering with fourteen predictive signals spanning cross-sectional, macroeconomic, factor-based, and strategy momentum indicators, (4) feature selection using permutation importance with time-series cross-validation performed inside the walk-forward loop, (5) walk-forward XGBoost prediction with Optuna hyperparameter optimization at each refit, and (6) strategy execution with transaction cost analysis. Statistical inference employs block bootstrap confidence intervals and Sharpe ratio difference tests.
\end{abstract}

\tableofcontents
\newpage

% ============================================================================
% SECTION 1: INTRODUCTION AND MOTIVATION
% ============================================================================
\section{Introduction and Motivation}

\subsection{The Case for Regime-Aware Portfolio Management}

The base HRP methodology (detailed in Methodology Part I) provides robust, diversified portfolio weights without requiring expected return estimates. However, it treats all market conditions identically, allocating capital based solely on the correlation structure without regard to the prevailing market regime.

Financial markets exhibit distinct regimes characterized by different return distributions, volatility levels, and correlation structures:

\begin{itemize}
    \item \textbf{Bull Markets}: Characterized by positive expected returns, moderate volatility, and lower average correlations (diversification works well)
    \item \textbf{Bear Markets}: Characterized by negative expected returns, elevated volatility, and correlation convergence (diversification breaks down)
\end{itemize}

Our regime-based enhancement aims to:
\begin{enumerate}
    \item Identify market regimes in real-time using Hidden Markov Models
    \item Predict regime transitions using machine learning
    \item Adjust portfolio exposure based on predicted regime
    \item Improve risk-adjusted returns while maintaining the HRP diversification benefits
\end{enumerate}

\subsection{Implementation Philosophy}

A critical design principle throughout this implementation is the \textbf{prevention of look-ahead bias}. Every computation uses only information available at decision time:

\begin{itemize}
    \item HMM regime labels use \textit{filtered} probabilities (forward algorithm only)
    \item Features use expanding-window Z-scores with lagged statistics
    \item XGBoost predictions use expanding training windows
    \item Macroeconomic data is lagged by one month to account for publication delays
\end{itemize}

\subsection{Six-Phase Architecture}

Our implementation follows six phases:

\begin{enumerate}
    \item \textbf{Phase 1 - Data Construction}: Prepare HMM input from market returns
    \item \textbf{Phase 2 - Regime Identification}: Fit expanding-window HMM with filtered probabilities
    \item \textbf{Phase 3 - Feature Engineering}: Compute 11 predictive features
    \item \textbf{Phase 3.5 - Feature Selection}: Permutation importance with time-series CV
    \item \textbf{Phase 4 - Prediction}: Walk-forward XGBoost with Optuna optimization
    \item \textbf{Phase 5/6 - Strategy Execution}: Multiple strategy variants with transaction costs
\end{enumerate}

% ============================================================================
% SECTION 2: DATA CONSTRUCTION (PHASE 1)
% ============================================================================
\section{Phase 1: Data Construction}

\subsection{Market Index Selection}

We use the HRP strategy returns for regime identification rather than an external market index. This approach:
\begin{itemize}
    \item Identifies regimes directly relevant to the strategy being traded
    \item Avoids potential misalignment between market-wide regimes and portfolio-specific performance
    \item Creates a feedback loop where regime detection is tailored to HRP characteristics
\end{itemize}

\begin{remark}
An alternative approach uses the CRSP Value-Weighted Index (\texttt{VWRETD}) as the market proxy. This index covers all NYSE, AMEX, and NASDAQ stocks weighted by market capitalization, includes dividends and delisting returns, and has history dating back to 1926. The choice between strategy returns and market returns depends on whether regime identification should be strategy-specific or market-wide.
\end{remark}

\subsection{Log Returns}

We transform simple returns to log returns for HMM input:
\begin{equation}
r_t^{\log} = \ln(1 + R_t)
\label{eq:log_return}
\end{equation}

where $R_t$ is the simple monthly return. Log returns have desirable properties:
\begin{itemize}
    \item Time additivity: $r_{t_1 \to t_2}^{\log} = \sum_{t=t_1}^{t_2} r_t^{\log}$
    \item Approximate normality (closer to Gaussian than simple returns)
    \item Symmetric treatment of gains and losses
\end{itemize}

\subsection{Rolling Downside Deviation}

Rather than using realized volatility (which treats upside and downside deviations equally), we compute \textit{downside deviation} to capture asymmetric risk:

\begin{definition}[Downside Deviation]
For a return series $\{r_t\}$ over window $[t-w+1, t]$ with threshold $\tau$ (typically 0):
\begin{equation}
\sigma_t^{\text{down}} = \sqrt{\frac{1}{n^-} \sum_{s=t-w+1}^{t} (r_s - \tau)^2 \cdot \mathbf{1}[r_s < \tau]}
\label{eq:downside_dev}
\end{equation}
where $n^- = \sum_{s} \mathbf{1}[r_s < \tau]$ is the count of returns below threshold.
\end{definition}

Our implementation uses:
\begin{itemize}
    \item Rolling window: $w = 12$ months
    \item Threshold: $\tau = 0$ (target return = 0)
    \item Minimum observations: If $n^- < 2$, fallback to $\sigma_t^{\text{down}} = 0.5 \times \sigma_t^{\text{full}}$
\end{itemize}

\begin{remark}
Downside deviation is more informative for regime detection than realized volatility because bear markets are characterized by frequent negative returns, while bull markets may have elevated volatility from upside moves that do not pose portfolio risk.
\end{remark}

\subsection{Feature Standardization}

Both HMM inputs are standardized to zero mean and unit variance:
\begin{align}
z_t^{\text{ret}} &= \frac{r_t^{\log} - \mu^{\text{ret}}}{\sigma^{\text{ret}}} \\
z_t^{\text{dd}} &= \frac{\sigma_t^{\text{down}} - \mu^{\text{dd}}}{\sigma^{\text{dd}}}
\end{align}

where $(\mu, \sigma)$ are computed over the available history at time $t$ (expanding window).

The HMM observation vector is:
\begin{equation}
\mathbf{x}_t = \begin{pmatrix} z_t^{\text{ret}} \\ z_t^{\text{dd}} \end{pmatrix} \in \R^2
\label{eq:hmm_input}
\end{equation}

% ============================================================================
% SECTION 3: REGIME IDENTIFICATION (PHASE 2)
% ============================================================================
\section{Phase 2: Regime Identification via Hidden Markov Models}

\subsection{Hidden Markov Model Specification}

We employ a two-state Gaussian Hidden Markov Model (HMM) to identify bull and bear market regimes.

\begin{definition}[Gaussian HMM]
A Gaussian HMM with $K$ states is defined by:
\begin{itemize}
    \item \textbf{Hidden states}: $S_t \in \{1, \ldots, K\}$
    \item \textbf{Initial distribution}: $\boldsymbol{\pi} = (\pi_1, \ldots, \pi_K)$ where $\pi_k = \Prob(S_1 = k)$
    \item \textbf{Transition matrix}: $\mathbf{A}$ where $A_{jk} = \Prob(S_{t+1} = k | S_t = j)$
    \item \textbf{Emission distribution}: $\mathbf{x}_t | S_t = k \sim \mathcal{N}(\boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)$
\end{itemize}
\end{definition}

Our configuration:
\begin{itemize}
    \item Number of states: $K = 2$ (Bear and Bull)
    \item Covariance type: Full (unrestricted $2 \times 2$ covariance matrices)
    \item Convergence tolerance: $10^{-4}$
    \item Maximum iterations: 1000
\end{itemize}

\subsection{The Forward Algorithm and Filtered Probabilities}

A critical distinction exists between \textit{filtered} and \textit{smoothed} probabilities:

\begin{definition}[Filtered vs. Smoothed Probabilities]
\begin{align}
\text{Filtered}: \quad & \gamma_t^{\text{filt}}(k) = \Prob(S_t = k | \mathbf{x}_{1:t}) \label{eq:filtered}\\
\text{Smoothed}: \quad & \gamma_t^{\text{smooth}}(k) = \Prob(S_t = k | \mathbf{x}_{1:T}) \label{eq:smoothed}
\end{align}
\end{definition}

Smoothed probabilities use the \textit{entire} sequence $\mathbf{x}_{1:T}$ including future observations, introducing look-ahead bias. We use only filtered probabilities, computed via the forward algorithm:

\begin{algorithm}[H]
\caption{Forward Algorithm for Filtered Probabilities}
\begin{algorithmic}[1]
\Require Observations $\mathbf{x}_{1:t}$, HMM parameters $(\boldsymbol{\pi}, \mathbf{A}, \{\boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k\})$
\Ensure Filtered probabilities $\gamma_t^{\text{filt}}$

\State \textbf{Initialization} ($t=1$):
\State $\alpha_1(k) = \pi_k \cdot \phi(\mathbf{x}_1; \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)$ for $k = 1, \ldots, K$

\State \textbf{Recursion} (for $t = 2, \ldots, T$):
\For{$k = 1, \ldots, K$}
    \State $\alpha_t(k) = \phi(\mathbf{x}_t; \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k) \sum_{j=1}^{K} \alpha_{t-1}(j) \cdot A_{jk}$
\EndFor

\State \textbf{Normalization}:
\State $\gamma_t^{\text{filt}}(k) = \frac{\alpha_t(k)}{\sum_{j=1}^{K} \alpha_t(j)}$

\State \Return $\gamma_t^{\text{filt}}$
\end{algorithmic}
\end{algorithm}

where $\phi(\mathbf{x}; \boldsymbol{\mu}, \boldsymbol{\Sigma})$ is the multivariate Gaussian density.

\subsection{Expanding-Window Training Protocol}

To ensure no look-ahead bias in parameter estimation, we employ an expanding-window training protocol:

\begin{algorithm}[H]
\caption{Expanding-Window HMM Fitting}
\begin{algorithmic}[1]
\Require Standardized observations $\{\mathbf{x}_t\}_{t=1}^{T}$, min training $T_{\min}$, refit frequency $f$
\Ensure Regime labels $\{s_t\}$, probabilities $\{p_t^{\text{bull}}\}$

\State Initialize: $t_{\text{last\_fit}} \gets -f$, $\mathcal{M} \gets \text{None}$

\For{$t = T_{\min}, \ldots, T$}
    \If{$t - t_{\text{last\_fit}} \geq f$ \textbf{or} $\mathcal{M} = \text{None}$}
        \State Fit HMM on $\mathbf{x}_{1:t}$: $\mathcal{M} \gets \text{GaussianHMM.fit}(\mathbf{x}_{1:t})$
        \State $t_{\text{last\_fit}} \gets t$
        \State Determine state mapping (see Section~\ref{sec:state_sorting})
    \EndIf
    
    \State Compute filtered probability: $\gamma_t \gets \mathcal{M}.\text{predict\_proba}(\mathbf{x}_{1:t})[-1]$
    \State Apply state mapping if needed
    \State $p_t^{\text{bull}} \gets \gamma_t[1]$
    \State $s_t \gets \mathbf{1}[p_t^{\text{bull}} \geq 0.5]$
\EndFor

\State \Return $\{s_t\}$, $\{p_t^{\text{bull}}\}$
\end{algorithmic}
\end{algorithm}

Configuration parameters:
\begin{itemize}
    \item Minimum training period: $T_{\min} = 60$ months (5 years)
    \item Refit frequency: $f = 12$ months (annual refitting)
\end{itemize}

\subsection{State Sorting Algorithm}
\label{sec:state_sorting}

HMMs do not inherently assign semantic meaning to state labels. We sort states by average return to ensure consistent interpretation:

\begin{equation}
\text{State } k^* = \argmax_k \frac{1}{|\{t: \hat{s}_t = k\}|} \sum_{t: \hat{s}_t = k} r_t^{\log}
\label{eq:state_sort}
\end{equation}

The state $k^*$ with the highest mean return is labeled ``Bull'' (state 1); the other is ``Bear'' (state 0). If the HMM's internal state 0 has higher returns, we swap the probability columns.

\subsection{Transition Matrix and Regime Statistics}

The empirical transition matrix is computed from the regime sequence:
\begin{equation}
\hat{A}_{jk} = \frac{\#\{t : s_t = j, s_{t+1} = k\}}{\#\{t : s_t = j\}}
\label{eq:empirical_trans}
\end{equation}

Expected regime duration (in months):
\begin{equation}
\E[\text{Duration}_k] = \frac{1}{1 - A_{kk}}
\label{eq:expected_duration}
\end{equation}

For a typical calibration, we observe:
\begin{itemize}
    \item Bull regime: $\sim$70\% of months, expected duration $\sim$20 months
    \item Bear regime: $\sim$30\% of months, expected duration $\sim$8 months
    \item Bear-to-Bull transition probability: $\sim$12\%
    \item Bull-to-Bear transition probability: $\sim$5\%
\end{itemize}

% ============================================================================
% SECTION 4: FEATURE ENGINEERING (PHASE 3)
% ============================================================================
\section{Phase 3: Feature Engineering}

\subsection{Feature Design Philosophy}

Our predictive features fall into four categories:
\begin{enumerate}
    \item \textbf{Cross-Sectional}: Computed from the distribution of stock returns/characteristics
    \item \textbf{Macroeconomic}: Derived from Federal Reserve economic data
    \item \textbf{Factor-Based}: Constructed from systematic factor portfolios
    \item \textbf{Strategy Momentum}: Time-series momentum of the HRP strategy itself
\end{enumerate}

All features are:
\begin{itemize}
    \item Computed using \textbf{rolling Z-scores} (36 or 60-month windows) with shifted statistics to prevent look-ahead bias
    \item \textbf{Lagged by one month} for macroeconomic data to account for publication delays
    \item \textbf{Winsorized} at the 1st and 99th percentiles inside the walk-forward loop using only training data bounds
\end{itemize}

\subsection{Expanding-Window Z-Score}

Rather than using a fixed rolling window, we compute Z-scores using shifted statistics to ensure no look-ahead bias:

\begin{equation}
z_t = \frac{x_t - \mu_{t-1:t-w}}{\sigma_{t-1:t-w}}
\label{eq:expanding_zscore}
\end{equation}

where:
\begin{align}
\mu_{t-1:t-w} &= \frac{1}{w} \sum_{s=t-w}^{t-1} x_s \\
\sigma_{t-1:t-w} &= \sqrt{\frac{1}{w-1} \sum_{s=t-w}^{t-1} (x_s - \mu_{t-1:t-w})^2}
\end{align}

The implementation uses rolling windows of 36 or 60 months depending on the feature, with statistics computed from $x_{t-w}, \ldots, x_{t-1}$ (shifted by 1), ensuring strictly ex-ante standardization.

\subsection{Cross-Sectional Features}

\subsubsection{Return Dispersion}

Cross-sectional standard deviation of stock returns:
\begin{equation}
\text{Dispersion}_t = \text{std}_{i \in \mathcal{U}_t}(R_{i,t})
\label{eq:dispersion}
\end{equation}

where $\mathcal{U}_t$ is the stock universe at time $t$.

\textbf{Economic Intuition}: High dispersion indicates stock-specific information dominating, while low dispersion suggests systematic (market) factors dominating. Bear markets often exhibit lower dispersion as correlations increase.

\subsubsection{Amihud Illiquidity}

Market-wide illiquidity measure:
\begin{equation}
\text{Amihud}_t = \text{median}_{i \in \mathcal{U}_t}\left(\frac{|R_{i,t}|}{\text{DollarVolume}_{i,t}}\right) \times 10^6
\label{eq:amihud}
\end{equation}

We use the log-transformed value for normality:
\begin{equation}
\text{Amihud}^{\log}_t = \ln(\text{Amihud}_t)
\end{equation}

\textbf{Economic Intuition}: Elevated illiquidity (high Amihud) often precedes or accompanies market stress, as liquidity providers withdraw.

\subsubsection{Average Pairwise Correlation}

We use a variance-ratio proxy for computational efficiency:
\begin{equation}
\hat{\rho}_t^{\text{avg}} \approx \frac{\Var(R_{\text{mkt},t})}{\text{mean}_i(\Var(R_{i,t}))}
\label{eq:avg_corr}
\end{equation}

This ratio equals 1 when all stocks are perfectly correlated, and $1/N$ when uncorrelated.

\textbf{Economic Intuition}: Rising correlations signal increasing systematic risk and reduced diversification benefits—a hallmark of bear markets.

\subsubsection{Valuation Spread}

The spread between value and growth stock valuations:
\begin{equation}
\text{ValSpread}_t = \ln\left(\text{median}_{i \in Q_5}(\text{BM}_{i,t})\right) - \ln\left(\text{median}_{i \in Q_1}(\text{BM}_{i,t})\right)
\label{eq:val_spread}
\end{equation}

where $Q_5$ and $Q_1$ are the highest and lowest book-to-market quintiles, respectively.

\textbf{Economic Intuition}: A wide valuation spread indicates extreme dispersion between cheap and expensive stocks, often observed before regime transitions.

\subsection{Macroeconomic Features}

All macroeconomic data is sourced from FRED (Federal Reserve Economic Data) and lagged by one month to account for publication delays.

\begin{table}[H]
\centering
\caption{Macroeconomic Features and Publication Lags}
\begin{tabular}{llll}
\toprule
\textbf{Feature} & \textbf{FRED Code} & \textbf{Calculation} & \textbf{Lag} \\
\midrule
Credit Spread & BAA, DGS10 & $\text{BAA} - \text{DGS10}$ & Real-time \\
Term Spread & DGS10, TB3MS & $\text{DGS10} - \text{TB3MS}$ & Real-time \\
CPI Volatility & CPIAUCSL & 24-month rolling std of YoY CPI & 1 month \\
M2 Growth & M2SL & Year-over-year \% change & 1 month \\
Unemployment Trend & UNRATE & $\text{UNRATE} - \text{MA}_{12}(\text{UNRATE})$ & 1 month \\
\bottomrule
\end{tabular}
\label{tab:macro_features}
\end{table}

\subsubsection{Credit Spread}

\begin{equation}
\text{CreditSpread}_t = Y_t^{\text{BAA}} - Y_t^{\text{10Y Treasury}}
\label{eq:credit_spread}
\end{equation}

\textbf{Economic Intuition}: Widening credit spreads indicate increasing default risk and flight to quality—strong bear market indicator.

\subsubsection{Term Spread}

\begin{equation}
\text{TermSpread}_t = Y_t^{\text{10Y Treasury}} - Y_t^{\text{3M Treasury}}
\label{eq:term_spread}
\end{equation}

\textbf{Economic Intuition}: An inverted yield curve (negative term spread) historically predicts recessions with high accuracy.

\subsubsection{CPI Volatility}

\begin{equation}
\text{CPI\_Vol}_t = \text{std}_{24m}\left(\frac{\text{CPI}_{t-1} - \text{CPI}_{t-13}}{\text{CPI}_{t-13}}\right)
\label{eq:cpi_vol}
\end{equation}

\textbf{Economic Intuition}: Inflation uncertainty is associated with economic instability and often precedes market downturns.

\subsubsection{M2 Money Supply Growth}

\begin{equation}
\text{M2\_Growth}_t = \frac{\text{M2}_{t-1} - \text{M2}_{t-13}}{\text{M2}_{t-13}}
\label{eq:m2_growth}
\end{equation}

\textbf{Economic Intuition}: Rapid money supply expansion can signal future inflation; contraction can signal tightening financial conditions.

\subsubsection{Unemployment Trend}

\begin{equation}
\text{UNRATE\_Trend}_t = \text{UNRATE}_{t-1} - \text{MA}_{12}(\text{UNRATE}_{t-1})
\label{eq:unrate_trend}
\end{equation}

\textbf{Economic Intuition}: Rising unemployment trend above its moving average is a lagging but reliable recession indicator.

\subsection{Factor-Based Features}

\subsubsection{Betting Against Beta (BAB) Factor Momentum}

We construct a BAB factor following \citet{frazzini2014betting}:

\begin{enumerate}
    \item Estimate rolling 36-month beta for each stock:
    \begin{equation}
    \hat{\beta}_{i,t} = \frac{\Cov(R_{i}, R_{\text{mkt}})}{\Var(R_{\text{mkt}})} \quad \text{(36-month window)}
    \end{equation}
    
    \item Clip betas to $[-5, 5]$ to remove outliers
    
    \item \textbf{Lag betas by one month} to avoid look-ahead bias:
    \begin{equation}
    \beta_{i,t}^{\text{sort}} = \hat{\beta}_{i,t-1}
    \end{equation}
    
    \item Form portfolios:
    \begin{align}
    R_t^{\text{Low}\beta} &= \text{mean}_{i \in Q_1(\beta^{\text{sort}})}(R_{i,t}) \\
    R_t^{\text{High}\beta} &= \text{mean}_{i \in Q_5(\beta^{\text{sort}})}(R_{i,t})
    \end{align}
    
    \item BAB factor return:
    \begin{equation}
    R_t^{\text{BAB}} = R_t^{\text{Low}\beta} - R_t^{\text{High}\beta}
    \end{equation}
    
    \item Compute 12-month factor momentum:
    \begin{equation}
    \text{BAB\_Mom}_t = \sum_{s=t-11}^{t} \ln(1 + R_s^{\text{BAB}})
    \end{equation}
\end{enumerate}

\textbf{Economic Intuition}: When low-beta stocks outperform (positive BAB momentum), it signals a risk-off environment characteristic of bear markets.

\subsection{Technical Indicators}

\subsubsection{Relative Strength Index (RSI)}

The 14-period RSI provides a momentum signal:
\begin{equation}
\text{RSI}_t = 100 - \frac{100}{1 + \frac{\text{AvgGain}_{14}}{\text{AvgLoss}_{14}}}
\label{eq:rsi}
\end{equation}

\textbf{Economic Intuition}: RSI values below 30 indicate oversold conditions (potential reversal), while values above 70 indicate overbought conditions.

\subsection{Strategy Momentum Features}

We compute time-series momentum features from the HRP strategy returns themselves:

\subsubsection{1-Month HRP Momentum}
\begin{equation}
\text{HRP\_Mom\_1M}_t = r_t^{\text{HRP}}
\label{eq:hrp_mom_1m}
\end{equation}

\subsubsection{3-Month HRP Momentum}
\begin{equation}
\text{HRP\_Mom\_3M}_t = \sum_{s=t-2}^{t} \ln(1 + r_s^{\text{HRP}})
\label{eq:hrp_mom_3m}
\end{equation}

\subsubsection{12-Month HRP Momentum}
\begin{equation}
\text{HRP\_Mom\_12M}_t = \sum_{s=t-11}^{t} \ln(1 + r_s^{\text{HRP}})
\label{eq:hrp_mom_12m}
\end{equation}

All momentum features are Z-scored using a 24-month rolling window to reduce burn-in data loss.

\textbf{Economic Intuition}: Strategy momentum captures regime persistence---strong recent performance tends to continue in bull markets, while poor recent performance may indicate bear market conditions.

\subsection{Feature Summary}

\begin{table}[H]
\centering
\caption{Complete Feature Set (14 Features)}
\begin{tabular}{llp{6cm}}
\toprule
\textbf{Feature} & \textbf{Category} & \textbf{Bear Market Signal} \\
\midrule
\texttt{dispersion\_z} & Cross-Sectional & Low dispersion (correlation convergence) \\
\texttt{amihud\_z} & Cross-Sectional & High illiquidity \\
\texttt{avg\_pairwise\_corr\_z} & Cross-Sectional & High correlation \\
\texttt{valuation\_spread\_z} & Cross-Sectional & Extreme spread \\
\texttt{credit\_spread} & Macro & Widening spread \\
\texttt{term\_spread} & Macro & Inverted/flat curve \\
\texttt{cpi\_vol} & Macro & High inflation uncertainty \\
\texttt{m2\_growth} & Macro & Rapid changes \\
\texttt{unrate\_trend} & Macro & Rising trend \\
\texttt{bab\_z} & Factor & Positive momentum \\
\texttt{rsi} & Technical & Extreme values \\
\texttt{hrp\_mom\_1m\_z} & Strategy & Negative momentum \\
\texttt{hrp\_mom\_3m\_z} & Strategy & Negative momentum \\
\texttt{hrp\_mom\_12m\_z} & Strategy & Negative momentum \\
\bottomrule
\end{tabular}
\label{tab:feature_summary}
\end{table}

% ============================================================================
% SECTION 5: FEATURE SELECTION (PHASE 3.5)
% ============================================================================
\section{Phase 3.5: Feature Selection via Permutation Importance}

\subsection{Motivation}

Not all features contribute equally to regime prediction. Feature selection serves to:
\begin{enumerate}
    \item Remove noise features that degrade out-of-sample performance
    \item Reduce overfitting by limiting model complexity
    \item Improve interpretability
    \item Decrease computational cost
\end{enumerate}

\begin{remark}[Feature Selection Inside Walk-Forward]
To prevent look-ahead bias, feature selection is performed \textbf{inside} the walk-forward loop at each model refit, using only training data available at that point. This ensures that feature importance is computed without access to future observations.
\end{remark}

\subsection{Time-Series Cross-Validation}

Standard $k$-fold cross-validation violates the temporal structure of financial data. We employ time-series cross-validation with expanding training windows:

\begin{algorithm}[H]
\caption{Time-Series Cross-Validation Splits}
\begin{algorithmic}[1]
\Require Total samples $N$, number of splits $K$, minimum training size $T_{\min}$
\Ensure List of (train, test) index pairs

\State $\text{test\_size} \gets \lfloor (N - T_{\min}) / K \rfloor$

\For{$k = 0, \ldots, K-1$}
    \State $\text{train\_end} \gets T_{\min} + k \times \text{test\_size}$
    \State $\text{test\_start} \gets \text{train\_end}$
    \State $\text{test\_end} \gets \min(\text{test\_start} + \text{test\_size}, N)$
    \State $\text{train\_idx} \gets [0, \text{train\_end})$
    \State $\text{test\_idx} \gets [\text{test\_start}, \text{test\_end})$
    \State \textbf{yield} $(\text{train\_idx}, \text{test\_idx})$
\EndFor
\end{algorithmic}
\end{algorithm}

Configuration:
\begin{itemize}
    \item Number of splits: $K = 5$
    \item Minimum training size: $T_{\min} = 120$ months (10 years)
\end{itemize}

\subsection{Permutation Importance}

For each feature, we measure its importance by the drop in model performance when that feature is randomly permuted:

\begin{equation}
\text{Importance}(f) = \text{AUC}^{\text{baseline}} - \text{AUC}^{\text{permuted}(f)}
\label{eq:perm_importance}
\end{equation}

\begin{algorithm}[H]
\caption{Permutation Importance with Time-Series CV}
\begin{algorithmic}[1]
\Require Features $\mathbf{X}$, labels $\mathbf{y}$, CV splits
\Ensure Importance scores for each feature

\For{each fold $(D_{\text{train}}, D_{\text{test}})$}
    \State Train XGBoost on $D_{\text{train}}$ with baseline parameters
    \State Compute baseline AUC on $D_{\text{test}}$
    
    \For{each feature $f$}
        \State Permute feature $f$ in $D_{\text{test}}$
        \State Compute permuted AUC
        \State $\text{Importance}_{\text{fold}}(f) \gets \text{AUC}^{\text{baseline}} - \text{AUC}^{\text{permuted}}$
    \EndFor
\EndFor

\State $\text{Importance}(f) \gets \text{mean}(\text{Importance}_{\text{fold}}(f))$
\State \Return $\{\text{Importance}(f)\}$
\end{algorithmic}
\end{algorithm}

\subsection{Selection Criterion}

Features are selected if their mean importance exceeds a threshold:
\begin{equation}
\mathcal{F}^{\text{selected}} = \{f : \text{Importance}(f) > \epsilon\}
\label{eq:feature_selection}
\end{equation}

We use $\epsilon = 0$ (keep features with positive contribution).

% ============================================================================
% SECTION 6: XGBOOST PREDICTION (PHASE 4)
% ============================================================================
\section{Phase 4: Walk-Forward XGBoost Prediction}

\subsection{XGBoost for Regime Classification}

XGBoost (eXtreme Gradient Boosting) is an ensemble learning method that builds sequential decision trees, each correcting errors of the previous ensemble:

\begin{equation}
\hat{y}_i^{(m)} = \sum_{k=1}^{m} f_k(\mathbf{x}_i), \quad f_k \in \mathcal{F}
\label{eq:xgboost}
\end{equation}

where $\mathcal{F}$ is the space of regression trees.

For binary classification, we use the logistic loss:
\begin{equation}
\mathcal{L} = -\frac{1}{N} \sum_{i=1}^{N} \left[y_i \log(\hat{p}_i) + (1-y_i)\log(1-\hat{p}_i)\right]
\label{eq:logloss}
\end{equation}

with regularization:
\begin{equation}
\Omega(f) = \gamma T + \frac{1}{2}\lambda \sum_{j=1}^{T} w_j^2 + \alpha \sum_{j=1}^{T} |w_j|
\label{eq:xgb_reg}
\end{equation}

where $T$ is the number of leaves, $w_j$ are leaf weights, and $(\gamma, \lambda, \alpha)$ are regularization parameters.

\subsection{Optuna Hyperparameter Optimization}

We employ Optuna's Tree-structured Parzen Estimator (TPE) sampler for Bayesian hyperparameter optimization.

\subsubsection{Search Space}

\begin{table}[H]
\centering
\caption{XGBoost Hyperparameter Search Space}
\begin{tabular}{llll}
\toprule
\textbf{Parameter} & \textbf{Range} & \textbf{Type} & \textbf{Purpose} \\
\midrule
\texttt{max\_depth} & $[2, 6]$ & Integer & Tree complexity \\
\texttt{min\_child\_weight} & $[1, 10]$ & Integer & Leaf sample threshold \\
\texttt{gamma} & $[0.0, 0.5]$ & Float & Split gain threshold \\
\texttt{reg\_alpha} ($\alpha$) & $[0.0, 1.0]$ & Float & L1 regularization \\
\texttt{reg\_lambda} ($\lambda$) & $[0.5, 3.0]$ & Float & L2 regularization \\
\texttt{subsample} & $[0.6, 1.0]$ & Float & Row sampling ratio \\
\texttt{colsample\_bytree} & $[0.5, 1.0]$ & Float & Column sampling ratio \\
\texttt{learning\_rate} & $[0.01, 0.2]$ & Float (log) & Step shrinkage \\
\texttt{n\_rounds} & $[50, 200]$ & Integer & Number of trees \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Objective Function}

The Optuna objective maximizes macro F1-score (average of Bear and Bull F1) via 5-fold time-series CV:

\begin{equation}
\text{Objective} = \frac{1}{K} \sum_{k=1}^{K} F1_{\text{macro}}^{(k)}
\label{eq:optuna_obj}
\end{equation}

Configuration:
\begin{itemize}
    \item Number of trials: 20 per refit (reduced for efficiency in walk-forward)
    \item Early stopping: 15 rounds without improvement
    \item Sampler: TPE with seed 42
\end{itemize}

\begin{remark}[Hyperparameter Tuning Inside Walk-Forward]
Hyperparameters are re-tuned at each model refit using only training data available at that point. This prevents look-ahead bias in hyperparameter selection and allows the model to adapt to changing market conditions.
\end{remark}

\subsection{Class Imbalance Handling}

Bull regimes are more frequent than Bear regimes, creating class imbalance. We address this via:

\begin{equation}
\texttt{scale\_pos\_weight} = \frac{n_{\text{Bear}}}{n_{\text{Bull}}}
\label{eq:scale_pos_weight}
\end{equation}

This reweights the loss function to penalize misclassification of the minority class more heavily.

\subsection{Walk-Forward Prediction Protocol}

\begin{algorithm}[H]
\caption{Walk-Forward XGBoost Prediction}
\begin{algorithmic}[1]
\Require Features $\mathbf{X}$, targets $\mathbf{y}$, Optuna-optimized parameters $\theta^*$
\Require Min training $T_{\min} = 120$, refit frequency $f = 12$
\Ensure Predictions $\{\hat{p}_t^{\text{bull}}\}$, feature importances

\State $t_{\text{last\_fit}} \gets -f$, $\mathcal{M} \gets \text{None}$

\For{$t = T_{\min}, \ldots, N$}
    \If{$t - t_{\text{last\_fit}} \geq f$}
        \State Train XGBoost on $(\mathbf{X}_{1:t}, \mathbf{y}_{1:t})$ with $\theta^*$
        \State Store feature importance (gain-based)
        \State $t_{\text{last\_fit}} \gets t$
    \EndIf
    
    \State $\hat{p}_t^{\text{bull}} \gets \mathcal{M}.\text{predict}(\mathbf{x}_t)$
    \State $\hat{s}_t \gets \mathbf{1}[\hat{p}_t^{\text{bull}} \geq 0.5]$
\EndFor

\State \Return $\{\hat{p}_t^{\text{bull}}, \hat{s}_t\}$
\end{algorithmic}
\end{algorithm}

\subsection{Prediction Metrics}

We evaluate predictions using:

\begin{itemize}
    \item \textbf{Accuracy}: $\frac{\text{TP} + \text{TN}}{N}$
    \item \textbf{ROC-AUC}: Area under the receiver operating characteristic curve
    \item \textbf{Bear Recall} (Sensitivity): $\frac{\text{TP}_{\text{Bear}}}{\text{TP}_{\text{Bear}} + \text{FN}_{\text{Bear}}}$
    \item \textbf{Bull Recall}: $\frac{\text{TP}_{\text{Bull}}}{\text{TP}_{\text{Bull}} + \text{FN}_{\text{Bull}}}$
    \item \textbf{F1-Score}: Harmonic mean of precision and recall
\end{itemize}

\begin{remark}
Bear Recall is particularly important: failing to detect a bear market results in full exposure during drawdowns, while a false bear signal only reduces exposure during a bull market (smaller opportunity cost).
\end{remark}

% ============================================================================
% SECTION 7: STRATEGY EXECUTION (PHASES 5-6)
% ============================================================================
\section{Phases 5-6: Strategy Execution}

\subsection{Two-Fund Separation Framework}

Our regime-based strategies implement two-fund separation between the HRP risky portfolio and the risk-free asset (T-Bills):
\begin{equation}
R_t^{\text{portfolio}} = \alpha_t \times R_t^{\text{HRP}} + (1 - \alpha_t) \times R_t^f
\label{eq:two_fund}
\end{equation}

where $\alpha_t \in [0, 1]$ is the allocation to the HRP portfolio at time $t$.

\subsection{Transaction Cost Model}

We separate transaction costs into two components:

\subsubsection{Inner Cost (HRP Rebalancing)}
Transaction costs from monthly HRP portfolio rebalancing:
\begin{equation}
\text{TC}_t^{\text{inner}} = \tau \times \sum_{i=1}^{N} |w_{i,t} - w_{i,t-1}|
\label{eq:inner_tc}
\end{equation}

\subsubsection{Outer Cost (Leverage Overlay)}
Transaction costs from allocation changes:
\begin{equation}
\text{TC}_t^{\text{outer}} = \tau \times |\alpha_t - \alpha_{t-1}|
\label{eq:outer_tc}
\end{equation}

where $\tau = 10$ bps (0.10\%) per unit of turnover.

\subsection{Strategy Variants}

We implement two primary strategy variants:

\subsubsection{Strategy 1: Buy \& Hold HRP (Baseline)}

Standard HRP with monthly rebalancing:
\begin{equation}
R_t^{\text{B\&H}} = R_t^{\text{HRP, gross}} - \text{TC}_t^{\text{inner}}
\label{eq:strat1}
\end{equation}

\subsubsection{Strategy 2: P(Bull) Scaled Allocation}

Smooth allocation scaled by bull probability with mean normalization:
\begin{equation}
\alpha_t^{\text{scaled}} = \frac{\hat{p}_t^{\text{bull}}}{\bar{p}^{\text{bull}}}
\label{eq:scaled_alloc}
\end{equation}

where $\bar{p}^{\text{bull}}$ is the average bull probability over the strategy period. This scaling ensures the average allocation equals 1 (full investment), while allowing for both de-risking (when $\hat{p}_t^{\text{bull}} < \bar{p}^{\text{bull}}$) and leveraging (when $\hat{p}_t^{\text{bull}} > \bar{p}^{\text{bull}}$).

\begin{equation}
R_t^{\text{scaled}} = \alpha_t^{\text{scaled}} \times R_t^{\text{HRP, net}} + (1 - \alpha_t^{\text{scaled}}) \times R_t^f - \text{TC}_t^{\text{outer}}
\label{eq:strat2}
\end{equation}

\begin{remark}
The probability scaling approach is preferred over binary switching because:
\begin{itemize}
    \item It reduces transaction costs from discrete allocation changes
    \item It utilizes the full information content of probability estimates
    \item It provides smoother allocation dynamics
\end{itemize}
\end{remark}

\subsection{Strategy Summary}

\begin{table}[H]
\centering
\caption{Strategy Variants Comparison}
\begin{tabular}{lllp{5cm}}
\toprule
\textbf{Strategy} & \textbf{Allocation} & \textbf{Range} & \textbf{Key Mechanism} \\
\midrule
1. Buy \& Hold HRP & $\alpha = 1$ & [1, 1] & Baseline diversification \\
2. P(Bull) Scaled & $\alpha = p/\bar{p}$ & Variable & Probability-weighted leverage \\
\bottomrule
\end{tabular}
\end{table}

% ============================================================================
% SECTION 8: STATISTICAL INFERENCE
% ============================================================================
\section{Statistical Inference}

\subsection{Block Bootstrap for Time Series}

Standard bootstrap assumes i.i.d. observations, which is violated by autocorrelated financial returns. We employ \textbf{block bootstrap} with block length $b = 12$ months to preserve the temporal dependence structure.

\begin{algorithm}[H]
\caption{Block Bootstrap Confidence Intervals}
\begin{algorithmic}[1]
\Require Returns $\{r_t\}_{t=1}^{N}$, block size $b$, bootstrap samples $B$, confidence level $1-\alpha$
\Ensure Point estimate $\hat{\theta}$ and CI $[\theta_L, \theta_U]$

\For{$j = 1, \ldots, B$}
    \State $n_{\text{blocks}} \gets \lceil N / b \rceil$
    \State Sample $n_{\text{blocks}}$ block start indices uniformly from $\{1, \ldots, N-b+1\}$
    \State Concatenate blocks to form bootstrap sample $\{r_t^{*(j)}\}$
    \State Truncate/pad to length $N$
    \State Compute statistic: $\theta_j^* \gets g(\{r_t^{*(j)}\})$
\EndFor

\State $\theta_L \gets Q_{\alpha/2}(\{\theta_j^*\})$
\State $\theta_U \gets Q_{1-\alpha/2}(\{\theta_j^*\})$

\State \Return $\hat{\theta} = g(\{r_t\})$, $[\theta_L, \theta_U]$
\end{algorithmic}
\end{algorithm}

Configuration:
\begin{itemize}
    \item Bootstrap samples: $B = 1000$
    \item Block length: $b = 12$ months
    \item Confidence level: 95\% ($\alpha = 0.05$)
\end{itemize}

\subsection{Performance Metrics with Confidence Intervals}

We compute bootstrap CIs for the following metrics:

\subsubsection{Annualized Return}
\begin{equation}
R_{\text{ann}} = \left(\prod_{t=1}^{N} (1 + r_t)\right)^{12/N} - 1
\label{eq:ann_ret_ci}
\end{equation}

\subsubsection{Annualized Volatility}
\begin{equation}
\sigma_{\text{ann}} = \text{std}(\{r_t\}) \times \sqrt{12}
\label{eq:ann_vol_ci}
\end{equation}

\subsubsection{Sharpe Ratio}
\begin{equation}
SR = \frac{\bar{r} \times 12}{\sigma_{\text{ann}}}
\label{eq:sharpe_ci}
\end{equation}

(assuming risk-free rate is small and approximately constant)

\subsubsection{Sortino Ratio}
\begin{equation}
\text{Sortino} = \frac{\bar{r} \times 12}{\sigma_{\text{downside}} \times \sqrt{12}}
\label{eq:sortino_ci}
\end{equation}

where $\sigma_{\text{downside}} = \text{std}(\{r_t : r_t < 0\})$.

\subsubsection{Maximum Drawdown}
\begin{equation}
\text{MDD} = \min_t \left(\frac{V_t}{\max_{s \leq t} V_s} - 1\right)
\label{eq:mdd_ci}
\end{equation}

\subsection{Sharpe Ratio Difference Test}

To test whether a strategy significantly outperforms the baseline, we compute the bootstrap distribution of Sharpe ratio differences:

\begin{equation}
\Delta SR = SR^{\text{strategy}} - SR^{\text{baseline}}
\label{eq:delta_sharpe}
\end{equation}

\begin{proposition}[Statistical Significance]
Let $[\Delta SR_L, \Delta SR_U]$ be the 95\% bootstrap CI for $\Delta SR$. Then:
\begin{itemize}
    \item If $\Delta SR_L > 0$: Strategy is \textbf{significantly better}
    \item If $\Delta SR_U < 0$: Strategy is \textbf{significantly worse}
    \item Otherwise: Difference is \textbf{not statistically significant}
\end{itemize}
\end{proposition}

\subsection{Prediction Quality Metrics}

Beyond classification accuracy, we track:

\subsubsection{Bear Detection Rate}
\begin{equation}
\text{Bear Detection} = \frac{\text{True Bear Predictions}}{\text{Total Actual Bear Months}}
\label{eq:bear_detection}
\end{equation}

\subsubsection{False Bear Rate}
\begin{equation}
\text{False Bear} = \frac{\text{Bear Predictions during Bull}}{\text{Total Actual Bull Months}}
\label{eq:false_bear}
\end{equation}

A good model maximizes Bear Detection while minimizing False Bear Rate.

% ============================================================================
% SECTION 9: IMPLEMENTATION DETAILS
% ============================================================================
\section{Implementation Details}

\subsection{Software Dependencies}

\begin{itemize}
    \item \textbf{hmmlearn}: Gaussian HMM implementation
    \item \textbf{xgboost}: Gradient boosting framework
    \item \textbf{optuna}: Hyperparameter optimization
    \item \textbf{scikit-learn}: Cross-validation, metrics
    \item \textbf{pandas/numpy}: Data manipulation
    \item \textbf{ta}: Technical analysis indicators
\end{itemize}

\subsection{Pipeline Configuration Parameters}

\begin{table}[H]
\centering
\caption{Pipeline Configuration Summary}
\begin{tabular}{llll}
\toprule
\textbf{Phase} & \textbf{Parameter} & \textbf{Value} & \textbf{Purpose} \\
\midrule
\multirow{3}{*}{HMM} & Min training & 60 months & Burn-in period \\
 & Refit frequency & 12 months & Annual update \\
 & Downside window & 12 months & Asymmetric risk measure \\
\midrule
\multirow{2}{*}{Features} & Z-score window & 36--60 months & Rolling normalization \\
 & Macro lag & 1 month & Publication delay \\
\midrule
\multirow{2}{*}{Selection} & CV splits & 3 (inside WF) & Validation folds \\
 & Threshold & 0 & Positive importance \\
\midrule
\multirow{4}{*}{XGBoost} & Min training & 120 months & Initial train set \\
 & Refit frequency & 12 months & Annual update \\
 & Optuna trials & 20/refit & Search iterations \\
 & Purge gap & 12 months & Prevent leakage \\
\midrule
\multirow{2}{*}{Strategy} & TX cost & 10 bps & Trading friction \\
 & Bear allocation & 50\% & Defensive posture \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Computational Considerations}

\begin{itemize}
    \item \textbf{Optuna Parallelization}: Trials can be parallelized across CPU cores
    \item \textbf{Memory}: Feature computation requires storing full return panel
    \item \textbf{Runtime}: Full pipeline (1960-2024) runs in $\sim$10-15 minutes on modern hardware
    \item \textbf{Checkpointing}: Walk-forward results are cached for resumability
\end{itemize}

% ============================================================================
% SECTION 10: CONCLUSION
% ============================================================================
\section{Conclusion}

This document has detailed the complete regime-based enhancement to the HRP portfolio optimization methodology. Key innovations include:

\begin{enumerate}
    \item \textbf{Strategy-Specific Regime Detection}: Using HRP strategy returns for HMM regime identification, ensuring regimes are relevant to the portfolio being traded
    
    \item \textbf{Filtered HMM Probabilities}: Using forward algorithm only, eliminating smoothed probability look-ahead bias
    
    \item \textbf{Downside Deviation}: HMM input using asymmetric risk measure rather than symmetric volatility
    
    \item \textbf{Comprehensive Feature Set}: 14 features spanning cross-sectional, macroeconomic, factor-based, and strategy momentum signals with proper publication lag handling
    
    \item \textbf{Walk-Forward Feature Selection}: Permutation importance computed inside the walk-forward loop using only training data
    
    \item \textbf{Walk-Forward Hyperparameter Tuning}: Optuna optimization at each model refit to avoid look-ahead bias in parameter selection
    
    \item \textbf{Purged Time-Series CV}: 12-month purge gap and embargo period to prevent information leakage
    
    \item \textbf{Transaction-Cost-Aware Strategies}: Separation of inner (HRP rebalancing) and outer (leverage overlay) costs
    
    \item \textbf{Block Bootstrap Inference}: Proper statistical testing respecting time-series dependence
\end{enumerate}

The combination of robust base allocation (HRP) with regime-aware dynamic exposure provides a principled framework for capturing the diversification benefits of risk parity while adapting to changing market conditions.

% ============================================================================
% REFERENCES
% ============================================================================
\bibliographystyle{apalike}
\begin{thebibliography}{99}

\bibitem[Ang \& Bekaert(2002)]{ang2002regime}
Ang, A., \& Bekaert, G. (2002).
\newblock International asset allocation with regime shifts.
\newblock \emph{The Review of Financial Studies}, 15(4):1137--1187.

\bibitem[Baum et al.(1970)]{baum1970hmm}
Baum, L. E., Petrie, T., Soules, G., \& Weiss, N. (1970).
\newblock A maximization technique occurring in the statistical analysis of probabilistic functions of Markov chains.
\newblock \emph{The Annals of Mathematical Statistics}, 41(1):164--171.

\bibitem[Chen \& Guestrin(2016)]{chen2016xgboost}
Chen, T., \& Guestrin, C. (2016).
\newblock XGBoost: A scalable tree boosting system.
\newblock \emph{Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining}, 785--794.

\bibitem[Frazzini \& Pedersen(2014)]{frazzini2014betting}
Frazzini, A., \& Pedersen, L. H. (2014).
\newblock Betting against beta.
\newblock \emph{Journal of Financial Economics}, 111(1):1--25.

\bibitem[Hamilton(1989)]{hamilton1989regime}
Hamilton, J. D. (1989).
\newblock A new approach to the economic analysis of nonstationary time series and the business cycle.
\newblock \emph{Econometrica}, 57(2):357--384.

\bibitem[Künsch(1989)]{kunsch1989jackknife}
Künsch, H. R. (1989).
\newblock The jackknife and the bootstrap for general stationary observations.
\newblock \emph{The Annals of Statistics}, 17(3):1217--1241.

\bibitem[López de Prado(2016)]{lopez2016building}
López de Prado, M. (2016).
\newblock Building diversified portfolios that outperform out-of-sample.
\newblock \emph{The Journal of Portfolio Management}, 42(4):59--69.

\bibitem[Moreira \& Muir(2017)]{moreira2017volatility}
Moreira, A., \& Muir, T. (2017).
\newblock Volatility-managed portfolios.
\newblock \emph{The Journal of Finance}, 72(4):1611--1644.

\bibitem[Rabiner(1989)]{rabiner1989hmm}
Rabiner, L. R. (1989).
\newblock A tutorial on hidden Markov models and selected applications in speech recognition.
\newblock \emph{Proceedings of the IEEE}, 77(2):257--286.

\bibitem[Akiba et al.(2019)]{akiba2019optuna}
Akiba, T., Sano, S., Yanase, T., Ohta, T., \& Koyama, M. (2019).
\newblock Optuna: A next-generation hyperparameter optimization framework.
\newblock \emph{Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining}, 2623--2631.

\bibitem[Amihud(2002)]{amihud2002illiquidity}
Amihud, Y. (2002).
\newblock Illiquidity and stock returns: cross-section and time-series effects.
\newblock \emph{Journal of Financial Markets}, 5(1):31--56.

\bibitem[Politis \& Romano(1994)]{politis1994stationary}
Politis, D. N., \& Romano, J. P. (1994).
\newblock The stationary bootstrap.
\newblock \emph{Journal of the American Statistical Association}, 89(428):1303--1313.

\end{thebibliography}

\end{document}
